<div class="row mb-3">
    <div class="col-auto">
        <h3 class="m-0">AdapterDrop</h3>
    </div>
    <div class="col text-right d-none d-lg-block">
        <a class="btn-colab-headline" href="https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/05_Adapter_Drop_Training.ipynb" target="_blank">
            <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
        </a>
    </div>
</div>

<p>
    AdapterDrop allows us to remove adapters on lower layers during training and inference. This can be realised with the
    <code>skip_layers</code> argument. It specifies for which layers the adapters should be skipped during a forward pass. In
    order to train a model with AdapterDrop, we specify a callback for the <code>Trainer</code> class that sets the <code>skip_layers</code>
    argument to the layers that should be skipped in each step as follows:
</p>
<pre class="code" id="QuickstartAdapterDropMore">
class AdapterDropTrainerCallback(TrainerCallback):
  def on_step_begin(self, args, state, control, **kwargs):
    skip_layers = list(range(np.random.randint(0, 11)))
    kwargs['model'].set_active_adapters("rotten_tomatoes", skip_layers=skip_layers)

  def on_evaluate(self, args, state, control, **kwargs):
    # Deactivate skipping layers during evaluation (otherwise it would use the
    # previous randomly chosen skip_layers and thus yield results not comparable
    # across different epochs)
    kwargs['model'].set_active_adapters("rotten_tomatoes", skip_layers=None)
</pre>
<p>
    Checkout the <a href="https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/02_Adapter_Inference.ipynb">AdapterDrop Colab Notebook</a> for further details.
</p>