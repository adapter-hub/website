<div class="row mb-3">
    <div class="col-auto">
        <h3 class="m-0">Train an Adapter ğŸ‹ï¸ï¸</h3>
    </div>
    <div class="col text-right d-none d-lg-block">
        <a class="btn-colab-headline" href="https://colab.research.google.com/github/Adapter-Hub/website/blob/master/app/static/notebooks/Adapter_Quickstart_Training.ipynb" target="_blank">
            <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
        </a>
    </div>
</div>

<p>Training a new task adapter requires only few modifications compared to fully fine-tuning a model with Hugging Face's <code>Trainer</code>.
    We first load a pre-trained model, e.g., <code>roberta-base</code> and add a new task adapter:</p>
<pre class="code">model = AutoModelWithHeads.from_pretrained('roberta-base')
model.add_adapter("sst-2", AdapterType.text_task)
model.train_adapter(["sst-2"])
</pre>
<p>By calling <code>train_adapter(["sst-2"])</code> we freeze all transformer parameters except for the parameters of sst-2 adapter.
Before training we add a new classification head to our model:</p>
<pre class="code">model.add_classification_head("sst-2", num_labels=2)
model.set_active_adapters([["sst-2"]])
</pre>
<p>The weights of this classification head can be stored together with the adapter weights to allow for a full reproducibility.
The method call <code>model.set_active_adapters([["sst-2"]])</code> registers the sst-2 adapter as a default for training. This also supports adapter stacking and adapter fusion!</p>
<p>We can then train our adapter using the Hugging Face <code>Trainer</code>:</p>

<pre class="code">trainer.train()
model.save_all_adapters('output-path')</pre>

<div class="row">
    <div class="col-auto pr-0" style="font-size: 2rem">ğŸ’¡</div>
    <div class="col">
        <div class="alert alert-secondary">
            <span class="font-weight-bold">Tip 1</span>ï¸: Adapter weights are usually initialized randomly. That is why we require a higher learning rate.
            We have found that a default adapter learning rate of <code>lr=0.0001</code> works well for most settings.</div>
    </div>
</div>

<div class="row">
    <div class="col-auto pr-0" style="font-size: 2rem">ğŸ’¡</div>
    <div class="col">
        <div class="alert alert-secondary">
            <span class="font-weight-bold">Tip 2</span>ï¸: Depending on your data set size you might also need to train longer than usual.
            To avoid overfitting you can evaluating the adapters after each epoch on the development set and only save the best model.</div>
    </div>
</div>

<p>That's it! <code>model.save_all_adapters('output-path')</code> exports all adapters. Consider sharing them on AdapterHub!</p>
