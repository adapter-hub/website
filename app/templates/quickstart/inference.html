<div class="row mb-3">
    <div class="col-auto">
        <h3 class="m-0">Load an Adapter for Inference üèÑ</h3>
    </div>
    <div class="col text-right d-none d-lg-block">
        <a class="btn-colab-headline" href="https://colab.research.google.com/github/Adapter-Hub/adapters/blob/main/notebooks/02_Adapter_Inference.ipynb" target="_blank">
            <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
        </a>
    </div>
</div>

<p>Loading existing adapters from our repository is as simple as adding one additional line of code:</p>
<pre class="code">from adapters import AutoAdapterModel

model = AutoAdapterModel.from_pretrained("bert-base-uncased")
model.load_adapter("sentiment/sst-2@ukp")
model.set_active_adapters("sst-2")</pre>
<p>The <a href="https://adapterhub.ml/adapters/ukp/bert-base-uncased_sentiment_sst-2_pfeiffer/">SST adapter</a> is light-weight: it is only 3MB! At
    the same time, it achieves <a href="https://arxiv.org/abs/2007.07779" target="_blank">results</a> that are on-par with fully fine-tuned BERT.
    We can now leverage SST adapter to predict the sentiment of sentences:</p>
<pre class="code" id="QuickstartInferenceMore">
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
tokens = tokenizer.tokenize("AdapterHub is awesome!")
input_tensor = torch.tensor([
    tokenizer.convert_tokens_to_ids(tokens)
])
outputs = model(input_tensor)</pre>
