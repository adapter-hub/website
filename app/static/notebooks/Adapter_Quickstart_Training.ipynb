{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4PyaAqxoaBgT"
   },
   "source": [
    "# **AdapterHub** quickstart example for **training**\n",
    "\n",
    "This is an adaptation of the HuggingFace [sentence classification notebook](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/trainer/01_text_classification.ipynb).\n",
    "\n",
    "First, install adapter-transformers from github/master, download the SST dataset, and import the required modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GQwP-DPOvJQl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/adapter-hub/adapter-transformers.git@v2\n",
      "  Cloning https://github.com/adapter-hub/adapter-transformers.git (to revision v2) to c:\\users\\hster\\appdata\\local\\temp\\pip-req-build-2jk43mih\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: requests in c:\\users\\hster\\anaconda3\\lib\\site-packages (from adapter-transformers==2.0.0a1) (2.21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hster\\anaconda3\\lib\\site-packages (from adapter-transformers==2.0.0a1) (4.31.1)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\hster\\anaconda3\\lib\\site-packages (from adapter-transformers==2.0.0a1) (0.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hster\\anaconda3\\lib\\site-packages (from adapter-transformers==2.0.0a1) (3.0.10)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hster\\anaconda3\\lib\\site-packages (from adapter-transformers==2.0.0a1) (2020.4.4)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\hster\\anaconda3\\lib\\site-packages (from adapter-transformers==2.0.0a1) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hster\\anaconda3\\lib\\site-packages (from adapter-transformers==2.0.0a1) (1.20.2)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\hster\\anaconda3\\lib\\site-packages (from adapter-transformers==2.0.0a1) (0.0.38)\n",
      "Requirement already satisfied: packaging in c:\\users\\hster\\anaconda3\\lib\\site-packages (from adapter-transformers==2.0.0a1) (19.0)\n",
      "Requirement already satisfied: zipp>=0.3.2 in c:\\users\\hster\\anaconda3\\lib\\site-packages (from importlib-metadata->adapter-transformers==2.0.0a1) (0.3.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\hster\\anaconda3\\lib\\site-packages (from packaging->adapter-transformers==2.0.0a1) (2.3.1)\n",
      "Requirement already satisfied: six in c:\\users\\hster\\anaconda3\\lib\\site-packages (from packaging->adapter-transformers==2.0.0a1) (1.12.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\hster\\anaconda3\\lib\\site-packages (from requests->adapter-transformers==2.0.0a1) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\hster\\anaconda3\\lib\\site-packages (from requests->adapter-transformers==2.0.0a1) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hster\\anaconda3\\lib\\site-packages (from requests->adapter-transformers==2.0.0a1) (2020.6.20)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\hster\\anaconda3\\lib\\site-packages (from requests->adapter-transformers==2.0.0a1) (2.8)\n",
      "Requirement already satisfied: click in c:\\users\\hster\\anaconda3\\lib\\site-packages (from sacremoses->adapter-transformers==2.0.0a1) (7.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\hster\\anaconda3\\lib\\site-packages (from sacremoses->adapter-transformers==2.0.0a1) (0.14.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\hster\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/adapter-hub/adapter-transformers.git@v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mRmudIhk8OUv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'transformers' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting SST...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"transformers/utils/download_glue_data.py\", line 154, in <module>\n",
      "    sys.exit(main(sys.argv[1:]))\n",
      "  File \"transformers/utils/download_glue_data.py\", line 150, in main\n",
      "    download_and_extract(task, args.data_dir)\n",
      "  File \"transformers/utils/download_glue_data.py\", line 50, in download_and_extract\n",
      "    urllib.request.urlretrieve(TASK2PATH[task], data_file)\n",
      "  File \"C:\\Users\\hster\\Anaconda3\\lib\\urllib\\request.py\", line 247, in urlretrieve\n",
      "    with contextlib.closing(urlopen(url, data)) as fp:\n",
      "  File \"C:\\Users\\hster\\Anaconda3\\lib\\urllib\\request.py\", line 222, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "  File \"C:\\Users\\hster\\Anaconda3\\lib\\urllib\\request.py\", line 531, in open\n",
      "    response = meth(req, response)\n",
      "  File \"C:\\Users\\hster\\Anaconda3\\lib\\urllib\\request.py\", line 641, in http_response\n",
      "    'http', request, response, code, msg, hdrs)\n",
      "  File \"C:\\Users\\hster\\Anaconda3\\lib\\urllib\\request.py\", line 569, in error\n",
      "    return self._call_chain(*args)\n",
      "  File \"C:\\Users\\hster\\Anaconda3\\lib\\urllib\\request.py\", line 503, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"C:\\Users\\hster\\Anaconda3\\lib\\urllib\\request.py\", line 649, in http_error_default\n",
      "    raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "urllib.error.HTTPError: HTTP Error 403: Forbidden\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transformers\n",
    "!python transformers/utils/download_glue_data.py --tasks SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lbwb3NRf8mBF"
   },
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, EvalPrediction, GlueDataset, GlueDataTrainingArguments, AutoModelWithHeads, AdapterType\n",
    "from transformers import GlueDataTrainingArguments as DataTrainingArguments\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    glue_compute_metrics,\n",
    "    glue_tasks_num_labels,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ddUrpFOHItG"
   },
   "source": [
    "Training a new task adapter requires only few modifications compared to fully fine-tuning a model with Hugging Face's Trainer. We first configure the training and data arguments (which we would usually set via the command line):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "co43JhjxZ7lT"
   },
   "outputs": [],
   "source": [
    "model_name = \"roberta-base\"\n",
    "\n",
    "data_args = GlueDataTrainingArguments(task_name=\"sst-2\", data_dir=\"./glue_data/SST-2\")\n",
    "training_args = TrainingArguments(\n",
    "    logging_steps=1000, \n",
    "    per_device_train_batch_size=32, \n",
    "    per_device_eval_batch_size=64, \n",
    "    save_steps=1000,\n",
    "    output_dir=\"./models/sst-2\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    "    learning_rate=0.0001,\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "set_seed(training_args.seed)\n",
    "num_labels = glue_tasks_num_labels[data_args.task_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IyhBjgKbHgtz"
   },
   "source": [
    "We then load a pre-trained model (*roberta-base*) and add a new *sst-2* task adapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ugxk0RUP8gXx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelWithHeads.from_pretrained(model_name)\n",
    "model.add_adapter(\"sst-2\")\n",
    "model.train_adapter(\"sst-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M6vjtq3NHtxS"
   },
   "source": [
    "By calling `train_adapter([\"sst-2\"])` we freeze all transformer parameters except for the parameters of sst-2 adapter. Before training we add a new classification head to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YVqa_5dLHwGX"
   },
   "outputs": [],
   "source": [
    "model.add_classification_head(\"sst-2\", num_labels=num_labels)\n",
    "model.set_active_adapters(\"sst-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aG0cq4fCH6J3"
   },
   "source": [
    "The weights of this classification head can be stored together with the adapter weights to allow for a full reproducibility. The method call model.set_active_adapters([[\"sst-2\"]]) registers the sst-2 adapter as a default for training. This also supports adapter stacking and adapter fusion!\n",
    "\n",
    "We can then train our adapter using the Hugging Face Trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tXHpbKVwwiuz"
   },
   "outputs": [],
   "source": [
    "train_dataset = GlueDataset(data_args, tokenizer=tokenizer)\n",
    "eval_dataset = GlueDataset(data_args, tokenizer=tokenizer, mode=\"dev\")\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return glue_compute_metrics(data_args.task_name, preds, p.label_ids)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PUrMl0-XIQab"
   },
   "source": [
    "That's it! `model.save_all_adapters('output-path')` exports all adapters. Consider sharing your adapters on AdapterHub!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HieI-Svs0BPP"
   },
   "outputs": [],
   "source": [
    "model.save_all_adapters(\".\")\n",
    "!ls -l sst-2"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Adapter-Quickstart-Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
