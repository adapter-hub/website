<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>https://adapterhub.ml/blog/</id>
  <title>The AdapterHub Blog</title>
  <updated>2025-05-20T19:53:55.395349+00:00</updated>
  <link href="https://adapterhub.ml/blog/"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator>
  <subtitle>The latest news from AdapterHub</subtitle>
  <entry>
    <id>https://adapterhub.ml/blog/2020/11/adapting-transformers-with-adapterhub</id>
    <title>Adapting Transformers with AdapterHub</title>
    <updated>2020-11-17T00:00:00+00:00</updated>
    <author>
      <name>Clifton Poth</name>
    </author>
    <content type="html">&lt;p&gt;Transformer models pre-trained on massive amounts of text data and subsequently fine-tuned on target tasks have led to considerable advances in NLP, achieving state-of-the-art results across the board. 
However, models such as BERT (&lt;a href="https://arxiv.org/pdf/1810.04805.pdf"&gt;Devlin et al., 2019&lt;/a&gt;) and RoBERTa (&lt;a href="https://arxiv.org/pdf/1907.11692.pdf"&gt;Liu et al., 2019&lt;/a&gt;) consist of several millions of parameters, and thus, sharing and distributing fully fine-tuned models for each individual downstream task can be prohibitive. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Adapters&lt;/strong&gt; are a light-weight alternative to full model fine-tuning, consisting of only a tiny set of newly introduced parameters at every transformer layer.
Adapters overcome several limitations typically observed with full model fine-tuning:
they are &lt;strong&gt;parameter-efficient&lt;/strong&gt;, they &lt;strong&gt;speed up training iterations&lt;/strong&gt;, and they are &lt;strong&gt;shareable&lt;/strong&gt; and &lt;strong&gt;composable&lt;/strong&gt; due to their modularity and compact size.
Moreover, adapters usually perform &lt;strong&gt;on-par with state-of-the-art full fine-tuning&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;With multiple different adapter architectures and a wide variety of pre-trained transformers available, training, sharing and re-using adapters is not straightforward.
As a solution,  &lt;strong&gt;&lt;a href="https://arxiv.org/pdf/2007.07779.pdf"&gt;AdapterHub, A Framework for Adapting Transformers&lt;/a&gt;&lt;/strong&gt; provides a unified interface to different adapter architectures and composition techniques, making them widely accessible to the research community.
Built on top of &lt;a href="https://github.com/huggingface/transformers"&gt;Huggingface's Transformers framework&lt;/a&gt;, the AdapterHub has access to a large base of pre-trained transformers.
In the following, we will go through the process of training, sharing, and composing adapters with AdapterHub.&lt;/p&gt;
&lt;h2 id="a-short-introduction-to-adapters"&gt;A Short Introduction to Adapters&lt;/h2&gt;
&lt;figure id="_caption-1"&gt;
&lt;img alt="" src="/static/images/steps.gif" title="Steps of working with adapters" /&gt;
&lt;figcaption&gt;&lt;span&gt;Figure&amp;nbsp;1:&lt;/span&gt; Steps of working with adapters&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Adapters provide a lightweight alternative to fully fine-tuning a pre-trained language model on a downstream task.
For a transformer-based architecture, a small set of new parameters is introduced in every transformer layer.
While different adapter architectures are possible, a simple layout using a down- and an up-projection layer first introduced by &lt;a href="https://arxiv.org/pdf/1902.00751.pdf"&gt;Houlsby et al. (2020)&lt;/a&gt; has proven to work well (see Figure 1 for illustration).
In many cases, adapters perform on-par with fully fine-tuned models.&lt;/p&gt;
&lt;p&gt;During training on the target task, all weights of the pre-trained language model are kept fix.
The only weights to be updated are those introduced by the adapter modules.
This results in modular knowledge representations which subsequently can be easily extracted from the underlying language model.
The extracted adapter modules then can be distributed independently and plugged into a language model dynamically.
The encapsulated character of adapters also allows for easy exchange and composition of different adapters (&lt;a href="https://arxiv.org/pdf/2005.00247.pdf"&gt;Pfeiffer et al., 2020a&lt;/a&gt;).
Since this workflow of using adapters is very universal, it can potentially be applied to a wide range of different use cases.
As an example, adapters have been used successfully for zero-shot cross-lingual transfer between different tasks (&lt;a href="https://arxiv.org/pdf/2005.00052.pdf"&gt;Pfeiffer et al., 2020b&lt;/a&gt;).
Figure 1 illustrates the described adapter workflow.&lt;/p&gt;
&lt;figure id="_caption-2"&gt;
&lt;img alt="" src="/static/images/size_comparison.png" title="Size comparison of a fully fine-tuned model and an adapter" /&gt;
&lt;figcaption&gt;&lt;span&gt;Figure&amp;nbsp;2:&lt;/span&gt; Size comparison of a fully fine-tuned model and an adapter&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Using adapters provides various benefits, especially in parameter efficiency.
The amount of updated adapter parameters are only about 1% of the fully fine-tuned model and in many cases only requires a few Megabytes of storage space.
This makes it easy to share adapters, store adapters for many different tasks and load additional adapters on-the-fly.
Additionally, their compact size with the majority of weights bein frozen, makes adapters a computationally efficient fine-tuning choice (&lt;a href="https://arxiv.org/pdf/2010.11918.pdf"&gt;Rücklé et al., 2020&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id="what-is-adapterhub"&gt;What is AdapterHub?&lt;/h2&gt;
&lt;p&gt;With AdapterHub, we have developed a framework which makes working with adapters straightforward.
AdapterHub is divided into two core components: &lt;a href="https://github.com/Adapter-Hub/adapter-transformers"&gt;adapter-transformers&lt;/a&gt;, a library built on top of HuggingFace &lt;code&gt;transformers&lt;/code&gt; that integrates adapter support into various popular Transformer-based language models, and &lt;a href="https://adapterhub.ml/explore"&gt;the Hub&lt;/a&gt;, an open platform for sharing, exploring and consuming pre-trained adapters.&lt;/p&gt;
&lt;figure id="_caption-3"&gt;
&lt;img alt="" src="/static/images/lifecycle.png" title="The AdapterHub lifecycle" /&gt;
&lt;figcaption&gt;&lt;span&gt;Figure&amp;nbsp;3:&lt;/span&gt; The AdapterHub lifecycle&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Based on Figure 3, we'll go through the lifecycle of working with AdapterHub on a higher level:
HuggingFace &lt;code&gt;transformers&lt;/code&gt; (🤗) builds the backbone of our framework.
A user who wants to train an adapter (👩🏾‍💻) loads a pre-trained language model (🤖) from 🤗.
In ①, new adapter modules are introduced to the loaded language model.
Afterwards, 👩🏾‍💻 trains the adapter on a downstream task (②).
As soon as training has completed, 👩🏾‍💻 can extract the trained adapter weights from the (unaltered) 🤖 in ③.
👩🏾‍💻 packs the adapter weights and uploads them to the Hub.
Here, 👨🏼‍💻 can find the pre-trained adapter in step ④.
Together with downloading the matching 🤖 from 🤗, 👨🏼‍💻 then can download the adapter from the Hub and integrate it into his own model (⑤).
In ⑥, he lastly can apply 👩🏾‍💻's adapter for his own purposes.&lt;/p&gt;
&lt;p&gt;In the following, we will have a look at some of these steps in more detail.&lt;/p&gt;
&lt;h2 id="training-an-adapter"&gt;Training an Adapter&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/01_Adapter_Training.ipynb" target="_blank"&gt;
&lt;img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Training an adapter on a downstream task is a straightforward process using &lt;code&gt;adapter-transformers&lt;/code&gt; which can be installed via pip:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip install adapter-transformers
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This package is fully compatible with HuggingFace's &lt;code&gt;transformers&lt;/code&gt; library and can act as a drop-in replacement. Therefore, we can instantiate a pre-trained language model and tokenizer in the familiar way:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RobertaTokenizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;RobertaConfig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;RobertaModelWithHeads&lt;/span&gt;

&lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RobertaTokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;roberta-base&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RobertaConfig&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;roberta-base&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;num_labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;id2label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;👎&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;👍&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RobertaModelWithHeads&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;roberta-base&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;There is one difference compared to HuggingFace transformers in the code above:
We use the new class &lt;code&gt;RobertaModelWithHeads&lt;/code&gt; which allows a more flexible way of configuring prediction heads.&lt;/p&gt;
&lt;p&gt;The next steps configure our adapter setup. Note that these are the only lines additionally needed to switch from full fine-tuning to adapter training.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AdapterType&lt;/span&gt;

&lt;span class="c1"&gt;# Add a new adapter&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;rotten_tomatoes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AdapterType&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text_task&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Add a matching classification head&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_classification_head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;rotten_tomatoes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Activate the adapter&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;rotten_tomatoes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We add a new adapter to our model by calling &lt;code&gt;add_adapter()&lt;/code&gt;. We pass a name (&lt;code&gt;"rotten_tomatoes"&lt;/code&gt;) and &lt;a href="https://docs.adapterhub.ml/adapters.html#adapter-types"&gt;the type of adapter&lt;/a&gt; (task adapter). Next, we add a binary classification head. It's convenient to give the prediction head the same name as the adapter. This allows us to activate both together in the next step. The &lt;code&gt;train_adapter()&lt;/code&gt; method does two things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It freezes all weights of the pre-trained model so only the adapter weights are updated during training.&lt;/li&gt;
&lt;li&gt;It activates the adapter and the prediction head such that both are used in every forward pass.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;All the rest of the training process is identical to a full fine-tuning approach. Check out &lt;a href="https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/01_Adapter_Training.ipynb"&gt;the Colab notebook on adapter training&lt;/a&gt; to see the full code.&lt;/p&gt;
&lt;p&gt;In the end, the trained adapter can be exported to the file system using a single line of code:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;./final_adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;rotten_tomatoes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="interacting-with-the-hub"&gt;Interacting with the Hub&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/02_Adapter_Inference.ipynb" target="_blank"&gt;
&lt;img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The adapter weights trained in the previous section subsequently can be distributed via the Hub, the second core component of the AdapterHub framework.
The Hub infrastructure is based on plain YAML description files contributed to a central GitHub repository.
The full process of contributing pre-trained adapters is described &lt;a href="https://docs.adapterhub.ml/contributing.html"&gt;in our documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://adapterhub.ml/explore"&gt;Explore section&lt;/a&gt; of the AdapterHub website acts as the starting point for discovering and consuming available pre-trained adapters. A matching adapter can be selected by task domain, training dataset, model architecture and adapter architecture and loaded into &lt;code&gt;adapter-transformers&lt;/code&gt; in the following.&lt;/p&gt;
&lt;p&gt;Before loading the adapter, we instantiate the model we want to use, a pre-trained &lt;code&gt;bert-base-uncased&lt;/code&gt; model from HuggingFace.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AutoTokenizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AutoModelWithHeads&lt;/span&gt;

&lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoTokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;bert-base-uncased&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoModelWithHeads&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;bert-base-uncased&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Using &lt;code&gt;load_adapter()&lt;/code&gt;, we download and add a pre-trained adapter from the Hub. The first parameter specifies the name of the adapter whereas the second selects the &lt;a href="https://docs.adapterhub.ml/adapters.html#adapter-architectures"&gt;adapter architectures&lt;/a&gt; to search for.&lt;/p&gt;
&lt;p&gt;Also note that most adapters come with a prediction head included. Thus, this method will also load the question answering head trained together with the adapter.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;adapter_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;qa/squad1@ukp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;houlsby&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With &lt;code&gt;set_active_adapters()&lt;/code&gt; we tell our model to use the adapter we just loaded in every forward pass.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_active_adapters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;adapter_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Again, these are all changes needed to set up a pre-trained language model with a pre-trained adapter.
The rest of the inference is identical to a setup without adapters.
To see a full example, check out &lt;a href="https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/02_Adapter_Inference.ipynb"&gt;the Colab notebook for adapter inference&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="adapter-composition"&gt;Adapter Composition&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/03_Adapter_Fusion.ipynb" target="_blank"&gt;
&lt;img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As presented earlier, adapters are especially suitable for various kinds of compositions on a new target task.
One of these composition approaches is &lt;em&gt;AdapterFusion&lt;/em&gt; (&lt;a href="https://arxiv.org/pdf/2005.00247.pdf"&gt;Pfeiffer et al., 2020&lt;/a&gt;) which is also tightly integrated into AdapterHub.&lt;/p&gt;
&lt;p&gt;The knowledge learned by multiple pre-trained adapters from the Hub can be leveraged to solve a new target task.
In this setup, only a newly introduced fusion layer is trained while the rest of the model is kept fix.&lt;/p&gt;
&lt;p&gt;First, we load three adapters pre-trained on different tasks from the Hub: MultiNLI, QQP and QNLI. As we don't need their prediction heads, we pass &lt;code&gt;with_head=False&lt;/code&gt; to the loading method. Next, we add a new fusion layer that combines all the adapters we've just loaded. Finally, we add a new classification head for our target task on top.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AdapterType&lt;/span&gt;

&lt;span class="c1"&gt;# Load the pre-trained adapters we want to fuse&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;nli/multinli@ukp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AdapterType&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text_task&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;load_as&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;multinli&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;with_head&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;sts/qqp@ukp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AdapterType&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text_task&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;with_head&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;nli/qnli@ukp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AdapterType&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text_task&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;with_head&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Add a fusion layer for all loaded adapters&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_fusion&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;multinli&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;qqp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;qnli&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# Add a classification head for our target task&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_classification_head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;cb&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;id2label&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The last preparation step is to define and activate our adapter setup. Similar to &lt;code&gt;train_adapter()&lt;/code&gt;, &lt;code&gt;train_fusion()&lt;/code&gt; does two things: It freezes all weights of the model (including adapters!) except for the fusion layer and classification head. It also activates the given adapter setup to be used in very forward pass.&lt;/p&gt;
&lt;p&gt;The syntax for the adapter setup (which is also applied to other methods such as &lt;code&gt;set_active_adapters()&lt;/code&gt;) works as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a single string is interpreted as a single adapter&lt;/li&gt;
&lt;li&gt;a list of strings is interpreted as a &lt;strong&gt;stack&lt;/strong&gt; of adapters&lt;/li&gt;
&lt;li&gt;a &lt;em&gt;nested&lt;/em&gt; list of strings is interpreted as a &lt;strong&gt;fusion&lt;/strong&gt; of adapters&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Unfreeze and activate fusion setup&lt;/span&gt;
&lt;span class="n"&gt;adapter_setup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;multinli&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;qqp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;qnli&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_fusion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;adapter_setup&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;See the full training example in &lt;a href="https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/03_Adapter_Fusion.ipynb"&gt;the Colab notebook on AdapterFusion&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Adapters are a promising new approach to transfer learning in NLP, providing benefits in efficiency and modularity.
AdapterHub provides tools for the full lifecycle of interacting with adapters.
The integration into the successful HuggingFace &lt;code&gt;transformers&lt;/code&gt; framework makes it straightforward to adapt training setups to adapters.
AdapterHub is continuously evolving with the addition of adapter support to new models, the integration of new application scenarios for adapters and a growing platform of pre-trained adapter modules.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Devlin, J., Chang, M., Lee, K., &amp;amp; Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. NAACL-HLT.&lt;/li&gt;
&lt;li&gt;Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., Laroussilhe, Q.D., Gesmundo, A., Attariyan, M., &amp;amp; Gelly, S. (2019). Parameter-Efficient Transfer Learning for NLP. ICML.&lt;/li&gt;
&lt;li&gt;Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., &amp;amp; Stoyanov, V. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. ArXiv, abs/1907.11692.&lt;/li&gt;
&lt;li&gt;Pfeiffer, J., Kamath, A., Rücklé, A., Cho, K., &amp;amp; Gurevych, I. (2020). AdapterFusion: Non-Destructive Task Composition for Transfer Learning. ArXiv, abs/2005.00247.&lt;/li&gt;
&lt;li&gt;Pfeiffer, J., Vulic, I., Gurevych, I., &amp;amp; Ruder, S. (2020). MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer. ArXiv, abs/2005.00052.&lt;/li&gt;
&lt;li&gt;Rücklé, A., Geigle, G., Glockner, M., Beck, T., Pfeiffer, J., Reimers, N., &amp;amp; Gurevych, I. (2020). AdapterDrop: On the Efficiency of Adapters in Transformers. ArXiv, abs/2010.11918.&lt;/li&gt;
&lt;li&gt;Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., &amp;amp; Brew, J. (2019). HuggingFace's Transformers: State-of-the-art Natural Language Processing. ArXiv, abs/1910.03771.&lt;/li&gt;
&lt;/ul&gt;</content>
    <link href="https://adapterhub.ml/blog/2020/11/adapting-transformers-with-adapterhub" rel="alternate"/>
    <summary>Adapters are a new, efficient and composable alternative to full fine-tuning of pre-trained language models.
AdapterHub makes working with adapters accessible by providing a framework for training, sharing, discovering and consuming adapter modules.
This post provides an extensive overview.
</summary>
    <published>2020-11-17T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://adapterhub.ml/blog/2023/11/introducing-adapters</id>
    <title>Introducing Adapters</title>
    <updated>2023-11-24T00:00:00+00:00</updated>
    <author>
      <name>Hannah Sterz</name>
    </author>
    <author>
      <name>Leon Engländer</name>
    </author>
    <author>
      <name>Timo Imhof</name>
    </author>
    <author>
      <name>Clifton Poth,</name>
    </author>
    <author>
      <name>Jonas Pfeiffer</name>
    </author>
    <content type="html">&lt;p&gt;We are happy to announce &lt;em&gt;Adapters&lt;/em&gt;, the new library at the heart of the AdapterHub framework.
&lt;em&gt;Adapters&lt;/em&gt; stands in direct tradition to our previous work with the &lt;code&gt;adapter-transformers&lt;/code&gt; library while simultaneously revamping the implementation from the ground up and smoothing many rough edges of the previous library.
This blog post summarizes the most important aspects of &lt;em&gt;Adapters&lt;/em&gt;, as described in detail &lt;a href="https://arxiv.org/abs/2311.11077"&gt;in our paper&lt;/a&gt; (to be presented as a system demo at EMNLP 2023).&lt;/p&gt;
&lt;p&gt;In the summer of 2020, when we released the first version of &lt;em&gt;AdapterHub&lt;/em&gt;, along with the &lt;code&gt;adapter-transformers&lt;/code&gt; library, adapters and parameter-efficient fine-tuning&lt;sup id="fnref:peft"&gt;&lt;a class="footnote-ref" href="#fn:peft"&gt;1&lt;/a&gt;&lt;/sup&gt; were still a niche research topic.
Adapters were first introduced to Transformer models a few months earlier (Houlsby et al., 2019) and &lt;em&gt;AdapterHub&lt;/em&gt; was the very first framework to provide comprehensive tools for working with adapters, dramatically lowering the barrier of training own adapters or leveraging pre-trained ones.&lt;/p&gt;
&lt;p&gt;In the now more than three years following, &lt;em&gt;AdapterHub&lt;/em&gt; has increasingly gained traction within the NLP community, being &lt;a href="https://github.com/adapter-hub/adapters/stargazers"&gt;liked by thousands&lt;/a&gt; and &lt;a href="https://www.semanticscholar.org/paper/AdapterHub%3A-A-Framework-for-Adapting-Transformers-Pfeiffer-R%C3%BCckl%C3%A9/063f8b1ecf2394ca776ac61869734de9c1953808?utm_source=direct_link"&gt;used by hundreds&lt;/a&gt; for their research.
However, the field of parameter-efficient fine-tuning has grown even faster.
Nowadays, with recent LLMs growing ever larger in size, adapter methods, which do not fine-tune the full model, but instead only update a small number of parameters, have become increasingly mainstream.
&lt;a href="https://github.com/calpt/awesome-adapter-resources"&gt;Multiple libraries, dozens of architectures and scores of applications&lt;/a&gt; compose a flourishing subfield of LLM research.&lt;/p&gt;
&lt;p&gt;Besides parameter-efficiency, modularity is a second important characteristic of adapters &lt;a href="https://arxiv.org/pdf/2302.11529.pdf"&gt;(Pfeiffer et al., 2023)&lt;/a&gt;.
Sadly, this is overlooked by many existing tools.
From the beginning on, &lt;em&gt;AdapterHub&lt;/em&gt; paid special attention to adapter modularity and composition, integrating setups like MAD-X (Pfeiffer et al., 2020).
&lt;em&gt;Adapters&lt;/em&gt; continues and expands this focus on modularity.&lt;/p&gt;
&lt;h2 id="the-library"&gt;The Library&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Adapters&lt;/em&gt; is a self-contained library supporting a diverse set of adapter methods, integrating them into many common Transformer architectures and allowing flexible and complex adapter configuration.
Modular transfer learning can be achieved by combining adapters via six different composition blocks.&lt;/p&gt;
&lt;p&gt;All in all, &lt;em&gt;Adapters&lt;/em&gt; offers substantial improvements compared to the initial &lt;code&gt;adapter-transformers&lt;/code&gt; library:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Decoupled from the HuggingFace &lt;code&gt;transformers&lt;/code&gt; library&lt;/li&gt;
&lt;li&gt;Support of 10 adapter methods&lt;/li&gt;
&lt;li&gt;Support of 6 composition blocks&lt;/li&gt;
&lt;li&gt;Support of 20 diverse models&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;Adapters&lt;/em&gt; can be easily installed via pip:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip install adapters
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The source code of &lt;em&gt;Adapters&lt;/em&gt; can be found &lt;a href="https://github.com/adapter-hub/adapters"&gt;on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the following, we highlight important components of &lt;em&gt;Adapters&lt;/em&gt;.
If you have used &lt;code&gt;adapter-transformers&lt;/code&gt; before, much of this will look familiar.
In this case, you might directly want to jump to our &lt;a href="https://docs.adapterhub.ml/transitioning.html"&gt;transitioning guide&lt;/a&gt;, which highlights relevant differences between &lt;em&gt;Adapters&lt;/em&gt; and &lt;code&gt;adapter-transformers&lt;/code&gt;.
The additions and changes compared to the latest version of &lt;code&gt;adapter-transformers&lt;/code&gt; can also be found &lt;a href="https://github.com/adapter-hub/adapters/releases/tag/v0.1.0"&gt;in our release notes&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="transformers-integration"&gt;Transformers Integration&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Adapters&lt;/em&gt; acts as an add-on to HuggingFace's Transformers library.
As a result, existing Transformers models can be easily attached with adapter functionality as follows:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;adapters&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AutoModelForSequenceClassification&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoModelForSequenceClassification&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;t5-base&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;adapters&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# Adding adapter-specific functionality&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;adapter0&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;However, we recommend using the model classes provided by &lt;em&gt;Adapters&lt;/em&gt;, such as &lt;code&gt;XXXAdapterModel&lt;/code&gt;, where "XXX" denotes the model architecture, e.g., Bert.
These models provide the adapter functionality without further initialization and support multiple heads.
The latter is especially relevant when using composition blocks which can handle multiple outputs, for instance, the BatchSplit composition block. Here's an example of how to use such an &lt;code&gt;XXXAdapterModel&lt;/code&gt; class:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AutoAdapterModel&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoAdapterModel&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;roberta-base&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;adapter1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;seq_bn&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# add the new adapter to the model&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_classification_head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;adapter1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# add a sequence classification head&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;adapter1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# freeze the model weights and activate the adapter&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="adapter-methods"&gt;Adapter Methods&lt;/h2&gt;
&lt;p&gt;Each adapter method is defined by a configuration object or string, allowing for flexible customization of various adapter module properties, including placement, capacity, residual connections, initialization, etc. We distinguish between single methods consisting of one type of adapter module and complex methods consisting of multiple different adapter module types.&lt;/p&gt;
&lt;h3 id="single-methods"&gt;Single Methods&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Adapters&lt;/em&gt; supports single adapter methods that introduce parameters in new feed-forward modules such as bottleneck adapters (Houlsby et al., 2019), introduce prompts at different locations such as prefix tuning (Li and Liang, 2021), reparameterize existing modules such as LoRA (Hu et al., 2022) or re-scale their output representations such as (IA)³ (Liu et al., 2022). For more information, see our &lt;a href="https://docs.adapterhub.ml/methods.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All adapter methods can be added to a model by the unified &lt;code&gt;add_adapter()&lt;/code&gt; method, e.g.:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;adapter2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;seq_bn&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Alternatively, a config class, along with custom parameters:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;PrefixTuningConfig&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;adapter3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;PrefixTuningConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prefix_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The following table gives an overview of many currently supported single methods, along with their configuration class and configuration string&lt;sup id="fnref:brackets"&gt;&lt;a class="footnote-ref" href="#fn:brackets"&gt;2&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Identifier&lt;/th&gt;
&lt;th&gt;Configuration class&lt;/th&gt;
&lt;th&gt;More information&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;[double_]seq_bn&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[Double]SeqBnConfig()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://docs.adapterhub.ml/methods.html#bottleneck-adapters"&gt;Bottleneck Adapters&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;par_bn&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ParBnConfig()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://docs.adapterhub.ml/methods.html#bottleneck-adapters"&gt;Bottleneck Adapters&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;[double_]seq_bn_inv&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[DoubleSeq]BnInvConfig()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://docs.adapterhub.ml/methods.html#language-adapters---invertible-adapters"&gt;Invertible Adapters&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;compacter[++]&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Compacter[PlusPlus]Config()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://docs.adapterhub.ml/methods.html#compacter"&gt;Compacter&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;prefix_tuning&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;PrefixTuningConfig()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://docs.adapterhub.ml/methods.html#prefix-tuning"&gt;Prefix Tuning&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;lora&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;LoRAConfig()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://docs.adapterhub.ml/methods.html#lora"&gt;LoRA&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ia3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;IA3Config()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://docs.adapterhub.ml/methods.html#ia-3"&gt;IA³&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;mam&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;MAMConfig()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="method_combinations.html#mix-and-match-adapters"&gt;Mix-and-Match Adapters&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;unipelt&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;UniPELTConfig()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="method_combinations.html#unipelt"&gt;UniPELT&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;prompt_tuning&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;PromptTuningConfig()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://docs.adapterhub.ml/methods.html#prompt_tuning"&gt;Prompt Tuning&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For more details on all adapter methods, visit &lt;a href="https://docs.adapterhub.ml/methods.html"&gt;our documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="complex-methods"&gt;Complex Methods&lt;/h3&gt;
&lt;p&gt;While different efficient fine-tuning methods and configurations have often been proposed as standalone, combining them for joint training has proven to be beneficial (He et al., 2022; Mao et al., 2022). To make this process easier, Adapters provides the possibility to group multiple configuration instances using the &lt;code&gt;ConfigUnion&lt;/code&gt; class. This flexible mechanism allows easy integration of multiple complex methods proposed in the literature (as the two examples outlined below), as well as the construction of other, new complex configurations currently not available nor benchmarked in the literature (Zhou et al., 2023).&lt;/p&gt;
&lt;p&gt;Example: &lt;strong&gt;Mix-and-Match Adapters&lt;/strong&gt; (He et al., 2022) were proposed as a combination of Prefix-Tuning and parallel bottleneck adapters. Using &lt;code&gt;ConfigUnion&lt;/code&gt;, this method can be defined as:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ConfigUnion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PrefixTuningConfig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ParBnConfig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AutoAdapterModel&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoAdapterModel&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;microsoft/deberta-v3-base&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;adapter_config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ConfigUnion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;PrefixTuningConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prefix_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;ParBnConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reduction_factor&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;my_adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;adapter_config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;set_active&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Learn more about complex adapter configurations using &lt;code&gt;ConfigUnion&lt;/code&gt; &lt;a href="https://docs.adapterhub.ml/method_combinations.html"&gt;in our documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="modularity-and-composition-blocks"&gt;Modularity and Composition Blocks&lt;/h2&gt;
&lt;figure id="_caption-1"&gt;
&lt;img alt="" src="/static/images/composition.png" title="Composition Blocks" /&gt;
&lt;figcaption&gt;&lt;span&gt;Figure&amp;nbsp;1:&lt;/span&gt; Composition Blocks&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;While the modularity and composability aspect of adapters have seen increasing interest in research, existing open-source libraries (Mangrulkar et al., 2022; Hu et al., 2023a) have largely overlooked these aspects. Adapters makes adapter compositions a central and accessible part of working with adapters by enabling the definition of complex, composed adapter setups. We define a set of simple composition blocks that each capture a specific method of aggregating the functionality of multiple adapters. Each composition block class takes a sequence of adapter identifiers plus optional configuration as arguments. The defined adapter setup is then parsed at runtime by Adapters to allow for dynamic switching between adapters per forward pass. Above, the different composition blocks are illustrated.&lt;/p&gt;
&lt;p&gt;An example composition could look as follows:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;adapters.composition&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;ac&lt;/span&gt;

&lt;span class="c1"&gt;# ...&lt;/span&gt;

&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;mam&amp;quot;&lt;/span&gt; &lt;span class="c1"&gt;# mix-and-match adapters&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;b&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;c&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_active_adapters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ac&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Stack&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ac&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Parallel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;b&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;c&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;active_adapters&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# The active config is: Stack[a, Parallel[b, c]]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With this setup activated, inputs in each layer would first flow through adapter "a" before being forwarded though "b" and "c" in parallel.&lt;/p&gt;
&lt;p&gt;To learn more, check out &lt;a href="https://adapterhub.ml/blog/2021/04/version-2-of-adapterhub-released/"&gt;this blog post&lt;/a&gt; and &lt;a href="https://docs.adapterhub.ml/adapter_composition.html"&gt;our documentation&lt;/a&gt;. &lt;/p&gt;
&lt;h2 id="evaluating-adapter-performance"&gt;Evaluating Adapter Performance&lt;/h2&gt;
&lt;figure id="_caption-2"&gt;
&lt;img alt="" src="/static/images/eval_results.png" title="Performance of different adapter architectures overdiffernt tasks evaluated with the RoBERTa model." /&gt;
&lt;figcaption&gt;&lt;span&gt;Figure&amp;nbsp;2:&lt;/span&gt; Performance of different adapter architectures overdiffernt tasks evaluated with the RoBERTa model.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;In addition to the aforementioned ease of use, we show that the adapter methods offered by our library are performant across a range of settings. To this end, we conduct evaluations on the single adapter implementations made available by Adapters.&lt;/p&gt;
&lt;p&gt;Results are shown in Figure 2. The obvious takeaway from our evaluations is that all adapter implementations offered by our framework are competitive with full model fine-tuning, across all task classes. Approaches that offer more tunable hyper-parameters (and thus allow for easy scaling), such as Bottleneck adapters, LoRA, and Prefix Tuning predictably have the highest topline performance, often surpassing full fine-tuning. However, extremely parameter-frugal methods like (IA)3, which add &amp;lt; 0.005% of the parameters of the base model, also perform commendably and only fall short by a small fraction. Finally, the Compacter is the least volatile among the single methods, obtaining the lowest standard deviation between runs on the majority of tasks.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The field of adapter/ PEFT methods will continue to advance rapidly and gain importance.
Already today, various interesting and promising approaches are not yet covered by &lt;em&gt;AdapterHub&lt;/em&gt; and the &lt;em&gt;Adapters&lt;/em&gt; library.
However, &lt;em&gt;Adapters&lt;/em&gt; aims to provide a new solid foundation for research and application of adapters, upon which new and extended methods can be successively added in the future.
&lt;em&gt;Adapters&lt;/em&gt; has a clear focus on parameter-efficiency &lt;em&gt;and&lt;/em&gt; modularity of adapters and builds on the rich and successful history of &lt;em&gt;AdapterHub&lt;/em&gt; and &lt;code&gt;adapter-transformers&lt;/code&gt;.
In the end, integrating the latest great research into the library is a community effort, and we invite you to &lt;a href="https://docs.adapterhub.ml/contributing.html"&gt;contribute in one of many possible ways&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., &amp;amp; Neubig, G. (2021, October). Towards a Unified View of Parameter-Efficient Transfer Learning. In International Conference on Learning Representations.&lt;/li&gt;
&lt;li&gt;Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., Laroussilhe, Q.D., Gesmundo, A., Attariyan, M., &amp;amp; Gelly, S. (2019). Parameter-Efficient Transfer Learning for NLP. ICML.&lt;/li&gt;
&lt;li&gt;Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., &amp;amp; Chen, W. (2021, October). LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations.&lt;/li&gt;
&lt;li&gt;Li, X. L., &amp;amp; Liang, P. (2021, August). Prefix-Tuning: Optimizing Continuous Prompts for Generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (pp. 4582-4597).&lt;/li&gt;
&lt;li&gt;Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M., &amp;amp; Raffel, C. A. (2022). Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35, 1950-1965.&lt;/li&gt;
&lt;li&gt;Mao, Y., Mathias, L., Hou, R., Almahairi, A., Ma, H., Han, J., ... &amp;amp; Khabsa, M. (2022, May). UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 6253-6264).&lt;/li&gt;
&lt;li&gt;Pfeiffer, J., Vulic, I., Gurevych, I., &amp;amp; Ruder, S. (2020). MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer. ArXiv, abs/2005.00052.&lt;/li&gt;
&lt;li&gt;Pfeiffer, J., Ruder, S., Vulic, I., &amp;amp; Ponti, E. (2023). Modular Deep Learning. ArXiv, abs/2302.11529.&lt;/li&gt;
&lt;li&gt;Zhou, H., Wan, X., Vulić, I., &amp;amp; Korhonen, A. (2023). AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning. arXiv preprint arXiv:2301.12132.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nc"&gt;@inproceedings&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nl"&gt;poth-etal-2023-adapters&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;title&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;author&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;{Poth, Clifton  and&lt;/span&gt;
&lt;span class="s"&gt;      Sterz, Hannah  and&lt;/span&gt;
&lt;span class="s"&gt;      Paul, Indraneil  and&lt;/span&gt;
&lt;span class="s"&gt;      Purkayastha, Sukannya  and&lt;/span&gt;
&lt;span class="s"&gt;      Engl{\&amp;quot;a}nder, Leon  and&lt;/span&gt;
&lt;span class="s"&gt;      Imhof, Timo  and&lt;/span&gt;
&lt;span class="s"&gt;      Vuli{\&amp;#39;c}, Ivan  and&lt;/span&gt;
&lt;span class="s"&gt;      Ruder, Sebastian  and&lt;/span&gt;
&lt;span class="s"&gt;      Gurevych, Iryna  and&lt;/span&gt;
&lt;span class="s"&gt;      Pfeiffer, Jonas}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;booktitle&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;month&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;dec&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;year&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;2023&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;address&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Singapore&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;publisher&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Association for Computational Linguistics&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;url&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;https://aclanthology.org/2023.emnlp-demo.13&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;pages&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;149--160&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:peft"&gt;
&lt;p&gt;We use the terms &lt;em&gt;parameter-efficient fine-tuning (PEFT)&lt;/em&gt; and &lt;em&gt;adapter&lt;/em&gt; interchangeably throughout this post and in all of our documents.&amp;#160;&lt;a class="footnote-backref" href="#fnref:peft" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:brackets"&gt;
&lt;p&gt;Options for identifiers and classes are given in brackets.&amp;#160;&lt;a class="footnote-backref" href="#fnref:brackets" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content>
    <link href="https://adapterhub.ml/blog/2023/11/introducing-adapters" rel="alternate"/>
    <summary>Introducing the new Adapters library the new package that supports adding parameter-efficient fine-tuning methods on top of transformers models and composition to achieve modular setups.
</summary>
    <published>2023-11-24T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://adapterhub.ml/blog/2023/03/adapterhub-v3_2</id>
    <title>Updates in Adapter-Transformers v3.2</title>
    <updated>2023-03-03T00:00:00+00:00</updated>
    <author>
      <name>Hannah Sterz</name>
    </author>
    <author>
      <name>Clifton Poth</name>
    </author>
    <author>
      <name>Leon Engländer</name>
    </author>
    <content type="html">&lt;p&gt;Throughout the last months, we worked on improving the &lt;code&gt;adapter-transformers&lt;/code&gt; library and including new features. This includes support for new models like CLIP and BEiT, more flexible adapter configuration, and adapter composition for prefix-tuning. In the following, we describe the new features and updates in more detail.&lt;/p&gt;
&lt;p&gt;You can find version 3.2 of &lt;code&gt;adapter-transformers&lt;/code&gt; &lt;a href="https://github.com/Adapter-Hub/adapter-transformers"&gt;on GitHub&lt;/a&gt; or install it via pip:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip install -U adapter-transformers
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="support-for-adapter-configuration-strings"&gt;Support for adapter configuration strings&lt;/h2&gt;
&lt;p&gt;For running experiments at a large scale with varying hyperparameters, it can be annoying to set the correct hyperparameters whenever running the scripts. Now, you can configure the adapter with a string. In previous versions, it was possible to use one of the predefined configurations via a string e.g. &lt;code&gt;pfeiffer&lt;/code&gt;. From v.3.2 on it is possible to adapt parameters within the string as well.
To create a Pfeiffer adapter with reduction factor 16 you can now use &lt;code&gt;pfeiffer[reduction_factor=16]&lt;/code&gt;. This can also help run the example scripts. &lt;a href="https://docs.adapterhub.ml/overview.html#configuration-strings"&gt;Learn more&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="adapter-composition-for-prefix-tuning"&gt;Adapter Composition for Prefix Tuning&lt;/h2&gt;
&lt;figure id="_caption-1"&gt;
&lt;img alt="" src="/static/images/v3_2_prefix_stack.png" title="Illustration of composition for prefix tuning (Pfeiffer et al.)" /&gt;
&lt;figcaption&gt;&lt;span&gt;Figure&amp;nbsp;1:&lt;/span&gt; Illustration of composition for prefix tuning (Pfeiffer et al.)&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Parameter-effifient fine-tuning methods have proven to be modular. Combining multiple adapters can be beneficial for transfer learning across languages. In v.3.2 we add &lt;code&gt;Stack&lt;/code&gt;, &lt;code&gt;Parallel&lt;/code&gt; &amp;amp; &lt;code&gt;BatchSplit&lt;/code&gt; compositions to prefix tuning.
In previous &lt;code&gt;adapter-transformers&lt;/code&gt; versions, you could combine multiple bottleneck adapters. You could use them in parallel or stack them. Now, this is also possible for prefix tuning adapters. Add multiple prefixes to the same model to combine the functionality of multiple adapters (&lt;code&gt;Stack&lt;/code&gt;) or perform several tasks simultaneously (&lt;code&gt;Parallel&lt;/code&gt;, &lt;code&gt;BatchSplit&lt;/code&gt;). &lt;a href="https://docs.adapterhub.ml/adapter_composition.html#stack"&gt;Learn more&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="enable-parallel-sequence-generation-with-adapters"&gt;Enable parallel sequence generation with adapters&lt;/h2&gt;
&lt;p&gt;In v3.2 you can use the &lt;code&gt;Parallel&lt;/code&gt; block in combination with the &lt;code&gt;model.generate()&lt;/code&gt; method. This allows to generate text for multiple adapters simultaneously. As a result, generation can now be used in a multi task inference setup and generate text for multiple tasks within one forward pass. &lt;/p&gt;
&lt;h2 id="new-model-integrations"&gt;New model integrations&lt;/h2&gt;
&lt;p&gt;The new v3.2 of &lt;code&gt;adapter-transformers&lt;/code&gt; adds support for adapters for several new models: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BEiT &lt;/li&gt;
&lt;li&gt;GPT-J &lt;/li&gt;
&lt;li&gt;CLIP &lt;/li&gt;
&lt;li&gt;ALBERT &lt;/li&gt;
&lt;li&gt;BertGeneration &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="other-notable-changes"&gt;Other notable changes&lt;/h2&gt;
&lt;p&gt;⚠️ &lt;strong&gt;Breaking change&lt;/strong&gt;: The latest release removes the &lt;code&gt;MultiLingAdapterArguments&lt;/code&gt; class which was previously used to add adapter support to training scripts.
It is now recommended to use the &lt;a href="https://docs.adapterhub.ml/classes/adapter_training.html#transformers.adapters.training.setup_adapter_training"&gt;&lt;code&gt;AdapterArguments&lt;/code&gt;&lt;/a&gt; class and &lt;a href="https://docs.adapterhub.ml/classes/adapter_training.html#transformers.adapters.training.setup_adapter_training"&gt;&lt;code&gt;setup_adapter_training&lt;/code&gt;&lt;/a&gt; method instead. &lt;a href="https://docs.adapterhub.ml/training.html"&gt;Learn more&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Finally, version 3.2 of &lt;code&gt;adapter-transformers&lt;/code&gt; updates the underlying transformers version from v.4.23.1 to v4.26.1&lt;/p&gt;
&lt;h2 id="fixes"&gt;Fixes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fixes for GLUE &amp;amp; dependency parsing example script&lt;/li&gt;
&lt;li&gt;Fix access to shared parameters of compacter (e.g. during sequence generation) &lt;/li&gt;
&lt;li&gt;Fix reference to adapter configs in &lt;code&gt;T5EncoderModel&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fix DeBERTa prefix tuning with enabled relative attention &lt;/li&gt;
&lt;li&gt;Fix gating for prefix tuning layers &lt;/li&gt;
&lt;li&gt;Fix input to T5 adapter layers&lt;/li&gt;
&lt;li&gt;Fix AdapterTrainer hyperparameter tuning&lt;/li&gt;
&lt;li&gt;Move loading best adapter to AdapterTrainer class&lt;/li&gt;
&lt;li&gt;Make HuggingFace Hub Mixin work with newer utilities &lt;/li&gt;
&lt;li&gt;Only compute fusion reg loss if the fusion layer is trained &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Pfeiffer, J., Ruder, S., Vulic, I., &amp;amp; Ponti, E. (2023). Modular Deep Learning. ArXiv, abs/2302.11529.&lt;/li&gt;
&lt;/ul&gt;</content>
    <link href="https://adapterhub.ml/blog/2023/03/adapterhub-v3_2" rel="alternate"/>
    <summary>With the newest release of our adapter-transformers library, version 3.2, we add composition blocks for prefix tuning and adapters to several new models.</summary>
    <published>2023-03-03T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://adapterhub.ml/blog/2024/08/adapters-update-reft-qlora-merging-models</id>
    <title>Adapters Library Updates: ReFT, QLoRA, Merging, New Models &amp; Hub</title>
    <updated>2024-08-10T00:00:00+00:00</updated>
    <author>
      <name>Clifton Poth</name>
    </author>
    <author>
      <name>Leon Engländer</name>
    </author>
    <author>
      <name>Timo Imhof</name>
    </author>
    <author>
      <name>Hannah Sterz</name>
    </author>
    <author>
      <name>Jonas Pfeiffer</name>
    </author>
    <content type="html">&lt;p&gt;Nine months ago, &lt;a href="https://adapterhub.ml/blog/2023/11/introducing-adapters/"&gt;we released &lt;em&gt;Adapters&lt;/em&gt;&lt;/a&gt;, our new unified library for parameter-efficient and modular fine-tuning.
&lt;em&gt;Adapters&lt;/em&gt; stands in direct tradition to our work on &lt;code&gt;adapter-transformers&lt;/code&gt; since 2020, the first open-source library for parameter-efficient fine-tuning.
Since its initial release, &lt;em&gt;Adapters&lt;/em&gt; has received various updates, the newest being released today.
In this post, we'll go through some of the most exciting new features released today and in the last few months.
You can find the full list of changes in the latest release &lt;a href=""&gt;in our release notes&lt;/a&gt;.&lt;/p&gt;
&lt;div class="toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#representation-fine-tuning-reft"&gt;Representation Fine-Tuning (ReFT)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#adapter-merging"&gt;Adapter Merging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#quantized-training"&gt;Quantized Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#new-models"&gt;New Models&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#whisper"&gt;Whisper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#other-models"&gt;Other Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#hub-updates"&gt;Hub Updates&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;You can find &lt;em&gt;Adapters&lt;/em&gt; &lt;a href="https://github.com/Adapter-Hub/adapters"&gt;on GitHub&lt;/a&gt; or install it via pip:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip install -U adapters
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="representation-fine-tuning-reft"&gt;Representation Fine-Tuning (ReFT)&lt;/h2&gt;
&lt;div align="center"&gt;
&lt;figure text-align="center"&gt;
&lt;img src="/static/images/reft.jpg" height="200"&gt;
  &lt;figcaption text-align="center"&gt;
    Illustrations from the ReFT paper (Wu et al., 2024): Left: The general framework of applying ReFT interventions. Right: Visualization of LoReFT.
  &lt;/figcaption&gt;
 &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Representation Fine-Tuning (ReFT), proposed by &lt;a href="https://arxiv.org/pdf/2404.03592"&gt;Wu et al. (2024)&lt;/a&gt;, is a novel efficient adapter method.
It leverages so-called interventions to adapt the pre-trained representations of a language model.
Within the context of ReFT, these interventions can intuitively be thought of as adapter modules placed after each Transformer layer.
In the general form, an intervention function &lt;script type="math/tex"&gt;\Phi&lt;/script&gt; can thus be defined as follows:&lt;/p&gt;
&lt;p&gt;
&lt;script type="math/tex; mode=display"&gt;
\Phi(h) = h + R^T (W h + b - R h)
&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;Here, &lt;script type="math/tex"&gt;R \in \mathbb{R}^{r \times d}&lt;/script&gt; and &lt;script type="math/tex"&gt;W \in \mathbb{R}^{r \times d}&lt;/script&gt; are low-rank matrices of rank &lt;script type="math/tex"&gt;r&lt;/script&gt;.
&lt;script type="math/tex"&gt;h&lt;/script&gt; is the layer output hidden state at a single sequence position, i.e. interventions can be applied independently at each position.&lt;/p&gt;
&lt;p&gt;Based on this general form, the ReFT paper proposes multiple instantiations of ReFT methods supported by &lt;em&gt;Adapters&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;LoReFT&lt;/strong&gt; enforces orthogonality of rows in &lt;script type="math/tex"&gt;R&lt;/script&gt;. Defined via &lt;a href="adapters.LoReftConfig"&gt;&lt;code&gt;LoReftConfig&lt;/code&gt;&lt;/a&gt; or via the &lt;code&gt;orthogonality&lt;/code&gt; attribute as in the following example:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ReftConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;all&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prefix_positions&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;suffix_positions&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;orthogonality&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# equivalent to LoreftConfig()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;NoReFT&lt;/strong&gt; does not enforce orthogonality in &lt;script type="math/tex"&gt;R&lt;/script&gt;. Defined via &lt;a href="adapters.NoReftConfig"&gt;&lt;code&gt;NoReftConfig&lt;/code&gt;&lt;/a&gt; or equivalently:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ReftConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;all&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prefix_positions&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;suffix_positions&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;orthogonality&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# equivalent to NoReftConfig()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;DiReFT&lt;/strong&gt; does not enforce orthogonality in &lt;script type="math/tex"&gt;R&lt;/script&gt; and additionally removes subtraction of &lt;script type="math/tex"&gt;R h&lt;/script&gt; in the intervention, Defined via &lt;a href="adapters.DiReftConfig"&gt;&lt;code&gt;DiReftConfig&lt;/code&gt;&lt;/a&gt; or equivalently:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ReftConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;all&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prefix_positions&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;suffix_positions&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;orthogonality&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;subtract_projection&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# equivalent to DiReftConfig()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In addition, &lt;em&gt;Adapters&lt;/em&gt; supports configuring multiple hyperparameters tuned in the ReFT paper in &lt;code&gt;ReftConfig&lt;/code&gt;, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;prefix_positions&lt;/code&gt;: number of prefix positions&lt;/li&gt;
&lt;li&gt;&lt;code&gt;suffix_positions&lt;/code&gt;: number of suffix positions&lt;/li&gt;
&lt;li&gt;&lt;code&gt;layers&lt;/code&gt;: The layers to intervene on. This can either be &lt;code&gt;"all"&lt;/code&gt; or a list of layer ids&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tied_weights&lt;/code&gt;: whether to tie parameters between prefixes and suffixes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can use ReFT adapters exactly as any other adapter type in &lt;em&gt;Adapters&lt;/em&gt;:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AutoAdapterModel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LoReftConfig&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoAdapterModel&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;roberta-base&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LoReftConfig&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loreft_adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loreft_adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# add training loop ...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Learn more about training adapters &lt;a href="https://github.com/adapter-hub/adapters/blob/main/notebooks/01_Adapter_Training.ipynb"&gt;in this notebook&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="adapter-merging"&gt;Adapter Merging&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://colab.research.google.com/github/Adapter-Hub/adapters/blob/main/notebooks/06_Task_Arithmetics.ipynb"&gt;&lt;img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We've expanded support for adapter merging, enabling the efficient combination of trained adapters without additional fine-tuning. Merging multiple adapters into a new one allows for efficient domain, language and task transfer. Adapter Merging is a form of Task Arithmetics (&lt;a href="https://arxiv.org/abs/2212.04089"&gt;Ilharco et al., 2023&lt;/a&gt;; &lt;a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/299a08ee712d4752c890938da99a77c6-Abstract-Conference.html"&gt;Zhang et al., 2023&lt;/a&gt;) and hence also allows increasing or unlearning specific skills. All adapter methods support linear merging. For &lt;em&gt;N&lt;/em&gt; adapters with parameters &lt;script type="math/tex"&gt;\Phi_i&lt;/script&gt; the merged adapter parameters &lt;script type="math/tex"&gt;\Phi_{merged}&lt;/script&gt; are calculated as:&lt;/p&gt;
&lt;p&gt;
&lt;script type="math/tex; mode=display"&gt;
\Phi_{merged} = \sum_{i=0}^{N} \lambda_i \Phi_i
&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;Where &lt;script type="math/tex"&gt;\lambda_i&lt;/script&gt; is the weight for each adapter. Example usage:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;average_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;adapter_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;merged_adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;adapter_list&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;lora1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;lora2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;lora3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="c1"&gt;# these are the λ_i&lt;/span&gt;
    &lt;span class="n"&gt;combine_strategy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;linear&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;For LoRA adapters, &lt;a href="https://arxiv.org/abs/2311.09344"&gt;Chronopoulou et al. (2023)&lt;/a&gt; have shown that linear combination can work effectively. However, the parameters of the LoRA matrices are interdependent. Hence simple linear combination may not always yield optimal results. Therefore, we support two additional LoRA-specific merging strategies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;combine_strategy = "lora_linear_only_negate_b"&lt;/code&gt;: As proposed by &lt;a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/299a08ee712d4752c890938da99a77c6-Abstract-Conference.html"&gt;Zhang et al. (2023)&lt;/a&gt; this method only negates the B matrix for negative weights:
  &lt;script type="math/tex; mode=display"&gt;
  A_{merged} = \sum_{i=0}^{N} |\lambda_i| A_i,\\
  B_{merged} = \sum_{i=0}^{N} \lambda_i B_i
  &lt;/script&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;combine_strategy = "lora_delta_w_svd"&lt;/code&gt;: Merges the LoRA delta W matrices and then applies SVD to obtain new A and B matrices.
  &lt;script type="math/tex; mode=display"&gt;
  \Delta W_{new} = \sum_{i=0}^N \lambda_i \cdot (\Delta W_i),\\
  A_{new}, B_{new} = \text{SVD}(\Delta W_{new})
  &lt;/script&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example usage:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;average_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;adapter_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;lora_svd_merged&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;adapter_list&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;lora1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;lora2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;lora3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;combine_strategy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;lora_delta_w_svd&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;svd_rank&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# &amp;quot;lora_delta_w_svd&amp;quot; requires the &amp;quot;svd_rank&amp;quot; parameter, which determines the r (rank) of the resulting LoRA adapter after singular value decomposition (SVD)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="quantized-training"&gt;Quantized Training&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://colab.research.google.com/github/Adapter-Hub/adapters/blob/main/notebooks/QLoRA_Llama_Finetuning.ipynb"&gt;&lt;img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Quantization of model weights has become an important method for drastically reducing the memory footprint of recent large language models.
Quantizing parameters down to 8 bits or 4 bits (&lt;a href="https://arxiv.org/pdf/2212.09720"&gt;Dettmers &amp;amp; Zettlemoyer, 2023&lt;/a&gt;) have enabled running large models on limited hardware with minimal performance reduction.&lt;/p&gt;
&lt;p&gt;While initially limited to model inference, &lt;strong&gt;QLoRA&lt;/strong&gt; (&lt;a href="https://arxiv.org/pdf/2305.14314"&gt;Dettmers et al., 2023&lt;/a&gt;) has proposed combining model quantization with adapter training using LoRA.&lt;/p&gt;
&lt;p&gt;QLoRA combines several innovations to reduce the memory footprint while fine-tuning a large LM. In short:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;4-bit NormalFloat quantization&lt;/em&gt; reduces the size of the base model to 4 bits per parameter while optimizing for maximizing the retained information.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Double quantization&lt;/em&gt; additionally quantizes constants required for quantization for additional memory saving.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Paged optimizers&lt;/em&gt; offloads optimizer states into CPU memory when they don't fit into GPU memory and automatically reloads them when needed.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;LoRA training&lt;/em&gt; fine-tunes LoRA layers on the task while keeping the quantized base model weights fixes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Make sure to check out &lt;a href="https://arxiv.org/pdf/2305.14314"&gt;the paper&lt;/a&gt; for detailed explanations!
The figure below visualizes the key differences between full fine-tuning, LoRA and QLoRA:&lt;/p&gt;
&lt;div align="center"&gt;
&lt;figure text-align="center"&gt;
&lt;img src="/static/images/qlora.jpg"&gt;
  &lt;figcaption text-align="center"&gt;
    Illustration from the QLoRA paper (Dettmers et al., 2023) comparing full fine-tuning, LoRA and QLoRA.
  &lt;/figcaption&gt;
 &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Model quantization and paged optimizers are integrated to the Transformers library via the &lt;strong&gt;&lt;a href="https://github.com/TimDettmers/bitsandbytes"&gt;bitsandbytes library&lt;/a&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AutoModelForCausalLM&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;BitsAndBytesConfig&lt;/span&gt;

&lt;span class="c1"&gt;# Load 4-bit quantized model&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoModelForCausalLM&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;meta-llama/Meta-Llama-3-8B&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;device_map&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;auto&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;quantization_config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;BitsAndBytesConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;load_in_4bit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;bnb_4bit_quant_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;nf4&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;bnb_4bit_use_double_quant&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;bnb_4bit_compute_dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bfloat16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;torch_dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bfloat16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Since &lt;em&gt;Adapters&lt;/em&gt; v0.2.0, all adapter implementations integrate seamlessly with quantized models, e.g. for QLoRA:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;adapters&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LoRAConfig&lt;/span&gt;

&lt;span class="n"&gt;adapters&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LoRAConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;qlora&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;qlora&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This approach isn't limited to LoRA - you can easily swap out the adapter config here! You can not only train QLoRA, but also QBottleneck adapters, QPrefixTuning and more!&lt;/p&gt;
&lt;p&gt;For a full guide, check out our &lt;a href="https://github.com/Adapter-Hub/adapters/blob/main/notebooks/QLoRA_Llama_Finetuning.ipynb"&gt;Notebook tutorial for quantized fine-tuning of Llama&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="new-models"&gt;New Models&lt;/h2&gt;
&lt;h3 id="whisper"&gt;Whisper&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/Adapter-Hub/adapters/blob/main/notebooks/Adapter_Whisper_Audio_FineTuning.ipynb"&gt;&lt;img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /&gt;&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;With the support of Whisper, we introduce the first model in the adapters library to operate in the audio domain, posing a fundamental step towards making our library more diverse for various modalities. 
Whisper was originally presented by OpenAI in their paper &lt;a href="https://arxiv.org/abs/2212.04356"&gt;Robust Speech Recognition via Large-Scale Weak Supervision&lt;/a&gt; and is a state-of-the-art model for audio processing trained on 680.000 hours of unsupervised data. &lt;/p&gt;
&lt;p&gt;Our &lt;code&gt;WhisperAdapterModel&lt;/code&gt; builds on the standard encoder-decoder architecture of the Hugging Face Whisper implementation and supports all the methods listed below, as well as flexible adding and removing of heads.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;(Bottleneck)&lt;br&gt; Adapters&lt;/th&gt;
&lt;th&gt;Prefix&lt;br&gt; Tuning&lt;/th&gt;
&lt;th&gt;LoRA&lt;/th&gt;
&lt;th&gt;Compacter&lt;/th&gt;
&lt;th&gt;Adapter&lt;br&gt; Fusion&lt;/th&gt;
&lt;th&gt;Invertible&lt;br&gt; Adapters&lt;/th&gt;
&lt;th&gt;Parallel&lt;br&gt; block&lt;/th&gt;
&lt;th&gt;Prompt&lt;br&gt; Tuning&lt;/th&gt;
&lt;th&gt;ReFT&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://docs.adapterhub.ml/classes/models/whisper.html"&gt;Whisper&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;✅&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We also support enabling adapter capabilities for existing static head models of the classes &lt;code&gt;WhisperForConditionalGeneration&lt;/code&gt; and &lt;code&gt;WhisperForAudioClassification&lt;/code&gt; via the &lt;code&gt;init()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;Since Whisper processes audio, the audio data requires additional processing steps that are different from standard text processing. For more information on that, check out our &lt;a href="https://github.com/Adapter-Hub/adapters/blob/main/notebooks/Adapter_Whisper_Audio_FineTuning.ipynb"&gt;new notebook tutorial&lt;/a&gt; on how to finetune Whisper with LoRA for transcription.&lt;/p&gt;
&lt;h3 id="other-models"&gt;Other Models&lt;/h3&gt;
&lt;p&gt;Since our initial release, we have also added a bunch of other models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.adapterhub.ml/classes/models/mt5.html"&gt;MT5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.adapterhub.ml/classes/models/mistral.html"&gt;Mistral&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.adapterhub.ml/classes/models/plbart.html"&gt;PLBart&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Go check them out if you are interested!&lt;/p&gt;
&lt;h2 id="hub-updates"&gt;Hub Updates&lt;/h2&gt;
&lt;p&gt;Within the last few weeks, we have archived the "original" Hub repository (found at: &lt;a href="https://github.com/adapter-hub/Hub"&gt;Adapter-Hub/Hub&lt;/a&gt;) released alongside our initial AdapterHub release in 2020.
The Hub repository on GitHub is now in read-only mode, meaning no new adapters can be added there.&lt;/p&gt;
&lt;p&gt;It's recommended to upload all new adapters to the Hugging Face Model Hub, which will be the only supported Hub for &lt;em&gt;Adapters&lt;/em&gt; in the future (&lt;a href="https://docs.adapterhub.ml/huggingface_hub.html"&gt;Learn more&lt;/a&gt;). We have moved all ~300 publicly accessible adapters, including all of our original collection and most third-party contributions over to the Hugging Face Model Hub. Check out our Hugging Face Hub page at: &lt;a href="https://huggingface.co/AdapterHub"&gt;https://huggingface.co/AdapterHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In v1.0 of Adapters, attempting to load adapters from the original Hub repo will automatically redirect to loading the same adapter from the Hugging Face Model Hub.
There is no breaking change in loading an adapter ID, the same adapter weights will be loaded.
However, some parameters related to Hub loading and discovery have been deprecated or removed.
Learn more about breaking changes &lt;a href="https://github.com/adapter-hub/adapters/discussions/725"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you use &lt;em&gt;Adapters&lt;/em&gt; in your research, please cite:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nc"&gt;@inproceedings&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nl"&gt;poth-etal-2023-adapters&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;title&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;author&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;{Poth, Clifton  and&lt;/span&gt;
&lt;span class="s"&gt;      Sterz, Hannah  and&lt;/span&gt;
&lt;span class="s"&gt;      Paul, Indraneil  and&lt;/span&gt;
&lt;span class="s"&gt;      Purkayastha, Sukannya  and&lt;/span&gt;
&lt;span class="s"&gt;      Engl{\&amp;quot;a}nder, Leon  and&lt;/span&gt;
&lt;span class="s"&gt;      Imhof, Timo  and&lt;/span&gt;
&lt;span class="s"&gt;      Vuli{\&amp;#39;c}, Ivan  and&lt;/span&gt;
&lt;span class="s"&gt;      Ruder, Sebastian  and&lt;/span&gt;
&lt;span class="s"&gt;      Gurevych, Iryna  and&lt;/span&gt;
&lt;span class="s"&gt;      Pfeiffer, Jonas}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;booktitle&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;month&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;dec&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;year&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;2023&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;address&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Singapore&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;publisher&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Association for Computational Linguistics&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;url&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;https://aclanthology.org/2023.emnlp-demo.13&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;pages&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;149--160&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    <link href="https://adapterhub.ml/blog/2024/08/adapters-update-reft-qlora-merging-models" rel="alternate"/>
    <summary>Today we are releasing the newest updates in our Adapters library. This post summarizes new features in the latest release as well as selected new features since our initial release in Nov 2023, including new adapter methods, new supported models and Hub updates.
</summary>
    <published>2024-08-10T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://adapterhub.ml/blog/2022/09/updates-in-adapter-transformers-v3-1</id>
    <title>Updates in Adapter-Transformers v3.1</title>
    <updated>2022-09-15T00:00:00+00:00</updated>
    <author>
      <name>Clifton Poth</name>
    </author>
    <content type="html">&lt;figure id="_caption-1"&gt;
&lt;img alt="" src="/static/images/v3_1_methods.png" title="Illustration of efficient fine-tuning methods added in v3.1 of adapter-transformers." /&gt;
&lt;figcaption&gt;&lt;span&gt;Figure&amp;nbsp;1:&lt;/span&gt; Illustration of efficient fine-tuning methods added in v3.1 of adapter-transformers.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Throughout the last few months, the field of parameter-efficient methods for fine-tuning Transformer-based models has seen a wide range of new innovations, proposing new adapter methods (e.g. &lt;a href="https://arxiv.org/pdf/2110.04366.pdf"&gt;He et al., 2021&lt;/a&gt;; &lt;a href="https://doi.org/10.48550/arXiv.2205.05638"&gt;Liu et al., 2022&lt;/a&gt;) and applying them to new domains and tasks (e.g. &lt;a href="https://arxiv.org/pdf/2205.13535.pdf"&gt;Chen et al., 2022&lt;/a&gt;).
With the newest release of our &lt;code&gt;adapter-transformers&lt;/code&gt; library, version 3.1, we take a further step towards integrating the diverse possibilities of parameter-efficient fine-tuning methods by supporting multiple new adapter methods and Transformer architectures.&lt;/p&gt;
&lt;p&gt;In the following sections, we highlight important new features and methods introduced with the new release.
The full changelog can be found &lt;a href="https://github.com/adapter-hub/adapter-transformers/releases/tag/adapters3.1.0"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div class="toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#new-adapter-methods"&gt;New Adapter Methods&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#lora"&gt;LoRA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ia3"&gt;(IA)^3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#unipelt"&gt;UniPELT&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#further-updates"&gt;Further Updates&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#new-model-integrations"&gt;New model integrations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#adapter_summary-method"&gt;adapter_summary() method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#transformers-upgrade"&gt;Transformers upgrade&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#references"&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;You can find &lt;code&gt;adapter-transformers&lt;/code&gt; &lt;a href="https://github.com/Adapter-Hub/adapter-transformers"&gt;on GitHub&lt;/a&gt; or install it via pip:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip install -U adapter-transformers
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="new-adapter-methods"&gt;New Adapter Methods&lt;/h2&gt;
&lt;p&gt;With &lt;a href="https://adapterhub.ml/blog/2022/03/adapter-transformers-v3-unifying-efficient-fine-tuning/"&gt;the release of &lt;code&gt;adapter-transformers&lt;/code&gt; v3&lt;/a&gt; a few months back, we started the process of integrating new adapter methods.
The new release v3.1 adds three new works that were released throughout the last year, namely &lt;em&gt;LoRA&lt;/em&gt; (&lt;a href="https://arxiv.org/pdf/2106.09685.pdf"&gt;Hu et al., 2021&lt;/a&gt;), &lt;em&gt;UniPELT&lt;/em&gt; (&lt;a href="https://aclanthology.org/2022.acl-long.433.pdf"&gt;Mao et al., 2022&lt;/a&gt;) and &lt;em&gt;(IA)^3&lt;/em&gt; (&lt;a href="https://doi.org/10.48550/arXiv.2205.05638"&gt;Liu et al., 2022&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Previously, we have already integrated bottleneck adapters (&lt;a href="https://arxiv.org/pdf/1902.00751.pdf"&gt;Houlsby et al., 2019&lt;/a&gt;), Prefix Tuning (&lt;a href="https://aclanthology.org/2021.acl-long.353.pdf"&gt;Li and Liang, 2021&lt;/a&gt;), parallel adapters, Mix-and-Match adapters (&lt;a href="https://arxiv.org/pdf/2110.04366.pdf"&gt;He et al., 2021&lt;/a&gt;) and Compacters (&lt;a href="https://arxiv.org/pdf/2106.04647.pdf"&gt;Mahabadi et al., 2021&lt;/a&gt;).
For more on these methods, please refer &lt;a href="https://adapterhub.ml/blog/2022/03/adapter-transformers-v3-unifying-efficient-fine-tuning/"&gt;the blog post for the release of v3&lt;/a&gt;.
For a more general introduction to working with adapters, please refer to &lt;a href="https://docs.adapterhub.ml/quickstart.html"&gt;our documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The following table compares the performance of our implementation of LoRA, (IA)^3 and bottleneck adapters, which are described in more detail afterwards, on the GLUE benchmark.
We use &lt;code&gt;roberta-base&lt;/code&gt; as the base Transformer model and train for 20 epochs with learning rates of 1e-3, 1e-4 and 1e-4 for (IA)^3, LoRA and bottleneck adapters, respectively.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;(IA)^3&lt;/th&gt;
&lt;th&gt;LoRA&lt;/th&gt;
&lt;th&gt;Adapter (Houlsby)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;COLA&lt;/td&gt;
&lt;td&gt;Matthews Correlation&lt;/td&gt;
&lt;td&gt;59.53&lt;/td&gt;
&lt;td&gt;58.35&lt;/td&gt;
&lt;td&gt;59.81&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MNLI&lt;/td&gt;
&lt;td&gt;Accuracy&lt;/td&gt;
&lt;td&gt;85.98&lt;/td&gt;
&lt;td&gt;87.15&lt;/td&gt;
&lt;td&gt;86.68&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MRPC&lt;/td&gt;
&lt;td&gt;F1&lt;/td&gt;
&lt;td&gt;89.5&lt;/td&gt;
&lt;td&gt;90.63&lt;/td&gt;
&lt;td&gt;90.53&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;QNLI&lt;/td&gt;
&lt;td&gt;Accuracy&lt;/td&gt;
&lt;td&gt;91.75&lt;/td&gt;
&lt;td&gt;92.82&lt;/td&gt;
&lt;td&gt;92.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;QQP&lt;/td&gt;
&lt;td&gt;F1&lt;/td&gt;
&lt;td&gt;85.96&lt;/td&gt;
&lt;td&gt;86.57&lt;/td&gt;
&lt;td&gt;88.41&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RTE&lt;/td&gt;
&lt;td&gt;Accuracy&lt;/td&gt;
&lt;td&gt;73.41&lt;/td&gt;
&lt;td&gt;72.08&lt;/td&gt;
&lt;td&gt;77.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SST2&lt;/td&gt;
&lt;td&gt;Accuracy&lt;/td&gt;
&lt;td&gt;93.92&lt;/td&gt;
&lt;td&gt;94.11&lt;/td&gt;
&lt;td&gt;94.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;STSB&lt;/td&gt;
&lt;td&gt;Spearmanr&lt;/td&gt;
&lt;td&gt;89.78&lt;/td&gt;
&lt;td&gt;89.82&lt;/td&gt;
&lt;td&gt;90.58&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="lora"&gt;LoRA&lt;/h3&gt;
&lt;div align="center"&gt;
&lt;figure text-align="center"&gt;
&lt;img src="/static/images/lora.png" height="350"&gt;
  &lt;figcaption text-align="center"&gt;
    Figure 2: Illustration of the Low-Rank Adaptation (LoRA) method within one Transformer layer. Trained components are colored in shades of magenta.
  &lt;/figcaption&gt;
 &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Low-Rank Adaptation (LoRA) is an efficient fine-tuning technique proposed by &lt;a href="https://arxiv.org/pdf/2106.09685.pdf"&gt;Hu et al. (2021)&lt;/a&gt;.
LoRA injects trainable low-rank decomposition matrices into the layers of a pre-trained model.
For any model layer expressed as a matrix multiplication of the form &lt;script type="math/tex"&gt;h = W_0 x&lt;/script&gt;, it therefore performs a reparameterization, such that:&lt;/p&gt;
&lt;p&gt;
&lt;script type="math/tex; mode=display"&gt;
h = W_0 x + \frac{\alpha}{r} B A x
&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;Here, &lt;script type="math/tex"&gt;A \in \mathbb{R}^{r\times k}&lt;/script&gt; and &lt;script type="math/tex"&gt;B \in \mathbb{R}^{d\times r}&lt;/script&gt; are the decomposition matrices and &lt;script type="math/tex"&gt;r&lt;/script&gt;, the low-dimensional rank of the decomposition, is the most important hyperparameter.&lt;/p&gt;
&lt;p&gt;While, in principle, this reparameterization can be applied to any weights matrix in a model, the original paper only adapts the attention weights of the Transformer self-attention sub-layer with LoRA.
&lt;code&gt;adapter-transformers&lt;/code&gt; additionally allows injecting LoRA into the dense feed-forward layers in the intermediate and output components of a Transformer block.
You can configure the locations where LoRA weights should be injected using the attributes in the &lt;a href="transformers.LoRAConfig"&gt;&lt;code&gt;LoRAConfig&lt;/code&gt;&lt;/a&gt; class.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example&lt;/em&gt;:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers.adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LoRAConfig&lt;/span&gt;

&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LoRAConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;lora_adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the design of LoRA, Hu et al. (2021) also pay special attention to keeping the inference latency overhead compared to full fine-tuning at a minimum.
To accomplish this, the LoRA reparameterization can be merged with the original pre-trained weights of a model for inference.
Thus, the adapted weights are directly used in every forward pass without passing activations through an additional module.
In &lt;code&gt;adapter-transformers&lt;/code&gt;, this can be realized using the built-in &lt;code&gt;merge_adapter()&lt;/code&gt; method:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;merge_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;lora_adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To continue training on this LoRA adapter or to deactivate it entirely, the merged weights first have to be reset again:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;lora_adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id="ia3"&gt;(IA)^3&lt;/h3&gt;
&lt;div align="center"&gt;
&lt;figure text-align="center"&gt;
&lt;img src="/static/images/ia3.png" height="400"&gt;
  &lt;figcaption text-align="center"&gt;
    Figure 3: Illustration of the (IA)^3 method within one Transformer layer. Trained components are colored in shades of magenta.
  &lt;/figcaption&gt;
 &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Infused Adapter by Inhibiting and Amplifying Inner Activations ((IA)^3)&lt;/em&gt; is an efficient fine-tuning method proposed within the &lt;em&gt;T-Few&lt;/em&gt; fine-tuning approach by &lt;a href="https://arxiv.org/pdf/2205.05638.pdf"&gt;Liu et al. (2022)&lt;/a&gt;.
(IA)^3 introduces trainable vectors &lt;script type="math/tex"&gt;l_W&lt;/script&gt; into different components of a Transformer model which perform element-wise rescaling of inner model activations.
For any model layer expressed as a matrix multiplication of the form &lt;script type="math/tex"&gt;h = W x&lt;/script&gt;, it therefore performs an element-wise multiplication with &lt;script type="math/tex"&gt;l_W&lt;/script&gt;, such that:&lt;/p&gt;
&lt;p&gt;
&lt;script type="math/tex; mode=display"&gt;
h = l_W \odot W x
&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;Here, &lt;script type="math/tex"&gt;\odot&lt;/script&gt; denotes element-wise multiplication where the entries of &lt;script type="math/tex"&gt;l_W&lt;/script&gt; are broadcasted to the shape of &lt;script type="math/tex"&gt;W&lt;/script&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example&lt;/em&gt;:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers.adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;IA3Config&lt;/span&gt;

&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;IA3Config&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ia3_adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The implementation of (IA)^3, as well as the &lt;code&gt;IA3Config&lt;/code&gt; class, are derived from the implementation of &lt;a href="#lora"&gt;LoRA&lt;/a&gt;, with a few main modifications.
First, (IA)^3 uses multiplicative composition of weights instead of additive composition as in LoRA.
Second, the added weights are not further decomposed into low-rank matrices.
Both of these modifications are controlled via the &lt;code&gt;composition_mode&lt;/code&gt; configuration attribute by setting &lt;code&gt;composition_mode="scale"&lt;/code&gt;.
Additionally, as the added weights are already of rank 1, &lt;code&gt;r=1&lt;/code&gt; is set.&lt;/p&gt;
&lt;p&gt;Beyond that, both methods share the same configuration attributes that allow you to specify in which Transformer components rescaling vectors will be injected.
Following the original implementation, &lt;code&gt;IA3Config&lt;/code&gt; adds rescaling vectors to the self-attention weights (&lt;code&gt;selfattn_lora=True&lt;/code&gt;) and the final feed-forward layer (&lt;code&gt;output_lora=True&lt;/code&gt;).
Further, you can modify which matrices of the attention mechanism to rescale by leveraging the &lt;code&gt;attn_matrices&lt;/code&gt; attribute.
By default, (IA)^3 injects weights into the key ('k') and value ('v') matrices, but not in the query ('q') matrix.&lt;/p&gt;
&lt;p&gt;Finally, similar to LoRA, (IA)^3 also allows merging the injected parameters with the original weight matrices of the Transformer model.
E.g.:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Merge (IA)^3 adapter&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;merge_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ia3_adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Reset merged weights&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ia3_adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id="unipelt"&gt;UniPELT&lt;/h3&gt;
&lt;div align="center"&gt;
&lt;figure text-align="center"&gt;
&lt;img src="/static/images/unipelt.png" height="400"&gt;
  &lt;figcaption text-align="center"&gt;
    Figure 4: Illustration of the UniPELT method within one Transformer layer. Trained components are colored in shades of magenta.
  &lt;/figcaption&gt;
 &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;An approach similar to the work of &lt;a href="https://arxiv.org/pdf/2110.04366.pdf"&gt;He et al. (2021)&lt;/a&gt; is taken by &lt;a href="https://arxiv.org/pdf/2110.07577.pdf"&gt;Mao et al. (2022)&lt;/a&gt; in their &lt;em&gt;UniPELT&lt;/em&gt; framework.
They, too, combine multiple efficient fine-tuning methods, namely LoRA, Prefix Tuning and bottleneck adapters, in a single unified setup.
&lt;em&gt;UniPELT&lt;/em&gt; additionally introduces a gating mechanism that controls the activation of the different submodules.&lt;/p&gt;
&lt;p&gt;Concretely, for each adapted module &lt;script type="math/tex"&gt;m&lt;/script&gt;, UniPELT adds a trainable gating value &lt;script type="math/tex"&gt;\mathcal{G}_m \in (0, 1)&lt;/script&gt; that is computed via a feed-forward network (&lt;script type="math/tex"&gt;W_{\mathcal{G}_m}&lt;/script&gt;) and sigmoid activation (&lt;script type="math/tex"&gt;\sigma&lt;/script&gt;) from the Transformer layer input states (&lt;script type="math/tex"&gt;x&lt;/script&gt;):&lt;/p&gt;
&lt;p&gt;
&lt;script type="math/tex; mode=display"&gt;\mathcal{G}_m \leftarrow \sigma(W_{\mathcal{G}_m} \cdot x)&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;These gating values are then used to scale the output activations of the injected adapter modules, e.g. for a LoRA layer:&lt;/p&gt;
&lt;p&gt;
&lt;script type="math/tex; mode=display"&gt;
h \leftarrow W_0 x + \mathcal{G}_{LoRA} B A x
&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;In the configuration classes of &lt;code&gt;adapter-transformers&lt;/code&gt;, these gating mechanisms can be activated via &lt;code&gt;use_gating=True&lt;/code&gt;.
The full UniPELT setup can be instantiated using &lt;code&gt;UniPELTConfig&lt;/code&gt;&lt;sup id="fnref:unipelt"&gt;&lt;a class="footnote-ref" href="#fn:unipelt"&gt;1&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers.adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;UniPELTConfig&lt;/span&gt;

&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;UniPELTConfig&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;unipelt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;which is identical to the following &lt;code&gt;ConfigUnion&lt;/code&gt;:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers.adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ConfigUnion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LoRAConfig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PrefixTuningConfig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PfeifferConfig&lt;/span&gt;

&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ConfigUnion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;LoRAConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;use_gating&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;PrefixTuningConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prefix_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;use_gating&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;PfeifferConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reduction_factor&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;use_gating&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;unipelt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Finally, as the gating values for each adapter module might provide interesting insights for analysis, &lt;code&gt;adapter-transformers&lt;/code&gt; comes with an integrated mechanism of returning all gating values computed during a model forward pass via the &lt;code&gt;output_adapter_gating_scores&lt;/code&gt; parameter:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_adapter_gating_scores&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;gating_scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;adapter_gating_scores&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that this parameter is only available to base model classes and &lt;a href="prediction_heads.md#adaptermodel-classes"&gt;AdapterModel classes&lt;/a&gt;.
In the example, &lt;code&gt;gating_scores&lt;/code&gt; holds a dictionary of the following form:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;{
    &amp;#39;&amp;lt;adapter_name&amp;gt;&amp;#39;: {
        &amp;lt;layer_id&amp;gt;: {
            &amp;#39;&amp;lt;module_location&amp;gt;&amp;#39;: np.array([...]),
            ...
        },
        ...
    },
    ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The following table shows some initial results when running our UniPELT implementation. All adapter setups are trained for 20 epochs with a learning rate of 1e-4. Reported scores are accuracies &lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;UniPELT (ours)&lt;/th&gt;
&lt;th&gt;UniPELT (paper)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SST-2&lt;/td&gt;
&lt;td&gt;bert-base-uncased&lt;/td&gt;
&lt;td&gt;92.32&lt;/td&gt;
&lt;td&gt;91.51&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SST-2&lt;/td&gt;
&lt;td&gt;roberta-base&lt;/td&gt;
&lt;td&gt;94.61&lt;/td&gt;
&lt;td&gt;---&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MNLI&lt;/td&gt;
&lt;td&gt;bert-base-uncased&lt;/td&gt;
&lt;td&gt;84.53&lt;/td&gt;
&lt;td&gt;83.89&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MNLI&lt;/td&gt;
&lt;td&gt;roberta-base&lt;/td&gt;
&lt;td&gt;87.41&lt;/td&gt;
&lt;td&gt;---&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="further-updates"&gt;Further Updates&lt;/h2&gt;
&lt;h3 id="new-model-integrations"&gt;New model integrations&lt;/h3&gt;
&lt;p&gt;Version 3.1 adds adapter support to the DeBERTa and Vision Transformer (ViT) architectures already integrated into HuggingFace Transformers.&lt;/p&gt;
&lt;p&gt;The ViT integration is of particular interest as it opens the application area of our adapter implementations to the computer vision domains.
While most of the current work on adapter methods for Transformers happened in the NLP domain, adapters for Transformers in the vision domain have also been investigated recently (&lt;a href="https://arxiv.org/pdf/2203.16329.pdf"&gt;He et al., 2022&lt;/a&gt;; &lt;a href="https://arxiv.org/pdf/2205.13535.pdf"&gt;Chen et al., 2022&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Below, we show some initial results of our ViT integration, using &lt;code&gt;google/vit-base-patch16-224&lt;/code&gt; as the pre-trained base model:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Full FT&lt;/th&gt;
&lt;th&gt;Houlsby&lt;/th&gt;
&lt;th&gt;Pfeiffer&lt;/th&gt;
&lt;th&gt;LoRA&lt;/th&gt;
&lt;th&gt;Prefix Tuning&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;CIFAR-10&lt;/td&gt;
&lt;td&gt;98.88&lt;/td&gt;
&lt;td&gt;98.72&lt;/td&gt;
&lt;td&gt;99.09&lt;/td&gt;
&lt;td&gt;98.84&lt;/td&gt;
&lt;td&gt;98.76&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CIFAR-100&lt;/td&gt;
&lt;td&gt;92.08&lt;/td&gt;
&lt;td&gt;92.4&lt;/td&gt;
&lt;td&gt;92.08&lt;/td&gt;
&lt;td&gt;91.77&lt;/td&gt;
&lt;td&gt;91.76&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;All scores are accuracies on the evaluation set &lt;sup id="fnref2:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h3 id="adapter_summary-method"&gt;&lt;code&gt;adapter_summary()&lt;/code&gt; method&lt;/h3&gt;
&lt;p&gt;The new release adds an &lt;code&gt;adapter_summary()&lt;/code&gt; method that provides information on all adapters currently loaded into a base model in tabular form.
The method can be used as follows:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoAdapterModel&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;roberta-base&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;ADAPTER_CONFIG_MAP&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;adapter_summary&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;... which produces this output:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;================================================================================
Name                     Architecture         #Param      %Param  Active   Train
--------------------------------------------------------------------------------
pfeiffer                 bottleneck          894,528       0.718       0       1
houlsby                  bottleneck        1,789,056       1.435       0       1
pfeiffer+inv             bottleneck        1,190,592       0.955       0       1
houlsby+inv              bottleneck        2,085,120       1.673       0       1
compacter++              bottleneck           28,576       0.023       0       1
compacter                bottleneck           57,088       0.046       0       1
prefix_tuning            prefix_tuning     9,872,384       7.920       0       1
prefix_tuning_flat       prefix_tuning       552,960       0.444       0       1
parallel                 bottleneck        7,091,712       5.689       0       1
scaled_parallel          bottleneck        7,091,724       5.690       0       1
lora                     lora                294,912       0.237       0       1
ia3                      lora                 55,296       0.044       0       1
mam                      union            22,493,984      18.046       0       1
unipelt                  union            11,083,376       8.892       0       1
--------------------------------------------------------------------------------
Full model                               124,645,632     100.000               1
================================================================================
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id="transformers-upgrade"&gt;Transformers upgrade&lt;/h3&gt;
&lt;p&gt;Version 3.1 of &lt;code&gt;adapter-transformers&lt;/code&gt; upgrades the underlying HuggingFace Transformers library from v4.17.0 to v4.21.3, bringing many new features and bug fixes created by HuggingFace.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., &amp;amp; Chen, W. (2022). LoRA: Low-Rank Adaptation of Large Language Models. ArXiv, abs/2106.09685.&lt;/li&gt;
&lt;li&gt;Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M., &amp;amp; Raffel, C. (2022). Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning. ArXiv, abs/2205.05638.&lt;/li&gt;
&lt;li&gt;Mao, Y., Mathias, L., Hou, R., Almahairi, A., Ma, H., Han, J., Yih, W., &amp;amp; Khabsa, M. (2021). UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning. ArXiv, abs/2110.07577.&lt;/li&gt;
&lt;li&gt;He, X., Li, C., Zhang, P., Yang, J., &amp;amp; Wang, X. (2022). Parameter-efficient Fine-tuning for Vision Transformers. ArXiv, abs/2203.16329.&lt;/li&gt;
&lt;li&gt;Chen, S., Ge, C., Tong, Z., Wang, J., Song, Y., Wang, J., &amp;amp; Luo, P. (2022). AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition. ArXiv, abs/2205.13535.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:unipelt"&gt;
&lt;p&gt;Note that the implementation of UniPELT in &lt;code&gt;adapter-transformers&lt;/code&gt; follows the implementation in the original code, which is slighlty different from the description in the paper. See &lt;a href="https://github.com/morningmoni/UniPELT/issues/1"&gt;here&lt;/a&gt; for more.&amp;#160;&lt;a class="footnote-backref" href="#fnref:unipelt" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Reported results for &lt;code&gt;adapter-transformers&lt;/code&gt; only contain a single run each without hyperparameter tuning.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content>
    <link href="https://adapterhub.ml/blog/2022/09/updates-in-adapter-transformers-v3-1" rel="alternate"/>
    <summary>With the newest release of our adapter-transformers library, version 3.1, we take a further step towards integrating the diverse possibilities of parameter-efficient fine-tuning methods by supporting multiple new adapter methods and Transformer architectures.</summary>
    <published>2022-09-15T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://adapterhub.ml/blog/2022/03/adapter-transformers-v3-unifying-efficient-fine-tuning</id>
    <title>Adapter-Transformers v3 - Unifying Efficient Fine-Tuning</title>
    <updated>2022-03-21T00:00:00+00:00</updated>
    <author>
      <name>Clifton Poth</name>
    </author>
    <author>
      <name>Hannah Sterz</name>
    </author>
    <content type="html">&lt;figure id="_caption-1"&gt;
&lt;img alt="" src="/static/images/v3_methods.png" title="Illustration of efficient fine-tuning methods supported in v3 of adapter-transformers." /&gt;
&lt;figcaption&gt;&lt;span&gt;Figure&amp;nbsp;1:&lt;/span&gt; Illustration of efficient fine-tuning methods supported in v3 of adapter-transformers.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Since adapters were first introduced to NLP as a light-weight alternative to full fine-tuning of language models (&lt;a href="https://arxiv.org/pdf/1902.00751.pdf"&gt;Houlsby et al., 2019&lt;/a&gt;), the relevance of efficient transfer learning methods has continuously gained importance throughout the field.
With Transformer-based language models growing from millions to billions or trillions of parameters, the inherent advantages of methods such as adapters - parameter efficiency, computational efficiency and modularity - have only become even more relevant.
Nowadays, the tool set of efficient fine-tuning methods contains a diverse palette of different methods, ranging from improved adapter architectures (&lt;a href="https://arxiv.org/pdf/2106.04647.pdf"&gt;Mahabadi et al., 2021&lt;/a&gt;, &lt;a href="https://aclanthology.org/2021.emnlp-main.351/"&gt;Ribeiro et al., 2021&lt;/a&gt;) to various methods of optimizing language model prompts (&lt;a href="https://aclanthology.org/2021.acl-long.353.pdf"&gt;Li and Liang, 2021&lt;/a&gt;, &lt;a href="https://aclanthology.org/2021.emnlp-main.243/"&gt;Lester et al., 2021&lt;/a&gt;).
Recent work also has made attempts at combining multiple methods into a single unified architecture (&lt;a href="https://arxiv.org/pdf/2110.04366.pdf"&gt;He et al., 2021&lt;/a&gt;, &lt;a href="https://arxiv.org/pdf/2110.07577.pdf"&gt;Mao et al., 2021&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;With the release of version 3.0 of &lt;code&gt;adapter-transformers&lt;/code&gt; today, we're taking the first steps at embracing this grown and diversified landscape of efficient fine-tuning methods.
Our library, an extension of the great &lt;a href="https://huggingface.co/transformers/"&gt;Transformers library by HuggingFace&lt;/a&gt;, was introduced as a straightforward way to train, share, load and use adapters within Transformer models.
The new version for the first time allows using methods beyond the "classic" adapter architecture within this framework, namely Prefix Tuning, Parallel adapters, Mix-and-Match adapters and Compacters.&lt;/p&gt;
&lt;p&gt;In the following sections, we will present all new features and methods introduced with the new release as well as all important changes one by one:&lt;/p&gt;
&lt;div class="toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#new-efficient-fine-tuning-methods"&gt;New Efficient Fine-Tuning Methods&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#recap-bottleneck-adapters"&gt;Recap: Bottleneck Adapters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#prefix-tuning"&gt;Prefix Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#parallel-mix-and-match-adapters"&gt;Parallel &amp;amp; Mix-and-Match adapters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#compacters"&gt;Compacters&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#library-updates-and-changes"&gt;Library Updates and Changes&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#xadaptermodel-classes"&gt;XAdapterModel classes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#flexible-configurations-with-configunion"&gt;Flexible configurations with ConfigUnion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#adaptersetup-context"&gt;AdapterSetup context&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#refactorings"&gt;Refactorings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#transformers-upgrade"&gt;Transformers upgrade&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#references"&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;You can find &lt;code&gt;adapter-transformers&lt;/code&gt; &lt;a href="https://github.com/Adapter-Hub/adapter-transformers"&gt;on GitHub&lt;/a&gt; or install it via pip:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip install -U adapter-transformers
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="new-efficient-fine-tuning-methods"&gt;New Efficient Fine-Tuning Methods&lt;/h2&gt;
&lt;p&gt;Version 3.0 of &lt;code&gt;adapter-transformers&lt;/code&gt; integrates a first batch of new efficient fine-tuning methods.
These include Prefix Tuning (&lt;a href="https://aclanthology.org/2021.acl-long.353.pdf"&gt;Li and Liang, 2021&lt;/a&gt;), Parallel adapters, Mix-and-Match adapters (&lt;a href="https://arxiv.org/pdf/2110.04366.pdf"&gt;He et al., 2021&lt;/a&gt;) and Compacters (&lt;a href="https://arxiv.org/pdf/2106.04647.pdf"&gt;Mahabadi et al., 2021&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The newly added methods seamlessly integrate into the existing framework of working with adapters, i.e. they share the same methods for creation (&lt;code&gt;add_adapter()&lt;/code&gt;), training (&lt;code&gt;train_adapter()&lt;/code&gt;), saving (&lt;code&gt;save_adapter()&lt;/code&gt;) and loading (&lt;code&gt;load_adapter()&lt;/code&gt;).
Each method is specified and configured using a specific configuration class, all of which derive from the common &lt;code&gt;AdapterConfigBase&lt;/code&gt; class.
Please refer to &lt;a href="https://docs.adapterhub.ml/quickstart.html"&gt;our documentation&lt;/a&gt; for more explanation on working with adapters.&lt;/p&gt;
&lt;h3 id="recap-bottleneck-adapters"&gt;Recap: Bottleneck Adapters&lt;/h3&gt;
&lt;div align="center"&gt;
&lt;figure text-align="center"&gt;
  &lt;img src="/static/images/bottleneck.png"  height="400"&gt;
  &lt;figcaption&gt;Figure 2: The bottleneck adapter network consists of a linear down projection, non-linearity, and up projection, followed by a residual connection. It 
   is positioned after the multihead attention layer and/or the feedforward layer.&lt;/figcaption&gt;
&lt;/figure&gt; 
&lt;/div&gt;

&lt;p&gt;Until version 3.0 of &lt;code&gt;adapter-transformers&lt;/code&gt;, we only supported bottleneck adapters. As illustrated above, small stitched-in layers that 
consist of bottleneck feed-forward layers and a residual connection are added to the pre-trained transformer layers. These adapters are typically placed after the attention block and/or 
after the feedforward layer. For further detail check out our documentation for 
bottleneck adapters &lt;a href="https://docs.adapterhub.ml/overview"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="prefix-tuning"&gt;Prefix Tuning&lt;/h3&gt;
&lt;div align="center"&gt;
&lt;figure text-align="center"&gt;
&lt;img src="/static/images/prefix.png" height="400"&gt;
  &lt;figcaption text-align="center"&gt;
    Figure 3: Prefix Tuning adds trainable prefix vectors to the key and value matrices in the model.  
  &lt;/figcaption&gt;
 &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Prefix Tuning (&lt;a href="https://aclanthology.org/2021.acl-long.353.pdf"&gt;Li and Liang, 2021&lt;/a&gt;) introduces new parameters in the multi-head attention blocks in each Transformer layer. 
In the illustration above the prefixes are marked pink and purple. More, specifically, we prepend trainable prefix vectors &lt;script type="math/tex"&gt;P^K&lt;/script&gt; and &lt;script type="math/tex"&gt;P^V&lt;/script&gt; to the keys and values of the attention head input, each of a configurable prefix length &lt;script type="math/tex"&gt;l&lt;/script&gt; (&lt;code&gt;prefix_length&lt;/code&gt; attribute):&lt;/p&gt;
&lt;p&gt;
&lt;script type="math/tex; mode=display"&gt;
head_i = \text{Attention}(Q W_i^Q, [P_i^K, K W_i^K], [P_i^V, V W_i^V])
&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;Following the original authors, the prefix vectors in &lt;script type="math/tex"&gt;P^K&lt;/script&gt; and &lt;script type="math/tex"&gt;P^V&lt;/script&gt; are not optimized directly, but reparameterized via a bottleneck MLP.
This behavior is controlled via the &lt;code&gt;flat&lt;/code&gt; attribute of the configuration.
Using &lt;code&gt;PrefixTuningConfig(flat=True)&lt;/code&gt; will create prefix tuning vectors that are optimized without reparameterization.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example&lt;/em&gt;:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers.adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;PrefixTuningConfig&lt;/span&gt;

&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PrefixTuningConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flat&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prefix_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;prefix_tuning&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As reparameterization using the bottleneck MLP is not necessary for performing inference on an already trained Prefix Tuning module, adapter-transformers includes a function to "eject" a reparameterized Prefix Tuning into a flat one:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eject_prefix_tuning&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;prefix_tuning&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will only retain the necessary parameters and reduces the size of the trained Prefix Tuning.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Results&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;The following table compares initial runs of our Prefix Tuning implementation&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; with the results reported by &lt;a href="https://arxiv.org/pdf/2110.04366.pdf"&gt;He et al. (2021)&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Metrics&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Ours&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SST-2&lt;/td&gt;
&lt;td&gt;roberta-base&lt;/td&gt;
&lt;td&gt;Acc.&lt;/td&gt;
&lt;td&gt;94&lt;/td&gt;
&lt;td&gt;94.72&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MNLI&lt;/td&gt;
&lt;td&gt;roberta-base&lt;/td&gt;
&lt;td&gt;Acc.&lt;/td&gt;
&lt;td&gt;86.3&lt;/td&gt;
&lt;td&gt;86.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;XSum&lt;/td&gt;
&lt;td&gt;bart-large&lt;/td&gt;
&lt;td&gt;R-1/R-2/R-L&lt;/td&gt;
&lt;td&gt;43.40/20.46/35.51&lt;/td&gt;
&lt;td&gt;43.00/20.05/35.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;WMT16 En-Ro&lt;/td&gt;
&lt;td&gt;bart-large&lt;/td&gt;
&lt;td&gt;BLEU&lt;/td&gt;
&lt;td&gt;35.6&lt;/td&gt;
&lt;td&gt;35.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="parallel-mix-and-match-adapters"&gt;Parallel &amp;amp; Mix-and-Match adapters&lt;/h3&gt;
&lt;div align="center"&gt;
&lt;figure text-align="center"&gt;
&lt;img src="/static/images/parallel.png" height="400"&gt;
  &lt;figcaption text-align="center"&gt;
    Figure 4: The parallel adapter computes representations in parallel to the transformer sublayer. It does not receive the output of 
    the attention or feedforward layer, but instead processes the same input such that the adapter is parallel to the attention or feedforward layer. The respective representations are added subsequently.
  &lt;/figcaption&gt;
 &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Parallel adapters have been proposed as a variant of the classic bottleneck adapter architecture.
Here, activations are passed via the bottleneck adapter layer &lt;em&gt;in parallel&lt;/em&gt; to the adapted Transformer sub-layer (i.e. feed-forward or attention layer),
as opposed to the established, sequential, order of computations.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2110.04366.pdf"&gt;He et al. (2021)&lt;/a&gt; study various variants and combinations of efficient fine-tuning methods.
Among others, they propose &lt;em&gt;Mix-and-Match Adapters&lt;/em&gt; as a combination of Prefix Tuning and parallel adapters.
This configuration is supported by adapter-transformers out-of-the-box:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers.adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;MAMConfig&lt;/span&gt;

&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MAMConfig&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;mam_adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;and is identical to using the following &lt;code&gt;ConfigUnion&lt;/code&gt; (see further below for more on &lt;code&gt;ConfigUnion&lt;/code&gt;):&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers.adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ConfigUnion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ParallelConfig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PrefixTuningConfig&lt;/span&gt;

&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ConfigUnion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;PrefixTuningConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bottleneck_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;ParallelConfig&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;mam_adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Results&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;The following table compares initial runs of our Mix-and-Match adapter implementation&lt;sup id="fnref2:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; with the results reported by &lt;a href="https://arxiv.org/pdf/2110.04366.pdf"&gt;He et al. (2021)&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Metrics&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Ours&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SST-2&lt;/td&gt;
&lt;td&gt;roberta-base&lt;/td&gt;
&lt;td&gt;Acc.&lt;/td&gt;
&lt;td&gt;94.2&lt;/td&gt;
&lt;td&gt;94.26&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MNLI&lt;/td&gt;
&lt;td&gt;roberta-base&lt;/td&gt;
&lt;td&gt;Acc.&lt;/td&gt;
&lt;td&gt;87.4&lt;/td&gt;
&lt;td&gt;86.47&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;XSum&lt;/td&gt;
&lt;td&gt;bart-large&lt;/td&gt;
&lt;td&gt;R-1/R-2/R-L&lt;/td&gt;
&lt;td&gt;45.12/21.90/36.91&lt;/td&gt;
&lt;td&gt;44.74/21.75/36.80&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;WMT16 En-Ro&lt;/td&gt;
&lt;td&gt;bart-large&lt;/td&gt;
&lt;td&gt;BLEU&lt;/td&gt;
&lt;td&gt;37.5&lt;/td&gt;
&lt;td&gt;36.9&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Additionally, the next table shows initial runs of our parallel adapter implementation, again compared with the results reported by &lt;a href="https://arxiv.org/pdf/2110.04366.pdf"&gt;He et al. (2021)&lt;/a&gt; when applicable.
We use a reduction factor of 2 (corresponding to a bottleneck dimension of 384 for roberta-base and 512 for bart-large).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Metrics&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Ours&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SST-2&lt;/td&gt;
&lt;td&gt;roberta-base&lt;/td&gt;
&lt;td&gt;Acc.&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;94.61&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MNLI&lt;/td&gt;
&lt;td&gt;roberta-base&lt;/td&gt;
&lt;td&gt;Acc.&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;86.41&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;XSum&lt;/td&gt;
&lt;td&gt;bart-large&lt;/td&gt;
&lt;td&gt;R-1/R-2/R-L&lt;/td&gt;
&lt;td&gt;44.35/20.98/35.98&lt;/td&gt;
&lt;td&gt;44.88/21.53/36.55&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;WMT16 En-Ro&lt;/td&gt;
&lt;td&gt;bart-large&lt;/td&gt;
&lt;td&gt;BLEU&lt;/td&gt;
&lt;td&gt;37.1&lt;/td&gt;
&lt;td&gt;36.4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="compacters"&gt;Compacters&lt;/h3&gt;
&lt;div align="center"&gt;
&lt;figure text-align="center"&gt;
&lt;img src="/static/images/compacter.png" height="400"&gt;
  &lt;figcaption text-align="center"&gt;
    Figure 5: The compacter replaces the linear down and up projection of the bottleneck adapter with a phm layer. 
    The phm layer obtains its weights by computing the kronecker product of two smaller matrices.
  &lt;/figcaption&gt;
 &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Another alternative to the classical bottleneck adapter is the Compacter (&lt;a href="https://arxiv.org/pdf/2106.04647.pdf"&gt;Mahabadi et al. (2021)&lt;/a&gt;). Here the linear down- and up-projection layer is replaced by a phm layer, which is marked in 
black on the illustration. In the phm layer, the weights matrix is constructed from two smaller matrices by computing their kroenecker product. These matrices can be factorized and shared between all transformer layers.&lt;/p&gt;
&lt;p&gt;To add a Compacter in adapter-transformers, simply provide a &lt;code&gt;CompacterConfig&lt;/code&gt;or a &lt;code&gt;CompacterPlusPlusConfig&lt;/code&gt; when adding the adapter:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers.adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;CompacterPlusPlusConfig&lt;/span&gt;

&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;CompacterPlusPlusConfig&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;compacter_plusplus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The following table compares the results of training a Compacter++&lt;sup id="fnref3:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; for T5 for the glue tasks with the results reported in &lt;a href="https://arxiv.org/pdf/2106.04647.pdf"&gt;Mahabadi et al. (2021)&lt;/a&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Metrics&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Ours&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;COLA&lt;/td&gt;
&lt;td&gt;Mathews Correlation&lt;/td&gt;
&lt;td&gt;61.27&lt;/td&gt;
&lt;td&gt;58.45&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SST-2&lt;/td&gt;
&lt;td&gt;Acc.&lt;/td&gt;
&lt;td&gt;93.81&lt;/td&gt;
&lt;td&gt;94.61&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MRPC&lt;/td&gt;
&lt;td&gt;Acc./F1&lt;/td&gt;
&lt;td&gt;90.69/93.33&lt;/td&gt;
&lt;td&gt;87.99/91.81&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;QQP&lt;/td&gt;
&lt;td&gt;Acc./F1&lt;/td&gt;
&lt;td&gt;90.17/86.93&lt;/td&gt;
&lt;td&gt;90.33/87.46&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;STS-B&lt;/td&gt;
&lt;td&gt;Pearson/Spearman Correlation&lt;/td&gt;
&lt;td&gt;90.46/90.93&lt;/td&gt;
&lt;td&gt;89.78/89.53&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MNLI&lt;/td&gt;
&lt;td&gt;Acc.&lt;/td&gt;
&lt;td&gt;85.71&lt;/td&gt;
&lt;td&gt;85.32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;QNLI&lt;/td&gt;
&lt;td&gt;Acc.&lt;/td&gt;
&lt;td&gt;93.08&lt;/td&gt;
&lt;td&gt;91.63&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RTE&lt;/td&gt;
&lt;td&gt;Acc.&lt;/td&gt;
&lt;td&gt;74.82&lt;/td&gt;
&lt;td&gt;77.25&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="library-updates-and-changes"&gt;Library Updates and Changes&lt;/h2&gt;
&lt;p&gt;Below, we highlight further updates and changes introduced with v3.0 of &lt;code&gt;adapter-transformers&lt;/code&gt;.
You can find a full change log &lt;a href="https://github.com/Adapter-Hub/adapter-transformers/releases/tag/adapters3.0.0"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="xadaptermodel-classes"&gt;&lt;code&gt;XAdapterModel&lt;/code&gt; classes&lt;/h3&gt;
&lt;p&gt;Version 3.0 introduces a new set of model classes (one class per model type) specifically designed for working with adapters.
These classes follow the general schema &lt;code&gt;XAdapterModel&lt;/code&gt;, where &lt;code&gt;X&lt;/code&gt; is the respective model type (e.g. &lt;code&gt;Bert&lt;/code&gt;, &lt;code&gt;GPT2&lt;/code&gt;).
They replace the &lt;code&gt;XModelWithHeads&lt;/code&gt; classes of earlier versions.
In summary, these classes provide the following main features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Flexible configuration of predictions heads (see &lt;a href="https://docs.adapterhub.ml/prediction_heads.html#adaptermodel-classes"&gt;documentation&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Compositions (such as parallel inference and &lt;code&gt;BatchSplit&lt;/code&gt;) of adapters with different prediction heads.&lt;/li&gt;
&lt;li&gt;One model class per model type, additionally, a &lt;code&gt;AutoAdapterModel&lt;/code&gt; class for automatic class detection.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;These classes are designed as the new default classes of &lt;code&gt;adapter-transformers&lt;/code&gt;. It is recommended to use these classes for working with adapters whenever possible.&lt;/strong&gt;
A usage example looks like this:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers.adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AutoAdapterModel&lt;/span&gt;

&lt;span class="c1"&gt;# Load class&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoAdapterModel&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;bert-base-uncased&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Configure adapters &amp;amp; heads&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;first_task&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;second_task&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_classification_head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;first_task&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_multiple_choice_head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;second_task&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_choices&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Define active setup&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Parallel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;first_task&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;second_task&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# Start training loop ...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;⚠️ All &lt;code&gt;XModelWithHeads&lt;/code&gt; classes are now deprecated as the new classes are direct replacements.&lt;/p&gt;
&lt;h3 id="flexible-configurations-with-configunion"&gt;Flexible configurations with &lt;code&gt;ConfigUnion&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;While different efficient fine-tuning methods and configurations have often been proposed as standalone, it might be beneficial to combine them for joint training.
We have already seen this for the &lt;em&gt;Mix-and-Match&lt;/em&gt; adapters proposed by &lt;a href="https://arxiv.org/pdf/2110.04366.pdf"&gt;He et al. (2021)&lt;/a&gt;.
To make this process easier, adapter-transformers provides the possibility to group multiple configuration instances together using the &lt;code&gt;ConfigUnion&lt;/code&gt; class.&lt;/p&gt;
&lt;p&gt;For example, this could be used to define different reduction factors for the adapter modules placed after the multi-head attention and the feed-forward blocks:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers.adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AdapterConfig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ConfigUnion&lt;/span&gt;

&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ConfigUnion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;AdapterConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mh_adapter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_adapter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reduction_factor&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;non_linearity&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;relu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;AdapterConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mh_adapter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_adapter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reduction_factor&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;non_linearity&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;relu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;union_adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id="adaptersetup-context"&gt;&lt;code&gt;AdapterSetup&lt;/code&gt; context&lt;/h3&gt;
&lt;p&gt;As a replacement to the &lt;code&gt;adapter_names&lt;/code&gt; parameter, v3.0 introduces a new &lt;code&gt;AdapterSetup&lt;/code&gt; class for dynamic and state-less configuration of activated adapters.
This class is intended to be used as a context manager, i.e. a typical use case would look like this:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# will use no adapters&lt;/span&gt;
&lt;span class="n"&gt;outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;AdapterSetup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Stack&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;b&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="c1"&gt;# will use the adapter stack &amp;quot;a&amp;quot; and &amp;quot;b&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that in the above example &lt;strong&gt;no&lt;/strong&gt; adapters are activated via &lt;code&gt;active_adapters&lt;/code&gt;. Within the &lt;code&gt;with&lt;/code&gt; block, the adapter implementation will dynamically read the currently active setup from the context manager.&lt;/p&gt;
&lt;p&gt;This solution allows dynamic adapter activation, e.g. also with nesting:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;AdapterSetup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Stack&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;b&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="c1"&gt;# will use the adapter stack &amp;quot;a&amp;quot; and &amp;quot;b&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;AdapterSetup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Fuse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;c&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;d&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;head_setup&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;e&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# will use fusion between &amp;quot;c&amp;quot; and &amp;quot;d&amp;quot; &amp;amp; head &amp;quot;e&amp;quot;&lt;/span&gt;
        &lt;span class="n"&gt;outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Most importantly, the context manager is &lt;strong&gt;thread-local&lt;/strong&gt;, i.e. we can use different setups in different threads with the same model instance.&lt;/p&gt;
&lt;p&gt;⚠️ Breaking change: The &lt;code&gt;adapter_names&lt;/code&gt; parameter is removed for all model classes.&lt;/p&gt;
&lt;h3 id="refactorings"&gt;Refactorings&lt;/h3&gt;
&lt;p&gt;Besides the already mentioned changes, v3.0 of &lt;code&gt;adapter-transformers&lt;/code&gt; comes with major refactorings in the integration of adapter implementations into model classes and model configurations (e.g., see &lt;a href="https://github.com/Adapter-Hub/adapter-transformers/pull/263"&gt;here&lt;/a&gt; and &lt;a href="https://github.com/Adapter-Hub/adapter-transformers/pull/304"&gt;here&lt;/a&gt;).
While these refactorings only affect the interface methods minimally, the process of integrating new model architectures has been substantially simplified.
Please refer to the &lt;a href="https://github.com/Adapter-Hub/adapter-transformers/blob/master/adding_adapters_to_a_model.md"&gt;updated model integration guide&lt;/a&gt; for more.&lt;/p&gt;
&lt;h3 id="transformers-upgrade"&gt;Transformers upgrade&lt;/h3&gt;
&lt;p&gt;Version 3.0 of &lt;code&gt;adapter-transformers&lt;/code&gt; upgrades the underlying HuggingFace Transformers library from v4.12.5 to v4.17.0, bringing many awesome new features created by HuggingFace.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The release of version 3.0 of &lt;code&gt;adapter-transformers&lt;/code&gt; today marks the starting point of integrating new efficient fine-tuning methods.
In this release, we integrated a first batch of recently proposed methods, including Prefix Tuning, Parallel adapters, Mix-and-Match adapters and Compacters.
Nonetheless, the range of available efficient fine-tuning methods goes far beyond these and continues to grow rapidly.
Thus, we expect to integrate more and more methods step by step.&lt;/p&gt;
&lt;p&gt;Finally, as we're a very small team, your help on &lt;code&gt;adapter-transformers&lt;/code&gt; is always very welcome.
Head over to our &lt;a href="https://github.com/Adapter-Hub/adapter-transformers"&gt;GitHub repository&lt;/a&gt; and reach out if you're interested in contributing in any way.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., Laroussilhe, Q.D., Gesmundo, A., Attariyan, M., &amp;amp; Gelly, S. (2019). Parameter-Efficient Transfer Learning for NLP. ICML 2019.&lt;/li&gt;
&lt;li&gt;Mahabadi, R.K., Henderson, J., &amp;amp; Ruder, S. (2021). Compacter: Efficient Low-Rank Hypercomplex Adapter Layers. ArXiv, abs/2106.04647.&lt;/li&gt;
&lt;li&gt;Leonardo F. R. Ribeiro, Yue Zhang, and Iryna Gurevych. 2021. Structural Adapters in Pretrained Language Models for AMR-to-Text Generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4269–4282, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.&lt;/li&gt;
&lt;li&gt;Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597, Online. Association for Computational Linguistics.&lt;/li&gt;
&lt;li&gt;Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045–3059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.&lt;/li&gt;
&lt;li&gt;He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., &amp;amp; Neubig, G. (2021). Towards a Unified View of Parameter-Efficient Transfer Learning. ArXiv, abs/2110.04366.&lt;/li&gt;
&lt;li&gt;Mao, Y., Mathias, L., Hou, R., Almahairi, A., Ma, H., Han, J., Yih, W., &amp;amp; Khabsa, M. (2021). UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning. ArXiv, abs/2110.07577.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Reported results for &lt;code&gt;adapter-transformers&lt;/code&gt; only contain a single run each without hyperparameter tuning.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content>
    <link href="https://adapterhub.ml/blog/2022/03/adapter-transformers-v3-unifying-efficient-fine-tuning" rel="alternate"/>
    <summary>With the release of version 3.0 of adapter-transformers today, we're taking the first steps at integrating the grown and diversified landscape of efficient fine-tuning methods. Version 3.0 adds support for a first batch of recently proposed methods, including Prefix Tuning, Parallel adapters, Mix-and-Match adapters and Compacters. Further, improvements and changes to various aspects of the library are introduced.</summary>
    <published>2022-03-21T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://adapterhub.ml/blog/2025/05/adapters-for-any-transformer</id>
    <title>Adapters for Any Transformer On the HuggingFace Hub</title>
    <updated>2025-05-21T00:00:00+00:00</updated>
    <author>
      <name>The AdapterHub Team</name>
    </author>
    <content type="html">&lt;p&gt;In recent weeks and months, we've been working on greatly improving the integration of the &lt;em&gt;Adapters&lt;/em&gt; library with the Hugging Face ecosystem.
This has resulted in our &lt;a href="https://docs.adapterhub.ml/plugin_interface.html"&gt;new adapter plugin interface&lt;/a&gt;.
The plugin interface allows you to integrate most of the &lt;em&gt;Adapters&lt;/em&gt; library's features into nearly any Transformers model on the Hugging Face Hub with minimal effort.
In this post, we'll walk you through using the plugin interface step by step and also show what else is new in the &lt;em&gt;Adapters&lt;/em&gt; library.&lt;/p&gt;
&lt;div class="toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#adapters-for-any-transformer-with-plugin-interface"&gt;Adapters for Any Transformer with Plugin Interface&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#understanding-the-model-architecture"&gt;Understanding the Model Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#creating-the-plugin-interface"&gt;Creating the Plugin Interface&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#loading-the-model-and-initializing-with-the-interface"&gt;Loading the Model and Initializing with the Interface&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#adding-and-training-an-adapter"&gt;Adding and Training an Adapter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#loading-processing-the-gsm8k-dataset-for-fine-tuning"&gt;Loading &amp;amp; Processing the GSM8K Dataset for Fine-tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#fine-tuning-the-adapter"&gt;Fine-tuning the Adapter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#multi-task-learning-with-adapters"&gt;Multi-Task Learning with Adapters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#new-adapter-method-vera"&gt;New Adapter Method: VeRA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#summary"&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;You can find &lt;em&gt;Adapters&lt;/em&gt; &lt;a href="https://github.com/Adapter-Hub/adapters"&gt;on GitHub&lt;/a&gt; or install it via pip:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip install -U adapters
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="adapters-for-any-transformer-with-plugin-interface"&gt;Adapters for Any Transformer with Plugin Interface&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;As notebook: &lt;a href="https://colab.research.google.com/github/Adapter-Hub/adapters/blob/main/notebooks/Adapter_Interface_Qwen.ipynb"&gt;&lt;img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /&gt;&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In the following, we'll walk through adding adapter support to a custom or not pre-supported model with the &lt;em&gt;Adapters&lt;/em&gt; library's &lt;a href="https://docs.adapterhub.ml/plugin_interface.html"&gt;plugin interface&lt;/a&gt;. Specifically, we'll be writing a plugin interface for the Qwen 3 model and then train an adapter for mathematical reasoning.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; The interface below for Qwen 2 and Qwen 3 already comes pre-supported in &lt;em&gt;Adapters&lt;/em&gt;, so you could skip this section entirely! It's merely to showcase how you could define interfaces for your own custom models!
You can find a list of all pre-supported models &lt;a href="https://docs.adapterhub.ml/model_overview.html"&gt;in our docs&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="understanding-the-model-architecture"&gt;Understanding the Model Architecture&lt;/h3&gt;
&lt;p&gt;Before creating our plugin interface, let's understand the basic structure of Qwen 3:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Like most Transformer language models, it consists of an embedding layer followed by a series of decoder layers&lt;/li&gt;
&lt;li&gt;Each layer contains a self-attention block and an MLP block&lt;/li&gt;
&lt;li&gt;The self-attention block includes query, key, value, and output projections&lt;/li&gt;
&lt;li&gt;The MLP block includes multiple linear projections&lt;/li&gt;
&lt;li&gt;Qwen applies layer norms &lt;em&gt;before&lt;/em&gt; the self-attention and MLP blocks&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To create an adapter interface, we need to map these components to the appropriate adapter hooks.&lt;/p&gt;
&lt;h3 id="creating-the-plugin-interface"&gt;Creating the Plugin Interface&lt;/h3&gt;
&lt;p&gt;Now we'll create a plugin interface for Qwen 3 that maps the model's architecture to the adapter framework.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;adapters&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AdapterModelInterface&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AutoModelForCausalLM&lt;/span&gt;

&lt;span class="n"&gt;plugin_interface&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AdapterModelInterface&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="c1"&gt;# Specify which adapter methods to enable&lt;/span&gt;
    &lt;span class="n"&gt;adapter_methods&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;lora&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;reft&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;bottleneck&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;

    &lt;span class="c1"&gt;# Map the model&amp;#39;s components to the adapter interface&lt;/span&gt;
    &lt;span class="n"&gt;model_embeddings&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;embed_tokens&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;      &lt;span class="c1"&gt;# Embedding layer&lt;/span&gt;
    &lt;span class="n"&gt;model_layers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;layers&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;                &lt;span class="c1"&gt;# Transformer layers&lt;/span&gt;
    &lt;span class="n"&gt;layer_self_attn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;self_attn&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;          &lt;span class="c1"&gt;# Self-attention module in each layer&lt;/span&gt;
    &lt;span class="n"&gt;layer_cross_attn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;                &lt;span class="c1"&gt;# Qwen doesn&amp;#39;t have cross-attention&lt;/span&gt;

    &lt;span class="c1"&gt;# Projection matrices within the attention module&lt;/span&gt;
    &lt;span class="n"&gt;attn_k_proj&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;k_proj&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;                 &lt;span class="c1"&gt;# Key projection&lt;/span&gt;
    &lt;span class="n"&gt;attn_q_proj&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;q_proj&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;                 &lt;span class="c1"&gt;# Query projection&lt;/span&gt;
    &lt;span class="n"&gt;attn_v_proj&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;v_proj&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;                 &lt;span class="c1"&gt;# Value projection&lt;/span&gt;
    &lt;span class="n"&gt;attn_o_proj&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;o_proj&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;                 &lt;span class="c1"&gt;# Output projection&lt;/span&gt;

    &lt;span class="c1"&gt;# MLP projections&lt;/span&gt;
    &lt;span class="n"&gt;layer_intermediate_proj&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;mlp.up_proj&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# Up projection in MLP&lt;/span&gt;
    &lt;span class="n"&gt;layer_output_proj&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;mlp.down_proj&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;      &lt;span class="c1"&gt;# Downward projection in MLP&lt;/span&gt;

    &lt;span class="n"&gt;layer_pre_self_attn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;input_layernorm&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# Hook directly before self-attention&lt;/span&gt;
    &lt;span class="n"&gt;layer_pre_ffn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;post_attention_layernorm&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# Hook directly before MLP&lt;/span&gt;
    &lt;span class="c1"&gt;# Qwen applies layer norms before attention and MLP, so no need to add them here&lt;/span&gt;
    &lt;span class="n"&gt;layer_ln_1&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;layer_ln_2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Each parameter in the interface maps to specific module names in the model's architecture, allowing the adapter methods to hook into the right components.&lt;/p&gt;
&lt;h3 id="loading-the-model-and-initializing-with-the-interface"&gt;Loading the Model and Initializing with the Interface&lt;/h3&gt;
&lt;p&gt;Now, let's load the Qwen 3 model and initialize it with our plugin interface.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Load the model&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoModelForCausalLM&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Qwen/Qwen3-1.7B-Base&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# Using the 1.7B version&lt;/span&gt;
    &lt;span class="n"&gt;device_map&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;auto&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# Automatically distribute model across available GPUs&lt;/span&gt;
    &lt;span class="n"&gt;torch_dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;bfloat16&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# Use half-precision for faster computation&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AutoTokenizer&lt;/span&gt;

&lt;span class="c1"&gt;# Load the tokenizer&lt;/span&gt;
&lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoTokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Qwen/Qwen3-1.7B-Base&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Set the pad token ID to be different from the model&amp;#39;s EOS token&lt;/span&gt;
&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pad_token_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;151645&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pad_token_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pad_token_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Initialize the adapter framework with our plugin interface&lt;/span&gt;
&lt;span class="c1"&gt;# Remove the interface argument to use the default interface&lt;/span&gt;
&lt;span class="n"&gt;adapters&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;interface&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;plugin_interface&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id="adding-and-training-an-adapter"&gt;Adding and Training an Adapter&lt;/h3&gt;
&lt;p&gt;With the interface in place, we can now add an adapter to our model.
In this example, we'll train a &lt;a href="https://docs.adapterhub.ml/methods.html#bottleneck-adapters"&gt;bottleneck adapter&lt;/a&gt;. You can easily switch to &lt;a href="https://docs.adapterhub.ml/overview.html#table-of-adapter-methods"&gt;one of the other supported adapter methods&lt;/a&gt; (e.g. LoRA) by changing the &lt;code&gt;adapter_config&lt;/code&gt;.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SeqBnConfig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LoRAConfig&lt;/span&gt;

&lt;span class="c1"&gt;# Add a LoRA adapter&lt;/span&gt;
&lt;span class="n"&gt;adapter_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;qwen-math-adapter&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;adapter_config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SeqBnConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;reduction_factor&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# Bottleneck size&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Alternatively e.g.: &lt;/span&gt;
&lt;span class="c1"&gt;# adapter_config = LoRAConfig(&lt;/span&gt;
&lt;span class="c1"&gt;#     r=32,  # Rank of the low-rank decomposition&lt;/span&gt;
&lt;span class="c1"&gt;#     alpha=16,  # Scaling factor for LoRA&lt;/span&gt;
&lt;span class="c1"&gt;# )&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;adapter_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;adapter_config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Activate the adapter&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_active_adapters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;adapter_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Set the model to train only the adapter parameters&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;adapter_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Verify adapter was correctly added&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;adapter_summary&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id="loading-processing-the-gsm8k-dataset-for-fine-tuning"&gt;Loading &amp;amp; Processing the GSM8K Dataset for Fine-tuning&lt;/h3&gt;
&lt;p&gt;For this example, we'll use the GSM8K dataset to fine-tune our model for solving grade school math problems.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_dataset&lt;/span&gt;

&lt;span class="c1"&gt;# Load the GSM8K dataset&lt;/span&gt;
&lt;span class="n"&gt;dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_dataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;openai/gsm8k&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;main&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Explore sample data&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Sample question:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;train&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;question&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;Sample answer:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;train&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;answer&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We need to tokenize our math problems and their solutions for training.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;preprocess_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;examples&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# Create full prompts with question and expected answer format&lt;/span&gt;
    &lt;span class="n"&gt;prompts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
        &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Solve the following math problem step-by-step:&lt;/span&gt;&lt;span class="se"&gt;\n\n&lt;/span&gt;&lt;span class="s2"&gt;Question: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="se"&gt;\n\n&lt;/span&gt;&lt;span class="s2"&gt;Answer: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; &amp;lt;|endoftext|&amp;gt;&amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;examples&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;question&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;examples&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;answer&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="c1"&gt;# Tokenize as regular sequences&lt;/span&gt;
    &lt;span class="n"&gt;tokenized&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prompts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;max_length&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;truncation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2048&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# For causal language modeling, labels are the same as input_ids&lt;/span&gt;
    &lt;span class="n"&gt;tokenized&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;labels&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenized&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;input_ids&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tokenized&lt;/span&gt;

&lt;span class="c1"&gt;# Apply preprocessing to the dataset&lt;/span&gt;
&lt;span class="n"&gt;tokenized_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;preprocess_function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batched&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;remove_columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;question&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;answer&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Dataset processed!&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id="fine-tuning-the-adapter"&gt;Fine-tuning the Adapter&lt;/h3&gt;
&lt;p&gt;Now we can fine-tune our adapter for solving math problems.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TrainingArguments&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;


&lt;span class="c1"&gt;# Set up training arguments&lt;/span&gt;
&lt;span class="n"&gt;training_args&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TrainingArguments&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;output_dir&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;./qwen-math-adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;per_device_train_batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# Increase or decrease based on GPU memory&lt;/span&gt;
    &lt;span class="n"&gt;per_device_eval_batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e-4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;num_train_epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# More epochs for complex task&lt;/span&gt;
    &lt;span class="n"&gt;save_steps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;eval_steps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;logging_steps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;evaluation_strategy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;steps&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;load_best_model_at_end&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;metric_for_best_model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loss&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# Use loss as metric for best model&lt;/span&gt;
    &lt;span class="n"&gt;greater_is_better&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# Lower loss is better&lt;/span&gt;
    &lt;span class="n"&gt;gradient_accumulation_steps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# Accumulate gradients to simulate larger batch sizes&lt;/span&gt;
    &lt;span class="n"&gt;bf16&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# Use mixed precision&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Split dataset into train and validation&lt;/span&gt;
&lt;span class="c1"&gt;# Use a bugger/ smaller subset as needed&lt;/span&gt;
&lt;span class="n"&gt;train_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenized_dataset&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;train&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tokenized_dataset&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;train&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="mi"&gt;4000&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="n"&gt;eval_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenized_dataset&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;test&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tokenized_dataset&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;test&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Training on &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; examples and evaluating on &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;eval_dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; examples&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AdapterTrainer&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;trl&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DataCollatorForCompletionOnlyLM&lt;/span&gt;

&lt;span class="c1"&gt;# Initialize the trainer&lt;/span&gt;
&lt;span class="n"&gt;trainer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AdapterTrainer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;processing_class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;training_args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;data_collator&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;DataCollatorForCompletionOnlyLM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;response_template&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Answer:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;train_dataset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;eval_dataset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;eval_dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Train only the adapter parameters&lt;/span&gt;
&lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;After training, we can save just the adapter weights.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Save only the adapter weights&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;./qwen-math-adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;adapter_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Additionally, we can push our newly trained adapter to the Hugging Face Model Hub:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;push_adapter_to_hub&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;qwen-math-adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;adapter_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="multi-task-learning-with-adapters"&gt;Multi-Task Learning with Adapters&lt;/h2&gt;
&lt;p&gt;The &lt;em&gt;Adapters&lt;/em&gt; library has long supported multi-task learning methods such as &lt;a href="https://docs.adapterhub.ml/adapter_composition.html#fuse"&gt;AdapterFusion&lt;/a&gt;.
In v1.2.0, MTL-LoRA has been added as a new multi-task method for adapters.&lt;/p&gt;
&lt;p&gt;MTL-LoRA was introduced in "MTL-LoRA: Low-Rank Adaptation for Multi-Task Learning" (&lt;a href="https://arxiv.org/pdf/2410.09437"&gt;Yang et al., 2024&lt;/a&gt;) and enhances LoRA for multi-task learning (MTL) by improving task differentiation and knowledge sharing.
It introduces a task-specific low-rank learnable matrix &lt;script type="math/tex"&gt;\Lambda_t&lt;/script&gt; to better capture task-specific information and utilizes &lt;script type="math/tex"&gt;n&lt;/script&gt; low-rank up-projection matrices for diverse information-sharing. A weighted averaging mechanism integrates these matrices, allowing adaptive knowledge transfer across tasks. Specifically, the MTL-LoRA output for task &lt;script type="math/tex"&gt;t&lt;/script&gt; is formulated as:  &lt;/p&gt;
&lt;p&gt;
&lt;script type="math/tex; mode=display"&gt;
h_t = (W + \Delta W_t)x_t = Wx_t + \sum_{i=1}^n\frac{\text{exp}(w_t^i/\tau)B^i}{\sum_{j=1}^n\text{exp}(w_t^{j}/\tau)}\Lambda_t A x_t
&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;where &lt;script type="math/tex"&gt;\tau&lt;/script&gt; controls the sharpness of weight distribution. &lt;/p&gt;
&lt;p&gt;&lt;code&gt;MTL-LoRA&lt;/code&gt; is trainable with &lt;code&gt;MultiTask&lt;/code&gt; composition and a datasets wich contains &lt;code&gt;task_ids&lt;/code&gt; column (see. &lt;a href="https://docs.adapterhub.ml/adapter_composition.html#multitask"&gt;&lt;code&gt;MultiTask&lt;/code&gt; Composition&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example&lt;/em&gt;:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;MTLLoRAConfig&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;adapters.composition&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;ac&lt;/span&gt;

&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MTLLoRAConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_up_projection&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;i&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;k&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;l&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;share_parameters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;adapter_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;i&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;k&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;l&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;active_adapters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ac&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MultiTask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;i&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;k&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;l&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="new-adapter-method-vera"&gt;New Adapter Method: VeRA&lt;/h2&gt;
&lt;p&gt;Vera is a LoRA based fine-tuning method proposed by &lt;a href="https://arxiv.org/pdf/2310.11454"&gt;Kopiczko et al. (2024)&lt;/a&gt;. In Vera, we add frozen matrices A and B that are shared across all layers. It reduces the number of trainable parameters but maintains the same performance when compared to LoRA. Furthermore, trainable scaling vectors &lt;script type="math/tex"&gt;b&lt;/script&gt; and &lt;script type="math/tex"&gt;d&lt;/script&gt; are introduced and are multipled by the frozen matrices to result in the equation:&lt;/p&gt;
&lt;p&gt;
&lt;script type="math/tex; mode=display"&gt; h = W_{0}x + \Lambda_{b}B\Lambda_{d}Ax &lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;where &lt;script type="math/tex"&gt;\Lambda_{b}&lt;/script&gt; and &lt;script type="math/tex"&gt;\Lambda_{d}&lt;/script&gt; receive updates during training.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example&lt;/em&gt;:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;VeraConfig&lt;/span&gt;

&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;VeraConfig&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;vera_config&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;The latest Adapters library release introduces a powerful plugin interface that allows extending adapter functionality to virtually any Transformer model on the HuggingFace Hub with minimal effort.
This release also brings new multi-task learning capabilities through MTL-LoRA, and adds the parameter-efficient VeRA adapter method.
For the full list of changes, refer to &lt;a href="https://github.com/adapter-hub/adapters/releases/tag/v1.2.0"&gt;the release notes of v1.2.0&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you use &lt;em&gt;Adapters&lt;/em&gt; in your research, please cite:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nc"&gt;@inproceedings&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nl"&gt;poth-etal-2023-adapters&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;title&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;author&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;{Poth, Clifton  and&lt;/span&gt;
&lt;span class="s"&gt;      Sterz, Hannah  and&lt;/span&gt;
&lt;span class="s"&gt;      Paul, Indraneil  and&lt;/span&gt;
&lt;span class="s"&gt;      Purkayastha, Sukannya  and&lt;/span&gt;
&lt;span class="s"&gt;      Engl{\&amp;quot;a}nder, Leon  and&lt;/span&gt;
&lt;span class="s"&gt;      Imhof, Timo  and&lt;/span&gt;
&lt;span class="s"&gt;      Vuli{\&amp;#39;c}, Ivan  and&lt;/span&gt;
&lt;span class="s"&gt;      Ruder, Sebastian  and&lt;/span&gt;
&lt;span class="s"&gt;      Gurevych, Iryna  and&lt;/span&gt;
&lt;span class="s"&gt;      Pfeiffer, Jonas}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;booktitle&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;month&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;dec&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;year&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;2023&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;address&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Singapore&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;publisher&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Association for Computational Linguistics&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;url&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;https://aclanthology.org/2023.emnlp-demo.13&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;pages&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;149--160&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    <link href="https://adapterhub.ml/blog/2025/05/adapters-for-any-transformer" rel="alternate"/>
    <summary>The latest release of Adapters v1.2.0 introduces a new adapter plugin interface that enables adding adapter functionality to nearly any Transformer model.
We go through the details of working with this interface and various additional novelties of the library.
</summary>
    <published>2025-05-21T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://adapterhub.ml/blog/2021/04/adapters-for-generative-and-seq2seq-models-in-nlp</id>
    <title>Adapters for Generative and Seq2Seq Models in NLP</title>
    <updated>2021-04-29T00:00:00+00:00</updated>
    <author>
      <name>Hannah Sterz*</name>
    </author>
    <author>
      <name>Clifton Poth*</name>
    </author>
    <author>
      <name>Andreas Rücklé</name>
    </author>
    <author>
      <name>Jonas Pfeiffer</name>
    </author>
    <content type="html">&lt;p align="center"&gt;
&lt;img src="/static/images/BARTLogo.png"&gt;
&lt;/p&gt;

&lt;p&gt;Adapters are becoming more and more important in machine learning for NLP. For instance, they enable us to efficiently train and share new task-specific models. Adapters are small layers that are stitched into pre-trained transformer-based models. During training, only the parameters of the adapter layers are finetuned, while the parameters of the pre-trained model remain frozen. As a result, it is sufficient to only store the adapter layers instead of storing fully finetuned models separately for each task. Furthermore, the lower number of parameters requires less memory and makes it easier to share the trained adapters. Adapters also enable new possibilities in transfer learning. As adapters are encapsulated between frozen layers, they can be regarded as modular units which can be composed in a number of different ways (For more details and examples check out &lt;a href="https://adapterhub.ml/blog/2020/11/adapting-transformers-with-adapterhub/"&gt;this blog post&lt;/a&gt;). &lt;a href="https://www.aclweb.org/anthology/D19-1165.pdf"&gt;Bapna et al. (2019)&lt;/a&gt; have shown that adapters are useful for sequence to sequence tasks. On a neural machine translation task, they achieved similar results with adapters as compared to a fully finetuned model. The modularity aspect of adapters in zero-shot machine translation has recently been demonstrated by &lt;a href="https://www.aclweb.org/anthology/2020.emnlp-main.361.pdf"&gt;Philip et al. (2020)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The AdapterHub framework makes adapters easy to use. Up until now, the framework included adapters for the models BERT, RoBERTa, XML-RoBERTa and DistilBERT. In the new version 2.0, the framework now also provides adapters for the language generation models BART and GPT-2. This will allow researchers and engineers to use adapters for sequence-to-sequence tasks.&lt;/p&gt;
&lt;h2 id="results-of-bart-and-gpt-2-with-adapters"&gt;Results of BART and GPT-2 with adapters&lt;/h2&gt;
&lt;p&gt;Before we dive into generation tasks, we will take a look at the performance on the GLUE benchmark. We compare the scores of a fully finetuned model with the scores of adapter-based models, either using the adapter configuration of &lt;a href="https://arxiv.org/pdf/2005.00247.pdf"&gt;Pfeiffer et al. (2020a)&lt;/a&gt; or &lt;a href="https://arxiv.org/pdf/1902.00751.pdf"&gt;Houlsby et al. (2020)&lt;/a&gt;. The GPT-2 model and BART models achieve the following scores:&lt;/p&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;th&gt; GPT-2 &lt;/th&gt;&lt;th&gt; Full &lt;/th&gt;&lt;th&gt; Pfeiffer &lt;/th&gt;&lt;th&gt; Houlsby &lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; RTE &lt;/td&gt;&lt;td&gt; 65.0 &lt;/td&gt;&lt;td&gt; 67.1 &lt;/td&gt;&lt;td&gt; 67.5 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   MRPC  &lt;/td&gt;&lt;td&gt; 83.8 &lt;/td&gt;&lt;td&gt; 83.5 &lt;/td&gt;&lt;td&gt; 80.4 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   STS-B &lt;/td&gt;&lt;td&gt; 86.7 &lt;/td&gt;&lt;td&gt; 85.3 &lt;/td&gt;&lt;td&gt; 85.4 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   CoLA  &lt;/td&gt;&lt;td&gt; 33.6 &lt;/td&gt;&lt;td&gt; 43.0 &lt;/td&gt;&lt;td&gt; 41.2 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   SST-2 &lt;/td&gt;&lt;td&gt; 90.0 &lt;/td&gt;&lt;td&gt; 90.5 &lt;/td&gt;&lt;td&gt; 90.9 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   QNLI  &lt;/td&gt;&lt;td&gt; 87.6 &lt;/td&gt;&lt;td&gt; 88.2 &lt;/td&gt;&lt;td&gt; 88.5 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   MNLI  &lt;/td&gt;&lt;td&gt; 82.2 &lt;/td&gt;&lt;td&gt; 81.6 &lt;/td&gt;&lt;td&gt; 81.7 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   QQP   &lt;/td&gt;&lt;td&gt; 88.5 &lt;/td&gt;&lt;td&gt; 87.1 &lt;/td&gt;&lt;td&gt; 87.7 &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The fully finetuned GPT-2 model is trained for 4 epochs with a learning rate of 1e-4. The adapters are trained for 10 epochs with a learning rate of 1e-4.&lt;/p&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;th&gt; BART &lt;/th&gt;&lt;th&gt; Full &lt;/th&gt;&lt;th&gt; Pfeiffer &lt;/th&gt;&lt;th&gt; Houlsby &lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; RTE &lt;/td&gt;&lt;td&gt; 71.12 &lt;/td&gt;&lt;td&gt; 69.7 &lt;/td&gt;&lt;td&gt; 69.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   MRPC  &lt;/td&gt;&lt;td&gt; 87.5&lt;/td&gt;&lt;td&gt; 86.8 &lt;/td&gt;&lt;td&gt; 88.2 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   STS-B &lt;/td&gt;&lt;td&gt; 89.0 &lt;/td&gt;&lt;td&gt; 88.1 &lt;/td&gt;&lt;td&gt; 88.3 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   CoLA  &lt;/td&gt;&lt;td&gt; 46.6 &lt;/td&gt;&lt;td&gt; 46.1 &lt;/td&gt;&lt;td&gt; 45.6 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   SST-2 &lt;/td&gt;&lt;td&gt; 92.7 &lt;/td&gt;&lt;td&gt; 93.7 &lt;/td&gt;&lt;td&gt; 93.6 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   QNLI  &lt;/td&gt;&lt;td&gt; 91.6 &lt;/td&gt;&lt;td&gt; 92.2 &lt;/td&gt;&lt;td&gt; 93.6 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   MNLI  &lt;/td&gt;&lt;td&gt; 85.7 &lt;/td&gt;&lt;td&gt; 85.9 &lt;/td&gt;&lt;td&gt; 85.9 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   QQP   &lt;/td&gt;&lt;td&gt; 89.3 &lt;/td&gt;&lt;td&gt; 88.4 &lt;/td&gt;&lt;td&gt; 88.6 &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The fully-finetuned BART model is trained for 3 epochs with a learning rate of 4e-5. The adapters are trained with early stopping for a maximum of 15 epochs with a learning rate of 1e-4.&lt;/p&gt;
&lt;p&gt;The results of the adapters are comparable to those of the fully finetuned model. On some tasks such as SST-2, the adapters achieve a higher score than the fully finetuned model for GPT-2 and BART. This matches the results of other models with adapters. In general, we can use adapters instead of fully finetuning the model without a deterioration in downstream task performance. &lt;/p&gt;
&lt;p&gt;Now we will take a look at the scores for sequence-to-sequence tasks. We train a GPT-2 model on the task proposed by &lt;a href="https://arxiv.org/abs/2004.10404"&gt;Chen et al. (2020)&lt;/a&gt;. This task requires the model to learn to generate entailing sentences w.r.t. the input. For example,  given a table containing the release dates for an album, the model is provided with a template and and has the objective to fill in the blanks.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Template: [ENT] was released in 6 [ENT] in [ENT].&lt;/p&gt;
&lt;p&gt;Gold sentence: Black Ice was released in 6 Countries in 2008.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is not sufficient for the model to simply enter a number from the table; it needs to count all countries the album was released in, in 2008. We trained the GPT-2 model with small-sized GPT-2 vocabulary using maximum likelihood estimation. The results are given in the following table:&lt;/p&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;&lt;th&gt; BLEU-1 &lt;/th&gt;&lt;th&gt; BLEU-2 &lt;/th&gt;&lt;th&gt; BLEU-3 &lt;/th&gt;&lt;th&gt; Adv-Acc &lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; GPT-2  &lt;/td&gt;&lt;td&gt; 48.8 &lt;/td&gt;&lt;td&gt; 27.1 &lt;/td&gt;&lt;td&gt; 12.6 &lt;/td&gt;&lt;td&gt; 62.3 &lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;
&lt;td&gt; GPT-2 + Pfeiffer &lt;/td&gt;&lt;td&gt; 46.3 &lt;/td&gt;&lt;td&gt; 24.8 &lt;/td&gt;&lt;td&gt; 11.2 &lt;/td&gt;&lt;td&gt; 60.1 &lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;
&lt;td&gt; GPT-2 + Houlsby &lt;/td&gt;&lt;td&gt; 45.5 &lt;/td&gt;&lt;td&gt; 23.9 &lt;/td&gt;&lt;td&gt; 10.5 &lt;/td&gt;&lt;td&gt; 59.7 &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;We observe that the models with adapters achieve a competitive results to full model fine-tuning. However, adapters have several advantages over fully finetuning, e.g., shorter training times, they require less memory to be stored, and they can easily be shared.&lt;/p&gt;
&lt;p&gt;To test the BART model on sequence-to-sequence tasks, we evaluated the model on the CNN/Daily Mail dataset (&lt;a href="https://arxiv.org/pdf/1506.03340.pdf"&gt;Hermann et al. (2015)&lt;/a&gt;; &lt;a href="https://arxiv.org/pdf/1704.04368.pdf"&gt;See et al., 2017&lt;/a&gt;) and the extreme summary dataset (XSum) dataset (&lt;a href="https://arxiv.org/pdf/1808.08745.pdf"&gt;Narayan et al., 2018&lt;/a&gt;). Both tasks have the objective to summarize newspaper articles. The main difference is that XSum requires the model to output short one sentence summaries. The results of the fully finetuned BART model and the adapters are as follows:&lt;/p&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;&lt;th&gt; R1 &lt;/th&gt;&lt;th&gt; R2 &lt;/th&gt;&lt;th&gt; RL &lt;/th&gt;
&lt;/tr&gt;&lt;tr&gt;
&lt;td&gt; CNN/Daily mail &lt;/td&gt;&lt;td&gt; 44.16 &lt;/td&gt;&lt;td&gt; 21.28 &lt;/td&gt;&lt;td&gt; 40.90 &lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;
&lt;td&gt;CNN/Daily mail + Pfeiffer &lt;/td&gt;&lt;td&gt; 43.40 &lt;/td&gt;&lt;td&gt; 20.86 &lt;/td&gt;&lt;td&gt; 30.66 &lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;table&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;&lt;th&gt; R1 &lt;/th&gt;&lt;th&gt; R2 &lt;/th&gt;&lt;th&gt; RL &lt;/th&gt;
&lt;/tr&gt;&lt;tr&gt;
&lt;td&gt; XSum &lt;/td&gt;&lt;td&gt; 45.14 &lt;/td&gt;&lt;td&gt; 22.27 &lt;/td&gt;&lt;td&gt; 37.26 &lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;
&lt;td&gt; XSum + Pfeiffer &lt;/td&gt;&lt;td&gt; 43.56 &lt;/td&gt;&lt;td&gt; 20.56 &lt;/td&gt;&lt;td&gt; 35.56 &lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;
&lt;td&gt; XSum + Houlsby &lt;/td&gt;&lt;td&gt;44.03 &lt;/td&gt;&lt;td&gt; 20.90 &lt;/td&gt;&lt;td&gt; 36.01 &lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Similar to the GPT-2 model, the BART model achieves the highest score when it is fully fine-tuned. The models with adapters achieve slightly lower scores, further indicating that adapters might in general achieve slightly lower scores on sequence-to-sequence tasks. However, as previously stated, they have several other advantages.&lt;/p&gt;
&lt;p&gt;Version 2.0 of the AdapterHub framework opens up new possibilities such as experimenting with summarization and text generation tasks. Adapters for BART and GPT-2 enable us to tackle a wide variety of text generation tasks with adapters.&lt;/p&gt;
&lt;h2 id="hands-on-example-train-an-adapter-to-write-poems"&gt;Hands-on example: Train an adapter to write poems&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/06_Text_Generation.ipynb"&gt;&lt;img alt="Open Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /&gt;&lt;/a&gt; &lt;br&gt;
To illustrate how we can use adapters for text generation, we provide a hands-on example for training adapters within GPT-2 on a poem dataset by &lt;a href="https://arxiv.org/pdf/2011.02686.pdf"&gt;Sheng et al. (2020)&lt;/a&gt; and let it create novel poems. The dataset contains poems from the Gutenberg project. The full code is available in the corresponding colab notebook linked above. If you have read the previous blog post, this might look very familiar. First, we need to add our adapters.  This is easily done with just a few lines of code:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AutoModelForCausalLM&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoModelForCausalLM&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;gpt2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Add a new adapter&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;poem&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Activate the adapter for training&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;poem&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We have created the GPT-2 model and added an adapter with &lt;code&gt;add_adapter()&lt;/code&gt;. We only need to pass the name of the adapter &lt;code&gt;"poem"&lt;/code&gt;. After adding the new adapter, we call &lt;code&gt;train_adapter()&lt;/code&gt; and pass the name of our adapter. This does two things: Firstly, it freezes all parameters of the pre-trained model such that only the parameters of the adapter are updated during training. Secondly, it activates the adapter so that it is used in the forward pass. Next, we can train our model the same way we would without an adapter. In the end, we can save our trained adapter as follows.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;path/to/adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;poem&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We call &lt;code&gt;save_adapter()&lt;/code&gt; and provide the path to the directory where the adapter should be saved and the name of the adapter we want to save.
Now that we have our trained adapter, we want to generate some poems and see what it has learned. First, we need to create a model with a language modeling head and load our trained adapter. Then we activate the loaded adapter.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GPT2LMHeadModel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;GPT2Tokenizer&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GPT2LMHeadModel&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;gpt2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;path/to/adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_active_adapters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;poem&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With &lt;code&gt;load_adapter()&lt;/code&gt; we can load an adapter from the Hub by passing the name of the adapter specified in the hub. We can also load a local adapter by providing the path to the adapter. Then, we activate our adapter such that is used in the forward pass with &lt;code&gt;set_active_adapters()&lt;/code&gt;.
Finally, we can think of a beginning of a poem and let the model finish it. In this case, the model generates 5 poems for the given beginning. We can choose the one we like most from those. We choose to start our poem with "In the night". One of the poems our model generated was:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In the night;&lt;br /&gt;
when the stars shine on her head.&lt;br /&gt;
the mounds are deep,&lt;br /&gt;
and the water's dark,&lt;br /&gt;
and the water's cold&lt;br /&gt;
and with her hand,&lt;br /&gt;
with her lips,&lt;br /&gt;
in song and song,&lt;br /&gt;
the sound of the birds&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This can easily be applied to other datasets. Feel free to train your own adapter and upload it at the &lt;a href="https://adapterhub.ml/"&gt;Hub&lt;/a&gt; or browse the adapters trained by the community.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The new version 2.0 of the AdapterHub framework supports adapters for GPT-2 and BART. The support of these two models offers new possibilities in solving sequence to sequence tasks with adapters. To checkout AdapterHub and its other features, visit us on &lt;a href="https://github.com/Adapter-Hub/adapter-transformers"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="acknowledgements"&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;We thank &lt;a href="https://www.behance.net/andrefellenberg"&gt;André Fellenberg&lt;/a&gt; for the BART illustration.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bapna, A., Arivazhagan, N., &amp;amp; Firat, O. (2019). Simple, scalable adaptation for neural machine translation. EMNLP 2019, &lt;a href="https://www.aclweb.org/anthology/D19-1165.pdf"&gt;https://www.aclweb.org/anthology/D19-1165.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Chen, W., Chen, J., Su, Y., Chen, Z., &amp;amp; Wang, W. Y. (2020). Logical natural language generation from open-domain tables. ACL 2020, &lt;a href="https://www.aclweb.org/anthology/2020.acl-main.708.pdf"&gt;https://www.aclweb.org/anthology/2020.acl-main.708.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hermann, K. M., Kočiský, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., &amp;amp; Blunsom, P. (2015). Teaching machines to read and comprehend. NeurIPS 2015 &lt;a href="https://proceedings.neurips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html."&gt;https://proceedings.neurips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., Laroussilhe, Q.D., Gesmundo, A., Attariyan, M., &amp;amp; Gelly, S. (2019). Parameter-Efficient Transfer Learning for NLP. ICML 2019, &lt;a href="http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf"&gt;http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Narayan, S., Cohen, S. B., &amp;amp; Lapata, M. (2018). Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. EMNLP 2018, &lt;a href="https://www.aclweb.org/anthology/D18-1206/"&gt;https://www.aclweb.org/anthology/D18-1206/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pfeiffer, J., Kamath, A., Rücklé, A., Cho, K., &amp;amp; Gurevych, I. (2021). AdapterFusion: Non-Destructive Task Composition for Transfer Learning. EACL 2021, &lt;a href="https://www.aclweb.org/anthology/2021.eacl-main.39.pdf"&gt;https://www.aclweb.org/anthology/2021.eacl-main.39.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Philip†, J., Bérard, A., Gallé, M., Besacier, L. (2020). Monolingual Adapters for Zero-Shot Neural Machine Translation. EMNLP 2020, &lt;a href="https://www.aclweb.org/anthology/2020.emnlp-main.361.pdf"&gt;https://www.aclweb.org/anthology/2020.emnlp-main.361.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See, A., Liu, P. J., &amp;amp; Manning, C. D. (2017). Get to the point: Summarization with pointer-generator networks. ACL 2017, &lt;a href="https://www.aclweb.org/anthology/P17-1099/"&gt;https://www.aclweb.org/anthology/P17-1099/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sheng, E., &amp;amp; Uthus, D. (2020). Investigating Societal Biases in a Poetry Composition System. Proceedings of the Second Workshop on Gender Bias in Natural Language Processing, &lt;a href="https://www.aclweb.org/anthology/2020.gebnlp-1.9/"&gt;https://www.aclweb.org/anthology/2020.gebnlp-1.9/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nc"&gt;@misc&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nl"&gt;sterz_2021&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
  &lt;span class="na"&gt;title&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="s"&gt;{Adapters for Generative and Seq2Seq Models in NLP}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="na"&gt;url&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="s"&gt;{https://adapterhub.ml/blog/2021/04/adapters-for-generative-and-seq2seq-models-in-nlp/}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
  &lt;span class="na"&gt;author&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="s"&gt;{Hannah Sterz and Clifton Poth and Andreas R\&amp;quot;uckl\&amp;#39;e and Jonas Pfeiffer}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
  &lt;span class="na"&gt;year&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="s"&gt;{2021}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
  &lt;span class="na"&gt;month&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="s"&gt;{Apr}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;* equal contribution&lt;/p&gt;</content>
    <link href="https://adapterhub.ml/blog/2021/04/adapters-for-generative-and-seq2seq-models-in-nlp" rel="alternate"/>
    <summary>Adapters have proven to be an efficient alternative to fully finetung models. The version 2.0 of the AdapterHub framework includes adapters for the BART and GPT2 models.
</summary>
    <published>2021-04-29T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://adapterhub.ml/blog/2021/04/version-2-of-adapterhub-released</id>
    <title>Version 2 of AdapterHub Released</title>
    <updated>2021-04-29T00:00:00+00:00</updated>
    <author>
      <name>Clifton Poth</name>
    </author>
    <author>
      <name>Hannah Sterz</name>
    </author>
    <content type="html">&lt;figure id="_caption-1"&gt;
&lt;img alt="" src="/static/images/v2_blocks.png" title="Illustration of adapter composition blocks supported in v2 of adapter-transformers." /&gt;
&lt;figcaption&gt;&lt;span&gt;Figure&amp;nbsp;1:&lt;/span&gt; Illustration of adapter composition blocks supported in v2 of adapter-transformers.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Adapters, a light-weight alternative to full language model fine-tuning, enable new ways of composing task-specific knowledge from multiple sources, e.g., for multi-task transfer learning (&lt;a href="https://arxiv.org/pdf/2005.00247.pdf"&gt;Pfeiffer et al., 2021&lt;/a&gt;) or cross-lingual transfer (&lt;a href="https://www.aclweb.org/anthology/2020.emnlp-main.617.pdf"&gt;Pfeiffer et al., 2020&lt;/a&gt;).
One of the most important advantages of adapters is their modularity, which allows many exciting possibilities for composition beyond the ones mentioned above.&lt;/p&gt;
&lt;p&gt;Today, we are releasing Version 2 of the &lt;a href="https://adapterhub.ml/"&gt;AdapterHub framework&lt;/a&gt;, including a major update of &lt;code&gt;adapter-transformers&lt;/code&gt;, which makes it easier to take advantage of the composability and flexibility of adapters.
&lt;code&gt;adapter-transformers&lt;/code&gt; --- an extension of the great &lt;a href="https://huggingface.co/transformers/"&gt;Transformers library by HuggingFace&lt;/a&gt; --- is the heart of the AdapterHub that simplifies the entire adapter lifecycle.
(Check out &lt;a href="https://adapterhub.ml/blog/2020/11/adapting-transformers-with-adapterhub/"&gt;our first blog post for more on this&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;In the following sections, we will discuss all new features and changes that we introduce with the v2 release.
You can find &lt;code&gt;adapter-transformers&lt;/code&gt; &lt;a href="https://github.com/Adapter-Hub/adapter-transformers"&gt;on GitHub&lt;/a&gt; or install it via pip:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip install -U adapter-transformers
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="whats-new"&gt;What's new&lt;/h2&gt;
&lt;h3 id="adapter-composition-blocks"&gt;Adapter composition blocks&lt;/h3&gt;
&lt;p&gt;The new version introduces a radically different way to define adapter setups in a Transformer model,
allowing much more advanced and flexible adapter composition possibilities.
An example setup using this new, modular composition mechanism might look like this:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;transformers.adapters.composition&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;ac&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;active_adapters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ac&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Stack&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ac&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;b&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;c&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;split_index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As we can see, the basic building blocks of this setup are simple objects representing different possibilities to combine individual adapters.
In the above example, &lt;code&gt;Stack&lt;/code&gt; describes stacking adapters layers on top of each other,
e.g., as it is used in the &lt;em&gt;MAD-X&lt;/em&gt; framework for cross-lingual transfer.
&lt;code&gt;Split&lt;/code&gt; results in splitting the input sequences between two adapters at a specified &lt;code&gt;split_index&lt;/code&gt;.
In the depicted setup, at every transformer layer the token representations are first passed through adapter &lt;code&gt;a&lt;/code&gt; before being split at the &lt;code&gt;split_index&lt;/code&gt; and passed through adapters &lt;code&gt;b&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; respectively.&lt;/p&gt;
&lt;p&gt;Besides the two blocks shown, &lt;code&gt;adapter-transformers&lt;/code&gt; includes a &lt;code&gt;Fuse&lt;/code&gt; block (for &lt;a href="https://arxiv.org/pdf/2005.00247.pdf"&gt;&lt;em&gt;AdapterFusion&lt;/em&gt;&lt;/a&gt;) and a &lt;code&gt;Parallel&lt;/code&gt; block (see below).
All of these blocks are derived from &lt;code&gt;AdapterCompositionBlock&lt;/code&gt;, and they can be flexibly combined in even very complex scenarios.
Figure 1 shows an illustration of the structure of each composition block.
For more information on specifying the active adapters using &lt;code&gt;active_adapters&lt;/code&gt; and the new composition blocks,
refer to the &lt;a href="https://docs.adapterhub.ml/adapter_composition.html"&gt;corresponding section in our documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="new-model-support-adapters-for-bart-and-gpt-2"&gt;New model support: Adapters for BART and GPT-2&lt;/h3&gt;
&lt;p&gt;Version 2 adds support for BART and GPT-2, marking a new type of models we support in the framework, namely sequence-to-sequence models (more to come!)&lt;/p&gt;
&lt;p&gt;We have &lt;a href="https://adapterhub.ml/blog/2021/04/adapters-for-generative-and-seq2seq-models-in-nlp/"&gt;a separate blog post&lt;/a&gt; that studies the effectiveness of adapters within these two models in greater detail! This blog post also includes a hands-on example where we train GPT-2 to generate poetry.&lt;/p&gt;
&lt;h3 id="adapterdrop"&gt;AdapterDrop&lt;/h3&gt;
&lt;p&gt;Version 2 of &lt;code&gt;adapter-transformers&lt;/code&gt; integrates some of the key ideas presented in &lt;em&gt;AdapterDrop&lt;/em&gt; &lt;a href="https://arxiv.org/pdf/2010.11918.pdf"&gt;(Rücklé et al., 2020)&lt;/a&gt;, namely, (1) parallel multi-task inference and (2) &lt;em&gt;robust&lt;/em&gt; AdapterDrop training. &lt;/p&gt;
&lt;p&gt;Parallel multi-task inference, for any given input, runs multiple task adapters in parallel and thereby achieves considerable improvements in inference speed compared to sequentially running multiple Transformer models (see the paper for more details). The &lt;code&gt;Parallel&lt;/code&gt; adapter composition block implements this behavior, which we describe in more detail &lt;a href="adapter_composition.html#parallel"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A central advantage of multi-task inference is that it shares the computations in lower transformer layers across all inference tasks (before the first adapter block). Dropping out adapters from lower transformer layers can thus result in even faster inference speeds, but it often comes at the cost of lower accuracies. To allow for &lt;em&gt;dynamic&lt;/em&gt; adjustment of the number of dropped adapter layers at run-time regarding the available computational resources, we introduce &lt;em&gt;robust&lt;/em&gt; adapter training. This technique drops adapters from a random number of lower transformer layers in each training step. The resulting adapter can be adjusted at run-time regarding the number of dropped layers, to dynamically select between a higher accuracy or faster inference speeds.
We present an example for robust &lt;em&gt;AdapterDrop&lt;/em&gt; training &lt;a href="https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/05_Adapter_Drop_Training.ipynb"&gt;in this Colab notebook&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="transformers-upgrade"&gt;Transformers upgrade&lt;/h3&gt;
&lt;p&gt;Version 2.0.0 upgrades the underlying HuggingFace Transformers library from v3.5.1 to v4.5.1, bringing many awesome new features created by HuggingFace.&lt;/p&gt;
&lt;h2 id="what-has-changed"&gt;What has changed&lt;/h2&gt;
&lt;h3 id="unified-handling-of-all-adapter-types"&gt;Unified handling of all adapter types&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Includes breaking changes ⚠️&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The new version removes the hard distinction between &lt;em&gt;task&lt;/em&gt; and &lt;em&gt;language&lt;/em&gt; adapters (realized using the &lt;code&gt;AdapterType&lt;/code&gt; enumeration in v1) everywhere in the library.
Instead, all adapters use the same set of methods.
This results in some breaking changes.
For example, you don't have to specify the adapter type anymore when adding a new adapter.
Instead of...&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# OLD (v1)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AdapterType&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text_task&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;houlsby&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;... you would simply write...&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# NEW (v2)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;houlsby&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;A similar change applies for loading adapters from the Hub using &lt;code&gt;load_adapter()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In v1, adapters of type &lt;code&gt;text_lang&lt;/code&gt; automatically had invertible adapter modules added.
As this type distinction is now removed, adding invertible adapters can be specified via the adapter config.
For example...&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# OLD (v1)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AdapterType&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text_task&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pfeiffer&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;... in v1 would be equivalent to the following in v2:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# NEW (v2)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pfeiffer+inv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id="changes-to-adapter_names-parameter"&gt;Changes to &lt;code&gt;adapter_names&lt;/code&gt; parameter&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Version 2.0.0 temporarily removed the &lt;code&gt;adapter_names&lt;/code&gt; parameter entirely. Due to user feedback, it was re-added in v2.0.1.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;One possibility to specify the active adapters is to use the &lt;code&gt;adapter_names&lt;/code&gt; parameter in each call to the model's &lt;code&gt;forward()&lt;/code&gt; method.
With the integration of the new, unified mechanism for specifying adapter setups using composition blocks,
it is now recommended to specify the active adapters via &lt;code&gt;set_active_adapters()&lt;/code&gt; or the &lt;code&gt;active_adapters&lt;/code&gt; property.
For example...&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# OLD (v1)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;adapter_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;awesome_adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;... would become...&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# NEW (v2)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;active_adapters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;awesome_adapter&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="internal-changes"&gt;Internal changes&lt;/h2&gt;
&lt;h3 id="changes-to-adapter-weights-dictionaries-and-config"&gt;Changes to adapter weights dictionaries and config&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Includes breaking changes ⚠️&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;With the unification of different adapter types and other internal refactorings, the names of the modules holding the adapters have changed.
This affects the weights dictionaries exported by &lt;code&gt;save_adapter()&lt;/code&gt;, making the adapters incompatible &lt;em&gt;in name&lt;/em&gt;.
Nonetheless, this does not visibly affect loading older adapters with the new version.
When loading an adapter trained with v1 in a newer version, &lt;code&gt;adapter-transformers&lt;/code&gt; will automatically convert the weights to the new format.
However, loading adapters trained with newer versions into an earlier v1.x version of the library does not work.&lt;/p&gt;
&lt;p&gt;Additionally, there have been some changes in the saved configuration dictionary, also including automatic conversions from older versions.&lt;/p&gt;
&lt;h3 id="refactorings-in-adapter-implementations"&gt;Refactorings in adapter implementations&lt;/h3&gt;
&lt;p&gt;There have been some refactorings mainly in the adapter mixin implementations.
Most importantly, all adapter-related code has been moved to the &lt;code&gt;transformers.adapters&lt;/code&gt; namespace.
Further details on the implementation can be found &lt;a href="https://github.com/Adapter-Hub/adapter-transformers/blob/master/adding_adapters_to_a_model.md"&gt;in the guide for adding adapters to a new model&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;As part of the new AdapterHub release, version 2 of &lt;code&gt;adapter-transformers&lt;/code&gt; brings a range of new features to broaden the possibilities of working with adapters.
The library is still under active development, so make sure to check it out &lt;a href="https://github.com/Adapter-Hub/adapter-transformers"&gt;on GitHub&lt;/a&gt;.
Also, we're always happy for any kind of contributions!&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., Laroussilhe, Q.D., Gesmundo, A., Attariyan, M., &amp;amp; Gelly, S. (2019). Parameter-Efficient Transfer Learning for NLP. ICML 2019, &lt;a href="http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf"&gt;http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pfeiffer, J., Rücklé, A., Poth, C., Kamath, A., Vulić, I., Ruder, S., Cho, K., &amp;amp; Gurevych, I. (2020). AdapterHub: A Framework for Adapting Transformers. EMNLP 2020 &lt;a href="https://www.aclweb.org/anthology/2020.emnlp-demos.7.pdf"&gt;https://www.aclweb.org/anthology/2020.emnlp-demos.7.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pfeiffer, J., Kamath, A., Rücklé, A., Cho, K., &amp;amp; Gurevych, I. (2021). AdapterFusion: Non-Destructive Task Composition for Transfer Learning. EACL 2021, &lt;a href="https://www.aclweb.org/anthology/2021.eacl-main.39.pdf"&gt;https://www.aclweb.org/anthology/2021.eacl-main.39.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pfeiffer, J., Vulic, I., Gurevych, I., &amp;amp; Ruder, S. (2020). MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer. EMNLP 2020, &lt;a href="https://www.aclweb.org/anthology/2020.emnlp-main.617/"&gt;https://www.aclweb.org/anthology/2020.emnlp-main.617/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Rücklé, A., Geigle, G., Glockner, M., Beck, T., Pfeiffer, J., Reimers, N., &amp;amp; Gurevych, I. (2020). AdapterDrop: On the Efficiency of Adapters in Transformers. ArXiv, &lt;a href="https://arxiv.org/abs/2010.11918"&gt;https://arxiv.org/abs/2010.11918&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., &amp;amp; Brew, J. (2019). Transformers: State-of-the-Art Natural Language Processing. EMNLP 2020, &lt;a href="https://www.aclweb.org/anthology/2020.emnlp-demos.6/"&gt;https://www.aclweb.org/anthology/2020.emnlp-demos.6/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>
    <link href="https://adapterhub.ml/blog/2021/04/version-2-of-adapterhub-released" rel="alternate"/>
    <summary>Today, we are releasing version 2 of the AdapterHub. This release introduces several exciting new ways for composing adapters through composition blocks, including AdapterFusion, parallel inference, Adapter stacking, and combinations thereof. Furthermore, we now support new Transformer architectures such as GPT-2 and BART.</summary>
    <published>2021-04-29T00:00:00+00:00</published>
  </entry>
</feed>
