<!doctype html>
<html lang="en">

<head>

    
        <!-- Global site tag (gtag.js)  -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-169190162-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-169190162-1');
        </script>
    

    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    
<meta name="description" content="Today we are releasing the newest updates in our Adapters library. This post summarizes new features in the latest release as well as selected new features since our initial release in Nov 2023, including new adapter methods, new supported models and Hub updates.
" />


    <link rel="icon" type="image/png" href="/static/adapter-bert-head.png" />

    <!-- Font Awesome -->
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">
    <!-- Pygments -->
    <link rel="stylesheet" href="/pygments.css">
    <!-- CSS -->
    
        <link rel="stylesheet" href="/static/gen/packed.css?c557e741">
    
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.css" />
    <!-- JS -->
    
        <script type="text/javascript" src="/static/gen/packed.js?2800c204"></script>
    
    <!-- MathJax -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          config: ["MMLorHTML.js"],
          jax: ["input/TeX", "output/HTML-CSS", "output/NativeMML"],
          extensions: ["MathMenu.js", "MathZoom.js"]
        });
    </script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js"></script>

    <title>AdapterHub - Adapters Library Updates: ReFT, QLoRA, Merging, New Models &amp; Hub</title>
</head>

<body>

<nav class="navbar navbar-expand-md navbar-light bg-light">
    <div class="container">
        <a class="navbar-brand p-0" href="/">
            <img src="/static/adapter-bert.png" width="28" class="bert"/>
            <span class="align-middle">AdapterHub</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
                aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fas fa-bars text-dark" style="font-size:28px"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarNav">
            <ul class="navbar-nav ml-auto">
                <div class="dropdown-divider d-md-none"></div>
                <li class="nav-item">
                    <a class="nav-link" href="/explore/">
                        <i class="fas fa-binoculars"></i>&nbsp; Explore
                    </a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="https://docs.adapterhub.ml/">
                        <i class="fas fa-book"></i>&nbsp; Docs
                    </a>
                </li>
                <li class="nav-item active">
                    <a class="nav-link" href="/blog/">
                        <i class="fas fa-bullhorn"></i>&nbsp; Blog
                    </a>
                </li>
                <li class="nav-item separator d-none d-md-inline-block"></li>
                <li class="nav-item nav-secondary justify-content-end d-none d-md-inline-block">
                    <a class="nav-link" href="https://github.com/adapter-hub">
                        <i class="fab fa-github"></i>&nbsp;
                    </a>
                </li>
                <li class="nav-item nav-secondary justify-content-end d-none d-md-inline-block">
                    <a class="nav-link" href="https://twitter.com/adapterhub">
                        <i class="fab fa-twitter"></i>&nbsp;
                    </a>
                </li>
            </ul>
        </div>
    </div>
</nav>

<div id="Banner"> </div>

<div id="Header" class="jumbotron jumbotron-fluid py-lg-5">
    <div class="container">
        
<div class="row breadcrumb-nav mb-3">
    <nav aria-label="breadcrumb" class="col">
        <ol class="breadcrumb bg-transparent">
            <li class="breadcrumb-item">
                <a href="/blog/">Blog</a>
            </li>
        </ol>
    </nav>
</div>

<h1>Adapters Library Updates: ReFT, QLoRA, Merging, New Models &amp; Hub</h1>
<ul class="blog-post-bar mt-4 mb-0">
    <li><i class="fa fa-calendar-alt"></i>&nbsp; 2024-08-10</li>
    <li><i class="fa fa-user"></i>&nbsp;
    
        Clifton Poth
        
        &nbsp;<a href="https://twitter.com/@clifapt"><i class="fab fa-twitter"></i></a>
        
        
        ,&nbsp;
        
    
        Leon Engl√§nder
        
        &nbsp;<a href="https://twitter.com/@LeonEnglaender"><i class="fab fa-twitter"></i></a>
        
        
        ,&nbsp;
        
    
        Timo Imhof
        
        &nbsp;<a href="https://twitter.com/@timo_imhof"><i class="fab fa-twitter"></i></a>
        
        
        ,&nbsp;
        
    
        Hannah Sterz
        
        &nbsp;<a href="https://twitter.com/@h_sterz"><i class="fab fa-twitter"></i></a>
        
        
        ,&nbsp;
        
    
        Jonas Pfeiffer
        
        &nbsp;<a href="https://twitter.com/@PfeiffJo"><i class="fab fa-twitter"></i></a>
        
        
    
    </li>
</ul>

    </div>
</div>

<div class="container pb-3 pb-md-5">
    
    

<div class="row">
    <div class="col-lg-10 my-2">
        

        <div class="blog-post-content">
            <p>Nine months ago, <a href="https://adapterhub.ml/blog/2023/11/introducing-adapters/">we released <em>Adapters</em></a>, our new unified library for parameter-efficient and modular fine-tuning.
<em>Adapters</em> stands in direct tradition to our work on <code>adapter-transformers</code> since 2020, the first open-source library for parameter-efficient fine-tuning.
Since its initial release, <em>Adapters</em> has received various updates, the newest being released today.
In this post, we'll go through some of the most exciting new features released today and in the last few months.
You can find the full list of changes in the latest release <a href="">in our release notes</a>.</p>
<div class="toc">
<ul>
<li><a href="#representation-fine-tuning-reft">Representation Fine-Tuning (ReFT)</a></li>
<li><a href="#adapter-merging">Adapter Merging</a></li>
<li><a href="#quantized-training">Quantized Training</a></li>
<li><a href="#new-models">New Models</a><ul>
<li><a href="#whisper">Whisper</a></li>
<li><a href="#other-models">Other Models</a></li>
</ul>
</li>
<li><a href="#hub-updates">Hub Updates</a></li>
<li><a href="#citation">Citation</a></li>
</ul>
</div>
<p>You can find <em>Adapters</em> <a href="https://github.com/Adapter-Hub/adapters">on GitHub</a> or install it via pip:</p>
<div class="codehilite"><pre><span></span><code>pip install -U adapters
</code></pre></div>

<h2 id="representation-fine-tuning-reft">Representation Fine-Tuning (ReFT)</h2>
<div align="center">
<figure text-align="center">
<img src="/static/images/reft.jpg" height="200">
  <figcaption text-align="center">
    Illustrations from the ReFT paper (Wu et al., 2024): Left: The general framework of applying ReFT interventions. Right: Visualization of LoReFT.
  </figcaption>
 </figure>
</div>

<p>Representation Fine-Tuning (ReFT), proposed by <a href="https://arxiv.org/pdf/2404.03592">Wu et al. (2024)</a>, is a novel efficient adapter method.
It leverages so-called interventions to adapt the pre-trained representations of a language model.
Within the context of ReFT, these interventions can intuitively be thought of as adapter modules placed after each Transformer layer.
In the general form, an intervention function <script type="math/tex">\Phi</script> can thus be defined as follows:</p>
<p>
<script type="math/tex; mode=display">
\Phi(h) = h + R^T (W h + b - R h)
</script>
</p>
<p>Here, <script type="math/tex">R \in \mathbb{R}^{r \times d}</script> and <script type="math/tex">W \in \mathbb{R}^{r \times d}</script> are low-rank matrices of rank <script type="math/tex">r</script>.
<script type="math/tex">h</script> is the layer output hidden state at a single sequence position, i.e. interventions can be applied independently at each position.</p>
<p>Based on this general form, the ReFT paper proposes multiple instantiations of ReFT methods supported by <em>Adapters</em>:</p>
<ul>
<li><strong>LoReFT</strong> enforces orthogonality of rows in <script type="math/tex">R</script>. Defined via <a href="adapters.LoReftConfig"><code>LoReftConfig</code></a> or via the <code>orthogonality</code> attribute as in the following example:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">config</span> <span class="o">=</span> <span class="n">ReftConfig</span><span class="p">(</span>
    <span class="n">layers</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="n">prefix_positions</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">suffix_positions</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">orthogonality</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>  <span class="c1"># equivalent to LoreftConfig()</span>
</code></pre></div>

<ul>
<li><strong>NoReFT</strong> does not enforce orthogonality in <script type="math/tex">R</script>. Defined via <a href="adapters.NoReftConfig"><code>NoReftConfig</code></a> or equivalently:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">config</span> <span class="o">=</span> <span class="n">ReftConfig</span><span class="p">(</span>
    <span class="n">layers</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="n">prefix_positions</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">suffix_positions</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">orthogonality</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>  <span class="c1"># equivalent to NoReftConfig()</span>
</code></pre></div>

<ul>
<li><strong>DiReFT</strong> does not enforce orthogonality in <script type="math/tex">R</script> and additionally removes subtraction of <script type="math/tex">R h</script> in the intervention, Defined via <a href="adapters.DiReftConfig"><code>DiReftConfig</code></a> or equivalently:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">config</span> <span class="o">=</span> <span class="n">ReftConfig</span><span class="p">(</span>
    <span class="n">layers</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="n">prefix_positions</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">suffix_positions</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">orthogonality</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">subtract_projection</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>  <span class="c1"># equivalent to DiReftConfig()</span>
</code></pre></div>

<p>In addition, <em>Adapters</em> supports configuring multiple hyperparameters tuned in the ReFT paper in <code>ReftConfig</code>, including:</p>
<ul>
<li><code>prefix_positions</code>: number of prefix positions</li>
<li><code>suffix_positions</code>: number of suffix positions</li>
<li><code>layers</code>: The layers to intervene on. This can either be <code>"all"</code> or a list of layer ids</li>
<li><code>tied_weights</code>: whether to tie parameters between prefixes and suffixes</li>
</ul>
<p>You can use ReFT adapters exactly as any other adapter type in <em>Adapters</em>:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">adapters</span> <span class="kn">import</span> <span class="n">AutoAdapterModel</span><span class="p">,</span> <span class="n">LoReftConfig</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoAdapterModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;roberta-base&quot;</span><span class="p">)</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">LoReftConfig</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;loreft_adapter&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_adapter</span><span class="p">(</span><span class="s2">&quot;loreft_adapter&quot;</span><span class="p">)</span>
<span class="c1"># add training loop ...</span>
</code></pre></div>

<p>Learn more about training adapters <a href="https://github.com/adapter-hub/adapters/blob/main/notebooks/01_Adapter_Training.ipynb">in this notebook</a>.</p>
<h2 id="adapter-merging">Adapter Merging</h2>
<p><a href="https://colab.research.google.com/github/Adapter-Hub/adapters/blob/main/notebooks/06_Task_Arithmetics.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<p>We've expanded support for adapter merging, enabling the efficient combination of trained adapters without additional fine-tuning. Merging multiple adapters into a new one allows for efficient domain, language and task transfer. Adapter Merging is a form of Task Arithmetics (<a href="https://arxiv.org/abs/2212.04089">Ilharco et al., 2023</a>; <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/299a08ee712d4752c890938da99a77c6-Abstract-Conference.html">Zhang et al., 2023</a>) and hence also allows increasing or unlearning specific skills. All adapter methods support linear merging. For <em>N</em> adapters with parameters <script type="math/tex">\Phi_i</script> the merged adapter parameters <script type="math/tex">\Phi_{merged}</script> are calculated as:</p>
<p>
<script type="math/tex; mode=display">
\Phi_{merged} = \sum_{i=0}^{N} \lambda_i \Phi_i
</script>
</p>
<p>Where <script type="math/tex">\lambda_i</script> is the weight for each adapter. Example usage:</p>
<div class="codehilite"><pre><span></span><code><span class="n">model</span><span class="o">.</span><span class="n">average_adapter</span><span class="p">(</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;merged_adapter&quot;</span><span class="p">,</span>
    <span class="n">adapter_list</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;lora1&quot;</span><span class="p">,</span> <span class="s2">&quot;lora2&quot;</span><span class="p">,</span> <span class="s2">&quot;lora3&quot;</span><span class="p">],</span>
    <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span> <span class="c1"># these are the Œª_i</span>
    <span class="n">combine_strategy</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

<p>For LoRA adapters, <a href="https://arxiv.org/abs/2311.09344">Chronopoulou et al. (2023)</a> have shown that linear combination can work effectively. However, the parameters of the LoRA matrices are interdependent. Hence simple linear combination may not always yield optimal results. Therefore, we support two additional LoRA-specific merging strategies:</p>
<ul>
<li><code>combine_strategy = "lora_linear_only_negate_b"</code>: As proposed by <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/299a08ee712d4752c890938da99a77c6-Abstract-Conference.html">Zhang et al. (2023)</a> this method only negates the B matrix for negative weights:
  <script type="math/tex; mode=display">
  A_{merged} = \sum_{i=0}^{N} |\lambda_i| A_i,\\
  B_{merged} = \sum_{i=0}^{N} \lambda_i B_i
  </script>
</li>
<li><code>combine_strategy = "lora_delta_w_svd"</code>: Merges the LoRA delta W matrices and then applies SVD to obtain new A and B matrices.
  <script type="math/tex; mode=display">
  \Delta W_{new} = \sum_{i=0}^N \lambda_i \cdot (\Delta W_i),\\
  A_{new}, B_{new} = \text{SVD}(\Delta W_{new})
  </script>
</li>
</ul>
<p>Example usage:</p>
<div class="codehilite"><pre><span></span><code><span class="n">model</span><span class="o">.</span><span class="n">average_adapter</span><span class="p">(</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;lora_svd_merged&quot;</span><span class="p">,</span>
    <span class="n">adapter_list</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;lora1&quot;</span><span class="p">,</span> <span class="s2">&quot;lora2&quot;</span><span class="p">,</span> <span class="s2">&quot;lora3&quot;</span><span class="p">],</span>
    <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span>
    <span class="n">combine_strategy</span><span class="o">=</span><span class="s2">&quot;lora_delta_w_svd&quot;</span><span class="p">,</span>
    <span class="n">svd_rank</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>  <span class="c1"># &quot;lora_delta_w_svd&quot; requires the &quot;svd_rank&quot; parameter, which determines the r (rank) of the resulting LoRA adapter after singular value decomposition (SVD)</span>
<span class="p">)</span>
</code></pre></div>

<h2 id="quantized-training">Quantized Training</h2>
<p><a href="https://colab.research.google.com/github/Adapter-Hub/adapters/blob/main/notebooks/QLoRA_Llama_Finetuning.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<p>Quantization of model weights has become an important method for drastically reducing the memory footprint of recent large language models.
Quantizing parameters down to 8 bits or 4 bits (<a href="https://arxiv.org/pdf/2212.09720">Dettmers &amp; Zettlemoyer, 2023</a>) have enabled running large models on limited hardware with minimal performance reduction.</p>
<p>While initially limited to model inference, <strong>QLoRA</strong> (<a href="https://arxiv.org/pdf/2305.14314">Dettmers et al., 2023</a>) has proposed combining model quantization with adapter training using LoRA.</p>
<p>QLoRA combines several innovations to reduce the memory footprint while fine-tuning a large LM. In short:</p>
<ul>
<li><em>4-bit NormalFloat quantization</em> reduces the size of the base model to 4 bits per parameter while optimizing for maximizing the retained information.</li>
<li><em>Double quantization</em> additionally quantizes constants required for quantization for additional memory saving.</li>
<li><em>Paged optimizers</em> offloads optimizer states into CPU memory when they don't fit into GPU memory and automatically reloads them when needed.</li>
<li><em>LoRA training</em> fine-tunes LoRA layers on the task while keeping the quantized base model weights fixes.</li>
</ul>
<p>Make sure to check out <a href="https://arxiv.org/pdf/2305.14314">the paper</a> for detailed explanations!
The figure below visualizes the key differences between full fine-tuning, LoRA and QLoRA:</p>
<div align="center">
<figure text-align="center">
<img src="/static/images/qlora.jpg">
  <figcaption text-align="center">
    Illustration from the QLoRA paper (Dettmers et al., 2023) comparing full fine-tuning, LoRA and QLoRA.
  </figcaption>
 </figure>
</div>

<p>Model quantization and paged optimizers are integrated to the Transformers library via the <strong><a href="https://github.com/TimDettmers/bitsandbytes">bitsandbytes library</a></strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="c1"># Load 4-bit quantized model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;meta-llama/Meta-Llama-3-8B&quot;</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">BitsAndBytesConfig</span><span class="p">(</span>
        <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">&quot;nf4&quot;</span><span class="p">,</span>
        <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

<p>Since <em>Adapters</em> v0.2.0, all adapter implementations integrate seamlessly with quantized models, e.g. for QLoRA:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">adapters</span>
<span class="kn">from</span> <span class="nn">adapters</span> <span class="kn">import</span> <span class="n">LoRAConfig</span>

<span class="n">adapters</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">LoRAConfig</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;qlora&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_adapter</span><span class="p">(</span><span class="s2">&quot;qlora&quot;</span><span class="p">)</span>
</code></pre></div>

<p>This approach isn't limited to LoRA - you can easily swap out the adapter config here! You can not only train QLoRA, but also QBottleneck adapters, QPrefixTuning and more!</p>
<p>For a full guide, check out our <a href="https://github.com/Adapter-Hub/adapters/blob/main/notebooks/QLoRA_Llama_Finetuning.ipynb">Notebook tutorial for quantized fine-tuning of Llama</a>.</p>
<h2 id="new-models">New Models</h2>
<h3 id="whisper">Whisper</h3>
<p><a href="https://github.com/Adapter-Hub/adapters/blob/main/notebooks/Adapter_Whisper_Audio_FineTuning.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a> </p>
<p>With the support of Whisper, we introduce the first model in the adapters library to operate in the audio domain, posing a fundamental step towards making our library more diverse for various modalities. 
Whisper was originally presented by OpenAI in their paper <a href="https://arxiv.org/abs/2212.04356">Robust Speech Recognition via Large-Scale Weak Supervision</a> and is a state-of-the-art model for audio processing trained on 680.000 hours of unsupervised data. </p>
<p>Our <code>WhisperAdapterModel</code> builds on the standard encoder-decoder architecture of the Hugging Face Whisper implementation and supports all the methods listed below, as well as flexible adding and removing of heads.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>(Bottleneck)<br> Adapters</th>
<th>Prefix<br> Tuning</th>
<th>LoRA</th>
<th>Compacter</th>
<th>Adapter<br> Fusion</th>
<th>Invertible<br> Adapters</th>
<th>Parallel<br> block</th>
<th>Prompt<br> Tuning</th>
<th>ReFT</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://docs.adapterhub.ml/classes/models/whisper.html">Whisper</a></td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
<td></td>
<td>‚úÖ</td>
</tr>
</tbody>
</table>
<p>We also support enabling adapter capabilities for existing static head models of the classes <code>WhisperForConditionalGeneration</code> and <code>WhisperForAudioClassification</code> via the <code>init()</code> function.</p>
<p>Since Whisper processes audio, the audio data requires additional processing steps that are different from standard text processing. For more information on that, check out our <a href="https://github.com/Adapter-Hub/adapters/blob/main/notebooks/Adapter_Whisper_Audio_FineTuning.ipynb">new notebook tutorial</a> on how to finetune Whisper with LoRA for transcription.</p>
<h3 id="other-models">Other Models</h3>
<p>Since our initial release, we have also added a bunch of other models:</p>
<ul>
<li><a href="https://docs.adapterhub.ml/classes/models/mt5.html">MT5</a></li>
<li><a href="https://docs.adapterhub.ml/classes/models/mistral.html">Mistral</a></li>
<li><a href="https://docs.adapterhub.ml/classes/models/plbart.html">PLBart</a></li>
</ul>
<p>Go check them out if you are interested!</p>
<h2 id="hub-updates">Hub Updates</h2>
<p>Within the last few weeks, we have archived the "original" Hub repository (found at: <a href="https://github.com/adapter-hub/Hub">Adapter-Hub/Hub</a>) released alongside our initial AdapterHub release in 2020.
The Hub repository on GitHub is now in read-only mode, meaning no new adapters can be added there.</p>
<p>It's recommended to upload all new adapters to the Hugging Face Model Hub, which will be the only supported Hub for <em>Adapters</em> in the future (<a href="https://docs.adapterhub.ml/huggingface_hub.html">Learn more</a>). We have moved all ~300 publicly accessible adapters, including all of our original collection and most third-party contributions over to the Hugging Face Model Hub. Check out our Hugging Face Hub page at: <a href="https://huggingface.co/AdapterHub">https://huggingface.co/AdapterHub</a>.</p>
<p>In v1.0 of Adapters, attempting to load adapters from the original Hub repo will automatically redirect to loading the same adapter from the Hugging Face Model Hub.
There is no breaking change in loading an adapter ID, the same adapter weights will be loaded.
However, some parameters related to Hub loading and discovery have been deprecated or removed.
Learn more about breaking changes <a href="https://github.com/adapter-hub/adapters/discussions/725">here</a>.</p>
<h2 id="citation">Citation</h2>
<p>If you use <em>Adapters</em> in your research, please cite:</p>
<div class="codehilite"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">poth-etal-2023-adapters</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">&quot;Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning&quot;</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">{Poth, Clifton  and</span>
<span class="s">      Sterz, Hannah  and</span>
<span class="s">      Paul, Indraneil  and</span>
<span class="s">      Purkayastha, Sukannya  and</span>
<span class="s">      Engl{\&quot;a}nder, Leon  and</span>
<span class="s">      Imhof, Timo  and</span>
<span class="s">      Vuli{\&#39;c}, Ivan  and</span>
<span class="s">      Ruder, Sebastian  and</span>
<span class="s">      Gurevych, Iryna  and</span>
<span class="s">      Pfeiffer, Jonas}</span><span class="p">,</span>
    <span class="na">booktitle</span> <span class="p">=</span> <span class="s">&quot;Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations&quot;</span><span class="p">,</span>
    <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
    <span class="na">year</span> <span class="p">=</span> <span class="s">&quot;2023&quot;</span><span class="p">,</span>
    <span class="na">address</span> <span class="p">=</span> <span class="s">&quot;Singapore&quot;</span><span class="p">,</span>
    <span class="na">publisher</span> <span class="p">=</span> <span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
    <span class="na">url</span> <span class="p">=</span> <span class="s">&quot;https://aclanthology.org/2023.emnlp-demo.13&quot;</span><span class="p">,</span>
    <span class="na">pages</span> <span class="p">=</span> <span class="s">&quot;149--160&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
        </div>

        <div class="my-5">
            <a href="https://github.com/adapter-hub/website/blob/master/posts/2024/08/adapters-update-reft-qlora-merging-models.md">
                <i class="fa fa-edit"></i>&nbsp; Edit Post on GitHub
            </a>
        </div>
        
        <div id="col-lg-10 comments">
            <script src="https://utteranc.es/client.js"
                repo="Adapter-Hub/website"
                issue-term="pathname"
                label="blog-post üí¨"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>
    </div>

</div>


</div>

<footer>
    <div class="container">
        <p class="float-md-right text-center text-md-right">
            <a href="https://arxiv.org/abs/2311.11077" target="_blank">Paper</a>
            <!--<span class="text-black-30 px-1">|</span>
            <a href="/imprint-privacy/">Imprint & Privacy</a>-->
        </p>
        <p class="text-muted text-center text-md-left">Brought to you with ‚ù§Ô∏è by the AdapterHub Team</p>
    </div>
</footer>

<script>
    document.addEventListener('DOMContentLoaded', function (event) {
        codecopy('pre') // your code tag selector!
        $('.activate-tooltip').tooltip();   // bootstrap tooltips
    })
</script>
<script src="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.js" data-cfasync="false"></script>
<script>
    window.cookieconsent.initialise({
    "palette": {
        "popup": {
        "background": "#d7f0f4"
        },
        "button": {
        "background": "#39b3c6",
        "text": "#ffffff"
        }
    },
    "theme": "classic"
    });
</script>
</body>
</html>