<!doctype html>
<html lang="en">

<head>

    
        <!-- Global site tag (gtag.js)  -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-169190162-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-169190162-1');
        </script>
    

    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    
<meta name="description" content="Adapters have proven to be an efficient alternative to fully finetung models. The version 2.0 of the AdapterHub framework includes adapters for the BART and GPT2 models.
" />


    <link rel="icon" type="image/png" href="/static/adapter-bert-head.png" />

    <!-- Font Awesome -->
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">
    <!-- Pygments -->
    <link rel="stylesheet" href="/pygments.css">
    <!-- CSS -->
    
        <link rel="stylesheet" href="/static/gen/packed.css?c557e741">
    
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.css" />
    <!-- JS -->
    
        <script type="text/javascript" src="/static/gen/packed.js?2800c204"></script>
    
    <!-- MathJax -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          config: ["MMLorHTML.js"],
          jax: ["input/TeX", "output/HTML-CSS", "output/NativeMML"],
          extensions: ["MathMenu.js", "MathZoom.js"]
        });
    </script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js"></script>

    <title>AdapterHub - Adapters for Generative and Seq2Seq Models in NLP</title>
</head>

<body>

<nav class="navbar navbar-expand-md navbar-light bg-light">
    <div class="container">
        <a class="navbar-brand p-0" href="/">
            <img src="/static/adapter-bert.png" width="28" class="bert"/>
            <span class="align-middle">AdapterHub</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
                aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fas fa-bars text-dark" style="font-size:28px"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarNav">
            <ul class="navbar-nav ml-auto">
                <div class="dropdown-divider d-md-none"></div>
                <li class="nav-item">
                    <a class="nav-link" href="/explore/">
                        <i class="fas fa-binoculars"></i>&nbsp; Explore
                    </a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="https://docs.adapterhub.ml/">
                        <i class="fas fa-book"></i>&nbsp; Docs
                    </a>
                </li>
                <li class="nav-item active">
                    <a class="nav-link" href="/blog/">
                        <i class="fas fa-bullhorn"></i>&nbsp; Blog
                    </a>
                </li>
                <li class="nav-item separator d-none d-md-inline-block"></li>
                <li class="nav-item nav-secondary justify-content-end d-none d-md-inline-block">
                    <a class="nav-link" href="https://github.com/adapter-hub">
                        <i class="fab fa-github"></i>&nbsp;
                    </a>
                </li>
                <li class="nav-item nav-secondary justify-content-end d-none d-md-inline-block">
                    <a class="nav-link" href="https://twitter.com/adapterhub">
                        <i class="fab fa-twitter"></i>&nbsp;
                    </a>
                </li>
            </ul>
        </div>
    </div>
</nav>

<div id="Banner"> </div>

<div id="Header" class="jumbotron jumbotron-fluid py-lg-5">
    <div class="container">
        
<div class="row breadcrumb-nav mb-3">
    <nav aria-label="breadcrumb" class="col">
        <ol class="breadcrumb bg-transparent">
            <li class="breadcrumb-item">
                <a href="/blog/">Blog</a>
            </li>
        </ol>
    </nav>
</div>

<h1>Adapters for Generative and Seq2Seq Models in NLP</h1>
<ul class="blog-post-bar mt-4 mb-0">
    <li><i class="fa fa-calendar-alt"></i>&nbsp; 2021-04-29</li>
    <li><i class="fa fa-user"></i>&nbsp;
    
        Hannah Sterz*
        
        &nbsp;<a href="https://twitter.com/@h_sterz"><i class="fab fa-twitter"></i></a>
        
        
        ,&nbsp;
        
    
        Clifton Poth*
        
        &nbsp;<a href="https://twitter.com/@clifapt"><i class="fab fa-twitter"></i></a>
        
        
        ,&nbsp;
        
    
        Andreas R√ºckl√©
        
        &nbsp;<a href="https://twitter.com/@arueckle"><i class="fab fa-twitter"></i></a>
        
        
        ,&nbsp;
        
    
        Jonas Pfeiffer
        
        &nbsp;<a href="https://twitter.com/@PfeiffJo"><i class="fab fa-twitter"></i></a>
        
        
    
    </li>
</ul>

    </div>
</div>

<div class="container pb-3 pb-md-5">
    
    

<div class="row">
    <div class="col-lg-10 my-2">
        

        <div class="blog-post-content">
            <p align="center">
<img src="/static/images/BARTLogo.png">
</p>

<p>Adapters are becoming more and more important in machine learning for NLP. For instance, they enable us to efficiently train and share new task-specific models. Adapters are small layers that are stitched into pre-trained transformer-based models. During training, only the parameters of the adapter layers are finetuned, while the parameters of the pre-trained model remain frozen. As a result, it is sufficient to only store the adapter layers instead of storing fully finetuned models separately for each task. Furthermore, the lower number of parameters requires less memory and makes it easier to share the trained adapters. Adapters also enable new possibilities in transfer learning. As adapters are encapsulated between frozen layers, they can be regarded as modular units which can be composed in a number of different ways (For more details and examples check out <a href="https://adapterhub.ml/blog/2020/11/adapting-transformers-with-adapterhub/">this blog post</a>). <a href="https://www.aclweb.org/anthology/D19-1165.pdf">Bapna et al. (2019)</a> have shown that adapters are useful for sequence to sequence tasks. On a neural machine translation task, they achieved similar results with adapters as compared to a fully finetuned model. The modularity aspect of adapters in zero-shot machine translation has recently been demonstrated by <a href="https://www.aclweb.org/anthology/2020.emnlp-main.361.pdf">Philip et al. (2020)</a>.</p>
<p>The AdapterHub framework makes adapters easy to use. Up until now, the framework included adapters for the models BERT, RoBERTa, XML-RoBERTa and DistilBERT. In the new version 2.0, the framework now also provides adapters for the language generation models BART and GPT-2. This will allow researchers and engineers to use adapters for sequence-to-sequence tasks.</p>
<h2 id="results-of-bart-and-gpt-2-with-adapters">Results of BART and GPT-2 with adapters</h2>
<p>Before we dive into generation tasks, we will take a look at the performance on the GLUE benchmark. We compare the scores of a fully finetuned model with the scores of adapter-based models, either using the adapter configuration of <a href="https://arxiv.org/pdf/2005.00247.pdf">Pfeiffer et al. (2020a)</a> or <a href="https://arxiv.org/pdf/1902.00751.pdf">Houlsby et al. (2020)</a>. The GPT-2 model and BART models achieve the following scores:</p>
<table>
<tr>
<th> GPT-2 </th><th> Full </th><th> Pfeiffer </th><th> Houlsby </th>
</tr>
<tr>
<td> RTE </td><td> 65.0 </td><td> 67.1 </td><td> 67.5 </td>
</tr>
<tr>
<td>   MRPC  </td><td> 83.8 </td><td> 83.5 </td><td> 80.4 </td>
</tr>
<tr>
<td>   STS-B </td><td> 86.7 </td><td> 85.3 </td><td> 85.4 </td>
</tr>
<tr>
<td>   CoLA  </td><td> 33.6 </td><td> 43.0 </td><td> 41.2 </td>
</tr>
<tr>
<td>   SST-2 </td><td> 90.0 </td><td> 90.5 </td><td> 90.9 </td>
</tr>
<tr>
<td>   QNLI  </td><td> 87.6 </td><td> 88.2 </td><td> 88.5 </td>
</tr>
<tr>
<td>   MNLI  </td><td> 82.2 </td><td> 81.6 </td><td> 81.7 </td>
</tr>
<tr>
<td>   QQP   </td><td> 88.5 </td><td> 87.1 </td><td> 87.7 </td>
</tr>
</table>

<p>The fully finetuned GPT-2 model is trained for 4 epochs with a learning rate of 1e-4. The adapters are trained for 10 epochs with a learning rate of 1e-4.</p>
<table>
<tr>
<th> BART </th><th> Full </th><th> Pfeiffer </th><th> Houlsby </th>
</tr>
<tr>
<td> RTE </td><td> 71.12 </td><td> 69.7 </td><td> 69.1</td>
</tr>
<tr>
<td>   MRPC  </td><td> 87.5</td><td> 86.8 </td><td> 88.2 </td>
</tr>
<tr>
<td>   STS-B </td><td> 89.0 </td><td> 88.1 </td><td> 88.3 </td>
</tr>
<tr>
<td>   CoLA  </td><td> 46.6 </td><td> 46.1 </td><td> 45.6 </td>
</tr>
<tr>
<td>   SST-2 </td><td> 92.7 </td><td> 93.7 </td><td> 93.6 </td>
</tr>
<tr>
<td>   QNLI  </td><td> 91.6 </td><td> 92.2 </td><td> 93.6 </td>
</tr>
<tr>
<td>   MNLI  </td><td> 85.7 </td><td> 85.9 </td><td> 85.9 </td>
</tr>
<tr>
<td>   QQP   </td><td> 89.3 </td><td> 88.4 </td><td> 88.6 </td>
</tr>
</table>

<p>The fully-finetuned BART model is trained for 3 epochs with a learning rate of 4e-5. The adapters are trained with early stopping for a maximum of 15 epochs with a learning rate of 1e-4.</p>
<p>The results of the adapters are comparable to those of the fully finetuned model. On some tasks such as SST-2, the adapters achieve a higher score than the fully finetuned model for GPT-2 and BART. This matches the results of other models with adapters. In general, we can use adapters instead of fully finetuning the model without a deterioration in downstream task performance. </p>
<p>Now we will take a look at the scores for sequence-to-sequence tasks. We train a GPT-2 model on the task proposed by <a href="https://arxiv.org/abs/2004.10404">Chen et al. (2020)</a>. This task requires the model to learn to generate entailing sentences w.r.t. the input. For example,  given a table containing the release dates for an album, the model is provided with a template and and has the objective to fill in the blanks.</p>
<blockquote>
<p>Template: [ENT] was released in 6 [ENT] in [ENT].</p>
<p>Gold sentence: Black Ice was released in 6 Countries in 2008.</p>
</blockquote>
<p>It is not sufficient for the model to simply enter a number from the table; it needs to count all countries the album was released in, in 2008. We trained the GPT-2 model with small-sized GPT-2 vocabulary using maximum likelihood estimation. The results are given in the following table:</p>
<table>
<tr>
<th></th><th> BLEU-1 </th><th> BLEU-2 </th><th> BLEU-3 </th><th> Adv-Acc </th>
</tr>
<tr>
<td> GPT-2  </td><td> 48.8 </td><td> 27.1 </td><td> 12.6 </td><td> 62.3 </td>
</tr><tr>
<td> GPT-2 + Pfeiffer </td><td> 46.3 </td><td> 24.8 </td><td> 11.2 </td><td> 60.1 </td>
</tr><tr>
<td> GPT-2 + Houlsby </td><td> 45.5 </td><td> 23.9 </td><td> 10.5 </td><td> 59.7 </td>
</tr>
</table>
<p>We observe that the models with adapters achieve a competitive results to full model fine-tuning. However, adapters have several advantages over fully finetuning, e.g., shorter training times, they require less memory to be stored, and they can easily be shared.</p>
<p>To test the BART model on sequence-to-sequence tasks, we evaluated the model on the CNN/Daily Mail dataset (<a href="https://arxiv.org/pdf/1506.03340.pdf">Hermann et al. (2015)</a>; <a href="https://arxiv.org/pdf/1704.04368.pdf">See et al., 2017</a>) and the extreme summary dataset (XSum) dataset (<a href="https://arxiv.org/pdf/1808.08745.pdf">Narayan et al., 2018</a>). Both tasks have the objective to summarize newspaper articles. The main difference is that XSum requires the model to output short one sentence summaries. The results of the fully finetuned BART model and the adapters are as follows:</p>
<table>
<tr>
<th></th><th> R1 </th><th> R2 </th><th> RL </th>
</tr><tr>
<td> CNN/Daily mail </td><td> 44.16 </td><td> 21.28 </td><td> 40.90 </td>
</tr><tr>
<td>CNN/Daily mail + Pfeiffer </td><td> 43.40 </td><td> 20.86 </td><td> 30.66 </td>
</tr></table>

<table>
<tr>
<th></th><th> R1 </th><th> R2 </th><th> RL </th>
</tr><tr>
<td> XSum </td><td> 45.14 </td><td> 22.27 </td><td> 37.26 </td>
</tr><tr>
<td> XSum + Pfeiffer </td><td> 43.56 </td><td> 20.56 </td><td> 35.56 </td>
</tr><tr>
<td> XSum + Houlsby </td><td>44.03 </td><td> 20.90 </td><td> 36.01 </td>
</tr></table>

<p>Similar to the GPT-2 model, the BART model achieves the highest score when it is fully fine-tuned. The models with adapters achieve slightly lower scores, further indicating that adapters might in general achieve slightly lower scores on sequence-to-sequence tasks. However, as previously stated, they have several other advantages.</p>
<p>Version 2.0 of the AdapterHub framework opens up new possibilities such as experimenting with summarization and text generation tasks. Adapters for BART and GPT-2 enable us to tackle a wide variety of text generation tasks with adapters.</p>
<h2 id="hands-on-example-train-an-adapter-to-write-poems">Hands-on example: Train an adapter to write poems</h2>
<p><a href="https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/06_Text_Generation.ipynb"><img alt="Open Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a> <br>
To illustrate how we can use adapters for text generation, we provide a hands-on example for training adapters within GPT-2 on a poem dataset by <a href="https://arxiv.org/pdf/2011.02686.pdf">Sheng et al. (2020)</a> and let it create novel poems. The dataset contains poems from the Gutenberg project. The full code is available in the corresponding colab notebook linked above. If you have read the previous blog post, this might look very familiar. First, we need to add our adapters.  This is easily done with just a few lines of code:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="c1"># Add a new adapter</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;poem&quot;</span><span class="p">)</span>
<span class="c1"># Activate the adapter for training</span>
<span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train_adapter</span><span class="p">(</span><span class="s2">&quot;poem&quot;</span><span class="p">)</span>
</code></pre></div>

<p>We have created the GPT-2 model and added an adapter with <code>add_adapter()</code>. We only need to pass the name of the adapter <code>"poem"</code>. After adding the new adapter, we call <code>train_adapter()</code> and pass the name of our adapter. This does two things: Firstly, it freezes all parameters of the pre-trained model such that only the parameters of the adapter are updated during training. Secondly, it activates the adapter so that it is used in the forward pass. Next, we can train our model the same way we would without an adapter. In the end, we can save our trained adapter as follows.</p>
<div class="codehilite"><pre><span></span><code><span class="n">model</span><span class="o">.</span><span class="n">save_adapter</span><span class="p">(</span><span class="s2">&quot;path/to/adapter&quot;</span><span class="p">,</span> <span class="s2">&quot;poem&quot;</span><span class="p">)</span>
</code></pre></div>

<p>We call <code>save_adapter()</code> and provide the path to the directory where the adapter should be saved and the name of the adapter we want to save.
Now that we have our trained adapter, we want to generate some poems and see what it has learned. First, we need to create a model with a language modeling head and load our trained adapter. Then we activate the loaded adapter.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2LMHeadModel</span><span class="p">,</span> <span class="n">GPT2Tokenizer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_adapter</span><span class="p">(</span><span class="s2">&quot;path/to/adapter&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">set_active_adapters</span><span class="p">(</span><span class="s2">&quot;poem&quot;</span><span class="p">)</span>
</code></pre></div>

<p>With <code>load_adapter()</code> we can load an adapter from the Hub by passing the name of the adapter specified in the hub. We can also load a local adapter by providing the path to the adapter. Then, we activate our adapter such that is used in the forward pass with <code>set_active_adapters()</code>.
Finally, we can think of a beginning of a poem and let the model finish it. In this case, the model generates 5 poems for the given beginning. We can choose the one we like most from those. We choose to start our poem with "In the night". One of the poems our model generated was:</p>
<blockquote>
<p>In the night;<br />
when the stars shine on her head.<br />
the mounds are deep,<br />
and the water's dark,<br />
and the water's cold<br />
and with her hand,<br />
with her lips,<br />
in song and song,<br />
the sound of the birds</p>
</blockquote>
<p>This can easily be applied to other datasets. Feel free to train your own adapter and upload it at the <a href="https://adapterhub.ml/">Hub</a> or browse the adapters trained by the community.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The new version 2.0 of the AdapterHub framework supports adapters for GPT-2 and BART. The support of these two models offers new possibilities in solving sequence to sequence tasks with adapters. To checkout AdapterHub and its other features, visit us on <a href="https://github.com/Adapter-Hub/adapter-transformers">GitHub</a>.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>We thank <a href="https://www.behance.net/andrefellenberg">Andr√© Fellenberg</a> for the BART illustration.</p>
<h2 id="references">References</h2>
<ul>
<li>Bapna, A., Arivazhagan, N., &amp; Firat, O. (2019). Simple, scalable adaptation for neural machine translation. EMNLP 2019, <a href="https://www.aclweb.org/anthology/D19-1165.pdf">https://www.aclweb.org/anthology/D19-1165.pdf</a></li>
<li>Chen, W., Chen, J., Su, Y., Chen, Z., &amp; Wang, W. Y. (2020). Logical natural language generation from open-domain tables. ACL 2020, <a href="https://www.aclweb.org/anthology/2020.acl-main.708.pdf">https://www.aclweb.org/anthology/2020.acl-main.708.pdf</a></li>
<li>Hermann, K. M., Koƒçisk√Ω, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., &amp; Blunsom, P. (2015). Teaching machines to read and comprehend. NeurIPS 2015 <a href="https://proceedings.neurips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html.">https://proceedings.neurips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html</a></li>
<li>Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., Laroussilhe, Q.D., Gesmundo, A., Attariyan, M., &amp; Gelly, S. (2019). Parameter-Efficient Transfer Learning for NLP. ICML 2019, <a href="http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf">http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf</a></li>
<li>Narayan, S., Cohen, S. B., &amp; Lapata, M. (2018). Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. EMNLP 2018, <a href="https://www.aclweb.org/anthology/D18-1206/">https://www.aclweb.org/anthology/D18-1206/</a></li>
<li>Pfeiffer, J., Kamath, A., R√ºckl√©, A., Cho, K., &amp; Gurevych, I. (2021). AdapterFusion: Non-Destructive Task Composition for Transfer Learning. EACL 2021, <a href="https://www.aclweb.org/anthology/2021.eacl-main.39.pdf">https://www.aclweb.org/anthology/2021.eacl-main.39.pdf</a></li>
<li>Philip‚Ä†, J., B√©rard, A., Gall√©, M., Besacier, L. (2020). Monolingual Adapters for Zero-Shot Neural Machine Translation. EMNLP 2020, <a href="https://www.aclweb.org/anthology/2020.emnlp-main.361.pdf">https://www.aclweb.org/anthology/2020.emnlp-main.361.pdf</a></li>
<li>See, A., Liu, P. J., &amp; Manning, C. D. (2017). Get to the point: Summarization with pointer-generator networks. ACL 2017, <a href="https://www.aclweb.org/anthology/P17-1099/">https://www.aclweb.org/anthology/P17-1099/</a></li>
<li>Sheng, E., &amp; Uthus, D. (2020). Investigating Societal Biases in a Poetry Composition System. Proceedings of the Second Workshop on Gender Bias in Natural Language Processing, <a href="https://www.aclweb.org/anthology/2020.gebnlp-1.9/">https://www.aclweb.org/anthology/2020.gebnlp-1.9/</a></li>
</ul>
<h2 id="citation">Citation</h2>
<div class="codehilite"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">sterz_2021</span><span class="p">,</span> 
  <span class="na">title</span><span class="p">=</span><span class="s">{Adapters for Generative and Seq2Seq Models in NLP}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://adapterhub.ml/blog/2021/04/adapters-for-generative-and-seq2seq-models-in-nlp/}</span><span class="p">,</span> 
  <span class="na">author</span><span class="p">=</span><span class="s">{Hannah Sterz and Clifton Poth and Andreas R\&quot;uckl\&#39;e and Jonas Pfeiffer}</span><span class="p">,</span> 
  <span class="na">year</span><span class="p">=</span><span class="s">{2021}</span><span class="p">,</span> 
  <span class="na">month</span><span class="p">=</span><span class="s">{Apr}</span>
<span class="p">}</span>
</code></pre></div>

<p>* equal contribution</p>
        </div>

        <div class="my-5">
            <a href="https://github.com/adapter-hub/website/blob/master/posts/2021/04/adapters-for-generative-and-seq2seq-models-in-nlp.md">
                <i class="fa fa-edit"></i>&nbsp; Edit Post on GitHub
            </a>
        </div>
        
        <div id="col-lg-10 comments">
            <script src="https://utteranc.es/client.js"
                repo="Adapter-Hub/website"
                issue-term="pathname"
                label="blog-post üí¨"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>
    </div>

</div>


</div>

<footer>
    <div class="container">
        <p class="float-md-right text-center text-md-right">
            <a href="https://arxiv.org/abs/2311.11077" target="_blank">Paper</a>
            <!--<span class="text-black-30 px-1">|</span>
            <a href="/imprint-privacy/">Imprint & Privacy</a>-->
        </p>
        <p class="text-muted text-center text-md-left">Brought to you with ‚ù§Ô∏è by the AdapterHub Team</p>
    </div>
</footer>

<script>
    document.addEventListener('DOMContentLoaded', function (event) {
        codecopy('pre') // your code tag selector!
        $('.activate-tooltip').tooltip();   // bootstrap tooltips
    })
</script>
<script src="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.js" data-cfasync="false"></script>
<script>
    window.cookieconsent.initialise({
    "palette": {
        "popup": {
        "background": "#d7f0f4"
        },
        "button": {
        "background": "#39b3c6",
        "text": "#ffffff"
        }
    },
    "theme": "classic"
    });
</script>
</body>
</html>