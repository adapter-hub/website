<!doctype html>
<html lang="en">

<head>

    
        <!-- Global site tag (gtag.js)  -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-169190162-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-169190162-1');
        </script>
    

    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    
<meta name="description" content="Introducing the new Adapters library the new package that supports adding parameter-efficient fine-tuning methods on top of transformers models and composition to achieve modular setups.
" />


    <link rel="icon" type="image/png" href="/static/adapter-bert-head.png" />

    <!-- Font Awesome -->
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">
    <!-- Pygments -->
    <link rel="stylesheet" href="/pygments.css">
    <!-- CSS -->
    
        <link rel="stylesheet" href="/static/gen/packed.css?c557e741">
    
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.css" />
    <!-- JS -->
    
        <script type="text/javascript" src="/static/gen/packed.js?2800c204"></script>
    
    <!-- MathJax -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          config: ["MMLorHTML.js"],
          jax: ["input/TeX", "output/HTML-CSS", "output/NativeMML"],
          extensions: ["MathMenu.js", "MathZoom.js"]
        });
    </script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js"></script>

    <title>AdapterHub - Introducing Adapters</title>
</head>

<body>

<nav class="navbar navbar-expand-md navbar-light bg-light">
    <div class="container">
        <a class="navbar-brand p-0" href="/">
            <img src="/static/adapter-bert.png" width="28" class="bert"/>
            <span class="align-middle">AdapterHub</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
                aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fas fa-bars text-dark" style="font-size:28px"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarNav">
            <ul class="navbar-nav ml-auto">
                <div class="dropdown-divider d-md-none"></div>
                <li class="nav-item">
                    <a class="nav-link" href="/explore/">
                        <i class="fas fa-binoculars"></i>&nbsp; Explore
                    </a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="https://docs.adapterhub.ml/">
                        <i class="fas fa-book"></i>&nbsp; Docs
                    </a>
                </li>
                <li class="nav-item active">
                    <a class="nav-link" href="/blog/">
                        <i class="fas fa-bullhorn"></i>&nbsp; Blog
                    </a>
                </li>
                <li class="nav-item separator d-none d-md-inline-block"></li>
                <li class="nav-item nav-secondary justify-content-end d-none d-md-inline-block">
                    <a class="nav-link" href="https://github.com/adapter-hub">
                        <i class="fab fa-github"></i>&nbsp;
                    </a>
                </li>
                <li class="nav-item nav-secondary justify-content-end d-none d-md-inline-block">
                    <a class="nav-link" href="https://twitter.com/adapterhub">
                        <i class="fab fa-twitter"></i>&nbsp;
                    </a>
                </li>
            </ul>
        </div>
    </div>
</nav>

<div id="Banner"> </div>

<div id="Header" class="jumbotron jumbotron-fluid py-lg-5">
    <div class="container">
        
<div class="row breadcrumb-nav mb-3">
    <nav aria-label="breadcrumb" class="col">
        <ol class="breadcrumb bg-transparent">
            <li class="breadcrumb-item">
                <a href="/blog/">Blog</a>
            </li>
        </ol>
    </nav>
</div>

<h1>Introducing Adapters</h1>
<ul class="blog-post-bar mt-4 mb-0">
    <li><i class="fa fa-calendar-alt"></i>&nbsp; 2023-11-24</li>
    <li><i class="fa fa-user"></i>&nbsp;
    
        Hannah Sterz
        
        &nbsp;<a href="https://twitter.com/@h_sterz"><i class="fab fa-twitter"></i></a>
        
        
        ,&nbsp;
        
    
        Leon Engl√§nder
        
        &nbsp;<a href="https://twitter.com/@LeonEnglaender"><i class="fab fa-twitter"></i></a>
        
        
        ,&nbsp;
        
    
        Timo Imhof
        
        &nbsp;<a href="https://twitter.com/@timo_imhof"><i class="fab fa-twitter"></i></a>
        
        
        ,&nbsp;
        
    
        Clifton Poth,
        
        &nbsp;<a href="https://twitter.com/@clifapt"><i class="fab fa-twitter"></i></a>
        
        
        ,&nbsp;
        
    
        Jonas Pfeiffer
        
        &nbsp;<a href="https://twitter.com/@PfeiffJo"><i class="fab fa-twitter"></i></a>
        
        
    
    </li>
</ul>

    </div>
</div>

<div class="container pb-3 pb-md-5">
    
    

<div class="row">
    <div class="col-lg-10 my-2">
        
            <a href="https://arxiv.org/pdf/2311.11077.pdf" class="d-block mb-4">
                <div class="code text-black">
                    Poth, C., Sterz, H., Paul, I., Purkayastha, S., Engl√§nder, L., Imhof, T., Vuli¬¥c, I., Ruder, S., Gurevych, I., &amp; Pfeiffer, J. (2023). Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning.
                </div>
            </a>
        

        <div class="blog-post-content">
            <p>We are happy to announce <em>Adapters</em>, the new library at the heart of the AdapterHub framework.
<em>Adapters</em> stands in direct tradition to our previous work with the <code>adapter-transformers</code> library while simultaneously revamping the implementation from the ground up and smoothing many rough edges of the previous library.
This blog post summarizes the most important aspects of <em>Adapters</em>, as described in detail <a href="https://arxiv.org/abs/2311.11077">in our paper</a> (to be presented as a system demo at EMNLP 2023).</p>
<p>In the summer of 2020, when we released the first version of <em>AdapterHub</em>, along with the <code>adapter-transformers</code> library, adapters and parameter-efficient fine-tuning<sup id="fnref:peft"><a class="footnote-ref" href="#fn:peft">1</a></sup> were still a niche research topic.
Adapters were first introduced to Transformer models a few months earlier (Houlsby et al., 2019) and <em>AdapterHub</em> was the very first framework to provide comprehensive tools for working with adapters, dramatically lowering the barrier of training own adapters or leveraging pre-trained ones.</p>
<p>In the now more than three years following, <em>AdapterHub</em> has increasingly gained traction within the NLP community, being <a href="https://github.com/adapter-hub/adapters/stargazers">liked by thousands</a> and <a href="https://www.semanticscholar.org/paper/AdapterHub%3A-A-Framework-for-Adapting-Transformers-Pfeiffer-R%C3%BCckl%C3%A9/063f8b1ecf2394ca776ac61869734de9c1953808?utm_source=direct_link">used by hundreds</a> for their research.
However, the field of parameter-efficient fine-tuning has grown even faster.
Nowadays, with recent LLMs growing ever larger in size, adapter methods, which do not fine-tune the full model, but instead only update a small number of parameters, have become increasingly mainstream.
<a href="https://github.com/calpt/awesome-adapter-resources">Multiple libraries, dozens of architectures and scores of applications</a> compose a flourishing subfield of LLM research.</p>
<p>Besides parameter-efficiency, modularity is a second important characteristic of adapters <a href="https://arxiv.org/pdf/2302.11529.pdf">(Pfeiffer et al., 2023)</a>.
Sadly, this is overlooked by many existing tools.
From the beginning on, <em>AdapterHub</em> paid special attention to adapter modularity and composition, integrating setups like MAD-X (Pfeiffer et al., 2020).
<em>Adapters</em> continues and expands this focus on modularity.</p>
<h2 id="the-library">The Library</h2>
<p><em>Adapters</em> is a self-contained library supporting a diverse set of adapter methods, integrating them into many common Transformer architectures and allowing flexible and complex adapter configuration.
Modular transfer learning can be achieved by combining adapters via six different composition blocks.</p>
<p>All in all, <em>Adapters</em> offers substantial improvements compared to the initial <code>adapter-transformers</code> library:</p>
<ol>
<li>Decoupled from the HuggingFace <code>transformers</code> library</li>
<li>Support of 10 adapter methods</li>
<li>Support of 6 composition blocks</li>
<li>Support of 20 diverse models</li>
</ol>
<p><em>Adapters</em> can be easily installed via pip:</p>
<div class="codehilite"><pre><span></span><code>pip install adapters
</code></pre></div>

<p>The source code of <em>Adapters</em> can be found <a href="https://github.com/adapter-hub/adapters">on GitHub</a>.</p>
<p>In the following, we highlight important components of <em>Adapters</em>.
If you have used <code>adapter-transformers</code> before, much of this will look familiar.
In this case, you might directly want to jump to our <a href="https://docs.adapterhub.ml/transitioning.html">transitioning guide</a>, which highlights relevant differences between <em>Adapters</em> and <code>adapter-transformers</code>.
The additions and changes compared to the latest version of <code>adapter-transformers</code> can also be found <a href="https://github.com/adapter-hub/adapters/releases/tag/v0.1.0">in our release notes</a>.</p>
<h2 id="transformers-integration">Transformers Integration</h2>
<p><em>Adapters</em> acts as an add-on to HuggingFace's Transformers library.
As a result, existing Transformers models can be easily attached with adapter functionality as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">adapters</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;t5-base&quot;</span><span class="p">)</span>
<span class="n">adapters</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="c1"># Adding adapter-specific functionality</span>

<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;adapter0&quot;</span><span class="p">)</span>
</code></pre></div>

<p>However, we recommend using the model classes provided by <em>Adapters</em>, such as <code>XXXAdapterModel</code>, where "XXX" denotes the model architecture, e.g., Bert.
These models provide the adapter functionality without further initialization and support multiple heads.
The latter is especially relevant when using composition blocks which can handle multiple outputs, for instance, the BatchSplit composition block. Here's an example of how to use such an <code>XXXAdapterModel</code> class:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">adapters</span> <span class="kn">import</span> <span class="n">AutoAdapterModel</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoAdapterModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;roberta-base&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;adapter1&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="s2">&quot;seq_bn&quot;</span><span class="p">)</span> <span class="c1"># add the new adapter to the model</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_classification_head</span><span class="p">(</span><span class="s2">&quot;adapter1&quot;</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># add a sequence classification head</span>

<span class="n">model</span><span class="o">.</span><span class="n">train_adapter</span><span class="p">(</span><span class="s2">&quot;adapter1&quot;</span><span class="p">)</span> <span class="c1"># freeze the model weights and activate the adapter</span>
</code></pre></div>

<h2 id="adapter-methods">Adapter Methods</h2>
<p>Each adapter method is defined by a configuration object or string, allowing for flexible customization of various adapter module properties, including placement, capacity, residual connections, initialization, etc. We distinguish between single methods consisting of one type of adapter module and complex methods consisting of multiple different adapter module types.</p>
<h3 id="single-methods">Single Methods</h3>
<p><em>Adapters</em> supports single adapter methods that introduce parameters in new feed-forward modules such as bottleneck adapters (Houlsby et al., 2019), introduce prompts at different locations such as prefix tuning (Li and Liang, 2021), reparameterize existing modules such as LoRA (Hu et al., 2022) or re-scale their output representations such as (IA)¬≥ (Liu et al., 2022). For more information, see our <a href="https://docs.adapterhub.ml/methods.html">documentation</a>.</p>
<p>All adapter methods can be added to a model by the unified <code>add_adapter()</code> method, e.g.:</p>
<div class="codehilite"><pre><span></span><code><span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;adapter2&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="s2">&quot;seq_bn&quot;</span><span class="p">)</span>
</code></pre></div>

<p>Alternatively, a config class, along with custom parameters:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">adapters</span> <span class="kn">import</span> <span class="n">PrefixTuningConfig</span>

<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;adapter3&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">PrefixTuningConfig</span><span class="p">(</span><span class="n">prefix_length</span><span class="o">=</span><span class="mi">20</span><span class="p">))</span>
</code></pre></div>

<p>The following table gives an overview of many currently supported single methods, along with their configuration class and configuration string<sup id="fnref:brackets"><a class="footnote-ref" href="#fn:brackets">2</a></sup>:</p>
<table>
<thead>
<tr>
<th>Identifier</th>
<th>Configuration class</th>
<th>More information</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>[double_]seq_bn</code></td>
<td><code>[Double]SeqBnConfig()</code></td>
<td><a href="https://docs.adapterhub.ml/methods.html#bottleneck-adapters">Bottleneck Adapters</a></td>
</tr>
<tr>
<td><code>par_bn</code></td>
<td><code>ParBnConfig()</code></td>
<td><a href="https://docs.adapterhub.ml/methods.html#bottleneck-adapters">Bottleneck Adapters</a></td>
</tr>
<tr>
<td><code>[double_]seq_bn_inv</code></td>
<td><code>[DoubleSeq]BnInvConfig()</code></td>
<td><a href="https://docs.adapterhub.ml/methods.html#language-adapters---invertible-adapters">Invertible Adapters</a></td>
</tr>
<tr>
<td><code>compacter[++]</code></td>
<td><code>Compacter[PlusPlus]Config()</code></td>
<td><a href="https://docs.adapterhub.ml/methods.html#compacter">Compacter</a></td>
</tr>
<tr>
<td><code>prefix_tuning</code></td>
<td><code>PrefixTuningConfig()</code></td>
<td><a href="https://docs.adapterhub.ml/methods.html#prefix-tuning">Prefix Tuning</a></td>
</tr>
<tr>
<td><code>lora</code></td>
<td><code>LoRAConfig()</code></td>
<td><a href="https://docs.adapterhub.ml/methods.html#lora">LoRA</a></td>
</tr>
<tr>
<td><code>ia3</code></td>
<td><code>IA3Config()</code></td>
<td><a href="https://docs.adapterhub.ml/methods.html#ia-3">IA¬≥</a></td>
</tr>
<tr>
<td><code>mam</code></td>
<td><code>MAMConfig()</code></td>
<td><a href="method_combinations.html#mix-and-match-adapters">Mix-and-Match Adapters</a></td>
</tr>
<tr>
<td><code>unipelt</code></td>
<td><code>UniPELTConfig()</code></td>
<td><a href="method_combinations.html#unipelt">UniPELT</a></td>
</tr>
<tr>
<td><code>prompt_tuning</code></td>
<td><code>PromptTuningConfig()</code></td>
<td><a href="https://docs.adapterhub.ml/methods.html#prompt_tuning">Prompt Tuning</a></td>
</tr>
</tbody>
</table>
<p>For more details on all adapter methods, visit <a href="https://docs.adapterhub.ml/methods.html">our documentation</a>.</p>
<h3 id="complex-methods">Complex Methods</h3>
<p>While different efficient fine-tuning methods and configurations have often been proposed as standalone, combining them for joint training has proven to be beneficial (He et al., 2022; Mao et al., 2022). To make this process easier, Adapters provides the possibility to group multiple configuration instances using the <code>ConfigUnion</code> class. This flexible mechanism allows easy integration of multiple complex methods proposed in the literature (as the two examples outlined below), as well as the construction of other, new complex configurations currently not available nor benchmarked in the literature (Zhou et al., 2023).</p>
<p>Example: <strong>Mix-and-Match Adapters</strong> (He et al., 2022) were proposed as a combination of Prefix-Tuning and parallel bottleneck adapters. Using <code>ConfigUnion</code>, this method can be defined as:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">adapters</span> <span class="kn">import</span> <span class="n">ConfigUnion</span><span class="p">,</span> <span class="n">PrefixTuningConfig</span><span class="p">,</span> <span class="n">ParBnConfig</span><span class="p">,</span> <span class="n">AutoAdapterModel</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoAdapterModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;microsoft/deberta-v3-base&quot;</span><span class="p">)</span>

<span class="n">adapter_config</span> <span class="o">=</span> <span class="n">ConfigUnion</span><span class="p">(</span>
    <span class="n">PrefixTuningConfig</span><span class="p">(</span><span class="n">prefix_length</span><span class="o">=</span><span class="mi">20</span><span class="p">),</span>
    <span class="n">ParBnConfig</span><span class="p">(</span><span class="n">reduction_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;my_adapter&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">adapter_config</span><span class="p">,</span> <span class="n">set_active</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>

<p>Learn more about complex adapter configurations using <code>ConfigUnion</code> <a href="https://docs.adapterhub.ml/method_combinations.html">in our documentation</a>.</p>
<h2 id="modularity-and-composition-blocks">Modularity and Composition Blocks</h2>
<figure id="_caption-1">
<img alt="" src="/static/images/composition.png" title="Composition Blocks" />
<figcaption><span>Figure&nbsp;1:</span> Composition Blocks</figcaption>
</figure>
<p>While the modularity and composability aspect of adapters have seen increasing interest in research, existing open-source libraries (Mangrulkar et al., 2022; Hu et al., 2023a) have largely overlooked these aspects. Adapters makes adapter compositions a central and accessible part of working with adapters by enabling the definition of complex, composed adapter setups. We define a set of simple composition blocks that each capture a specific method of aggregating the functionality of multiple adapters. Each composition block class takes a sequence of adapter identifiers plus optional configuration as arguments. The defined adapter setup is then parsed at runtime by Adapters to allow for dynamic switching between adapters per forward pass. Above, the different composition blocks are illustrated.</p>
<p>An example composition could look as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">adapters.composition</span> <span class="k">as</span> <span class="nn">ac</span>

<span class="c1"># ...</span>

<span class="n">config</span> <span class="o">=</span> <span class="s2">&quot;mam&quot;</span> <span class="c1"># mix-and-match adapters</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">set_active_adapters</span><span class="p">(</span><span class="n">ac</span><span class="o">.</span><span class="n">Stack</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="n">ac</span><span class="o">.</span><span class="n">Parallel</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">)))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">active_adapters</span><span class="p">)</span> <span class="c1"># The active config is: Stack[a, Parallel[b, c]]</span>
</code></pre></div>

<p>With this setup activated, inputs in each layer would first flow through adapter "a" before being forwarded though "b" and "c" in parallel.</p>
<p>To learn more, check out <a href="https://adapterhub.ml/blog/2021/04/version-2-of-adapterhub-released/">this blog post</a> and <a href="https://docs.adapterhub.ml/adapter_composition.html">our documentation</a>. </p>
<h2 id="evaluating-adapter-performance">Evaluating Adapter Performance</h2>
<figure id="_caption-2">
<img alt="" src="/static/images/eval_results.png" title="Performance of different adapter architectures overdiffernt tasks evaluated with the RoBERTa model." />
<figcaption><span>Figure&nbsp;2:</span> Performance of different adapter architectures overdiffernt tasks evaluated with the RoBERTa model.</figcaption>
</figure>
<p>In addition to the aforementioned ease of use, we show that the adapter methods offered by our library are performant across a range of settings. To this end, we conduct evaluations on the single adapter implementations made available by Adapters.</p>
<p>Results are shown in Figure 2. The obvious takeaway from our evaluations is that all adapter implementations offered by our framework are competitive with full model fine-tuning, across all task classes. Approaches that offer more tunable hyper-parameters (and thus allow for easy scaling), such as Bottleneck adapters, LoRA, and Prefix Tuning predictably have the highest topline performance, often surpassing full fine-tuning. However, extremely parameter-frugal methods like (IA)3, which add &lt; 0.005% of the parameters of the base model, also perform commendably and only fall short by a small fraction. Finally, the Compacter is the least volatile among the single methods, obtaining the lowest standard deviation between runs on the majority of tasks.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The field of adapter/ PEFT methods will continue to advance rapidly and gain importance.
Already today, various interesting and promising approaches are not yet covered by <em>AdapterHub</em> and the <em>Adapters</em> library.
However, <em>Adapters</em> aims to provide a new solid foundation for research and application of adapters, upon which new and extended methods can be successively added in the future.
<em>Adapters</em> has a clear focus on parameter-efficiency <em>and</em> modularity of adapters and builds on the rich and successful history of <em>AdapterHub</em> and <code>adapter-transformers</code>.
In the end, integrating the latest great research into the library is a community effort, and we invite you to <a href="https://docs.adapterhub.ml/contributing.html">contribute in one of many possible ways</a>.</p>
<h2 id="references">References</h2>
<ul>
<li>He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., &amp; Neubig, G. (2021, October). Towards a Unified View of Parameter-Efficient Transfer Learning. In International Conference on Learning Representations.</li>
<li>Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., Laroussilhe, Q.D., Gesmundo, A., Attariyan, M., &amp; Gelly, S. (2019). Parameter-Efficient Transfer Learning for NLP. ICML.</li>
<li>Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., &amp; Chen, W. (2021, October). LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations.</li>
<li>Li, X. L., &amp; Liang, P. (2021, August). Prefix-Tuning: Optimizing Continuous Prompts for Generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (pp. 4582-4597).</li>
<li>Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M., &amp; Raffel, C. A. (2022). Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35, 1950-1965.</li>
<li>Mao, Y., Mathias, L., Hou, R., Almahairi, A., Ma, H., Han, J., ... &amp; Khabsa, M. (2022, May). UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 6253-6264).</li>
<li>Pfeiffer, J., Vulic, I., Gurevych, I., &amp; Ruder, S. (2020). MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer. ArXiv, abs/2005.00052.</li>
<li>Pfeiffer, J., Ruder, S., Vulic, I., &amp; Ponti, E. (2023). Modular Deep Learning. ArXiv, abs/2302.11529.</li>
<li>Zhou, H., Wan, X., Vuliƒá, I., &amp; Korhonen, A. (2023). AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning. arXiv preprint arXiv:2301.12132.</li>
</ul>
<h2 id="citation">Citation</h2>
<div class="codehilite"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">poth-etal-2023-adapters</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">&quot;Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning&quot;</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">{Poth, Clifton  and</span>
<span class="s">      Sterz, Hannah  and</span>
<span class="s">      Paul, Indraneil  and</span>
<span class="s">      Purkayastha, Sukannya  and</span>
<span class="s">      Engl{\&quot;a}nder, Leon  and</span>
<span class="s">      Imhof, Timo  and</span>
<span class="s">      Vuli{\&#39;c}, Ivan  and</span>
<span class="s">      Ruder, Sebastian  and</span>
<span class="s">      Gurevych, Iryna  and</span>
<span class="s">      Pfeiffer, Jonas}</span><span class="p">,</span>
    <span class="na">booktitle</span> <span class="p">=</span> <span class="s">&quot;Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations&quot;</span><span class="p">,</span>
    <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
    <span class="na">year</span> <span class="p">=</span> <span class="s">&quot;2023&quot;</span><span class="p">,</span>
    <span class="na">address</span> <span class="p">=</span> <span class="s">&quot;Singapore&quot;</span><span class="p">,</span>
    <span class="na">publisher</span> <span class="p">=</span> <span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
    <span class="na">url</span> <span class="p">=</span> <span class="s">&quot;https://aclanthology.org/2023.emnlp-demo.13&quot;</span><span class="p">,</span>
    <span class="na">pages</span> <span class="p">=</span> <span class="s">&quot;149--160&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>

<div class="footnote">
<hr />
<ol>
<li id="fn:peft">
<p>We use the terms <em>parameter-efficient fine-tuning (PEFT)</em> and <em>adapter</em> interchangeably throughout this post and in all of our documents.&#160;<a class="footnote-backref" href="#fnref:peft" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:brackets">
<p>Options for identifiers and classes are given in brackets.&#160;<a class="footnote-backref" href="#fnref:brackets" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>
        </div>

        <div class="my-5">
            <a href="https://github.com/adapter-hub/website/blob/master/posts/2023/11/introducing-adapters.md">
                <i class="fa fa-edit"></i>&nbsp; Edit Post on GitHub
            </a>
        </div>
        
        <div id="col-lg-10 comments">
            <script src="https://utteranc.es/client.js"
                repo="Adapter-Hub/website"
                issue-term="pathname"
                label="blog-post üí¨"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>
    </div>

</div>


</div>

<footer>
    <div class="container">
        <p class="float-md-right text-center text-md-right">
            <a href="https://arxiv.org/abs/2311.11077" target="_blank">Paper</a>
            <!--<span class="text-black-30 px-1">|</span>
            <a href="/imprint-privacy/">Imprint & Privacy</a>-->
        </p>
        <p class="text-muted text-center text-md-left">Brought to you with ‚ù§Ô∏è by the AdapterHub Team</p>
    </div>
</footer>

<script>
    document.addEventListener('DOMContentLoaded', function (event) {
        codecopy('pre') // your code tag selector!
        $('.activate-tooltip').tooltip();   // bootstrap tooltips
    })
</script>
<script src="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.js" data-cfasync="false"></script>
<script>
    window.cookieconsent.initialise({
    "palette": {
        "popup": {
        "background": "#d7f0f4"
        },
        "button": {
        "background": "#39b3c6",
        "text": "#ffffff"
        }
    },
    "theme": "classic"
    });
</script>
</body>
</html>