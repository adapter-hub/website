<!doctype html>
<html lang="en">

<head>

    
        <!-- Global site tag (gtag.js)  -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-169190162-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-169190162-1');
        </script>
    

    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
     

    <link rel="icon" type="image/png" href="/static/adapter-bert-head.png" />

    <!-- Font Awesome -->
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css">
    <!-- Pygments -->
    <link rel="stylesheet" href="/pygments.css">
    <!-- CSS -->
    
        <link rel="stylesheet" href="/static/gen/packed.css?c557e741">
    
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.css" />
    <!-- JS -->
    
        <script type="text/javascript" src="/static/gen/packed.js?2800c204"></script>
    
    <!-- MathJax -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          config: ["MMLorHTML.js"],
          jax: ["input/TeX", "output/HTML-CSS", "output/NativeMML"],
          extensions: ["MathMenu.js", "MathZoom.js"]
        });
    </script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js"></script>

    <title>AdapterHub - CoLA</title>
</head>

<body>

<nav class="navbar navbar-expand-md navbar-light bg-light">
    <div class="container">
        <a class="navbar-brand p-0" href="/">
            <img src="/static/adapter-bert.png" width="28" class="bert"/>
            <span class="align-middle">AdapterHub</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
                aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fas fa-bars text-dark" style="font-size:28px"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarNav">
            <ul class="navbar-nav ml-auto">
                <div class="dropdown-divider d-md-none"></div>
                <li class="nav-item active">
                    <a class="nav-link" href="/explore/">
                        <i class="fas fa-binoculars"></i>&nbsp; Explore
                    </a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="https://docs.adapterhub.ml/">
                        <i class="fas fa-book"></i>&nbsp; Docs
                    </a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="/blog/">
                        <i class="fas fa-bullhorn"></i>&nbsp; Blog
                    </a>
                </li>
                <li class="nav-item separator d-none d-md-inline-block"></li>
                <li class="nav-item nav-secondary justify-content-end d-none d-md-inline-block">
                    <a class="nav-link" href="https://github.com/adapter-hub">
                        <i class="fab fa-github"></i>&nbsp;
                    </a>
                </li>
                <li class="nav-item nav-secondary justify-content-end d-none d-md-inline-block">
                    <a class="nav-link" href="https://twitter.com/adapterhub">
                        <i class="fab fa-twitter"></i>&nbsp;
                    </a>
                </li>
            </ul>
        </div>
    </div>
</nav>

<div id="Banner"> </div>

<div id="Header" class="jumbotron jumbotron-fluid py-lg-5">
    <div class="container">
        
<div class="row breadcrumb-nav mb-3">
    <nav aria-label="breadcrumb" class="col">
        <ol class="breadcrumb bg-transparent">
            <li class="breadcrumb-item">
                <a href="/explore/">Explore</a>
            </li>
            <li class="breadcrumb-item">
                <a href="/explore/text_task/">Task</a>
            </li>













        </ol>
    </nav>
</div>

<div class="row adapter-nav" id="ModelTypeNav">
    <div class="col-md-12">
        <div class="row">
            <div class="col-md-auto mr-5 mb-2">
                <h1>
                    Task Adapters
                </h1>
            </div>
            <div class="col-md-auto">
                <div class="navigation row">
                    <div class="col-xs-auto ml-3">
                        <div class="row">
                            <div class="col-auto">
                                <p>Pre-trained model: </p>
                            </div>
                            <div class="col">
                                <div class="dropdown show mr-2 mb-1">
                                    <a class="btn dropdown-toggle" href="#" role="button" id="dropdownMenuLink1" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                                        All architectures
                                    </a>
                                    <div class="dropdown-menu" aria-labelledby="dropdownMenuLink1">
                                        <a class="dropdown-item active"
                                           href="/explore/lingaccept/cola/">
                                            All architectures
                                        </a>
                                        <div class="dropdown-divider"></div>
                                        
                                            <a class="dropdown-item "
                                               href="/explore/lingaccept/cola/bert/">
                                                bert
                                            </a>
                                        
                                            <a class="dropdown-item "
                                               href="/explore/lingaccept/cola/xlm-roberta/">
                                                xlm-roberta
                                            </a>
                                        
                                            <a class="dropdown-item "
                                               href="/explore/lingaccept/cola/distilbert/">
                                                distilbert
                                            </a>
                                        
                                            <a class="dropdown-item "
                                               href="/explore/lingaccept/cola/gpt2/">
                                                gpt2
                                            </a>
                                        
                                            <a class="dropdown-item "
                                               href="/explore/lingaccept/cola/bart/">
                                                bart
                                            </a>
                                        
                                            <a class="dropdown-item "
                                               href="/explore/lingaccept/cola/roberta/">
                                                roberta
                                            </a>
                                        
                                            <a class="dropdown-item "
                                               href="/explore/lingaccept/cola/mbart/">
                                                mbart
                                            </a>
                                        
                                    </div>
                                </div>
                                
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <div class="row mt-3">
            <div class="col-md-auto mr-5">
                <h2 class="mt-0">
                    CoLA
                </h2>
            </div>
            <div class="col-md documentation mt-2 mt-md-1">
                <i class="fa fa-book"></i>
                The Corpus of Linguistic Acceptability (CoLA) in its full form consists of 10657 sentences from 23 linguistics publications, expertly annotated for acceptability (grammaticality) by their original authors.
The public version provided here contains 9594 sentences belonging to training and development sets, and excludes 1063 sentences belonging to a held out test set.

            </div>
        </div>
        <div class="row mt-2 subtask-button-list">
            
            <a class="btn btn-outline-light" href="https://nyu-mll.github.io/CoLA/"><i class="fa fa-globe"></i>&nbsp; Website</a>
            
            
        </div>
    </div>
</div>

    </div>
</div>

<div class="container pb-3 pb-md-5">
    
    
<div class="row adapters">
    
        <div class="col-lg-6 adapter my-lg-3 my-2">
            <a class="btn card bg-light border-0" href="/adapters/ukp/bert-base-uncased_lingaccept_cola_pfeiffer/">
                <div class="card-body">
                    <h5 class="card-title">
                        
                        lingaccept/cola@ukp
                        
                        <span class="notice">
                            bert-base-uncased
                        </span>
                    </h5>
                    <div class="features">
                        
                            <span class="badge badge-primary">1 version</span>
                            <span class="badge badge-primary">Architecture: pfeiffer</span>
                            
                            
                        
                        <span class="badge badge-primary">Head:&nbsp;
                            
                            <i class="fas fa-check"></i>
                            
                        </span>
                    </div>
                    <p class="card-text">
                        
                            Adapter in Pfeiffer architecture trained on the CoLA task for 20 epochs with early stopping and a learning rate of 1e-4.
See https://arxiv.org/pdf/2007.07779.pdf.

                        
                    </p>
                </div>
            </a>
        </div>
    
        <div class="col-lg-6 adapter my-lg-3 my-2">
            <a class="btn card bg-light border-0" href="/adapters/ukp/distilbert-base-uncased_lingaccept_cola_houlsby/">
                <div class="card-body">
                    <h5 class="card-title">
                        
                        lingaccept/cola@ukp
                        
                        <span class="notice">
                            distilbert-base-uncased
                        </span>
                    </h5>
                    <div class="features">
                        
                            <span class="badge badge-primary">1 version</span>
                            <span class="badge badge-primary">Architecture: houlsby</span>
                            
                            
                        
                        <span class="badge badge-primary">Head:&nbsp;
                            
                            <i class="fas fa-check"></i>
                            
                        </span>
                    </div>
                    <p class="card-text">
                        
                            Adapter for distilbert-base-uncased in Houlsby architecture trained on the CoLA dataset for 15 epochs with early stopping and a learning rate of 1e-4.

                        
                    </p>
                </div>
            </a>
        </div>
    
        <div class="col-lg-6 adapter my-lg-3 my-2">
            <a class="btn card bg-light border-0" href="/adapters/ukp/roberta-base-cola_pfeiffer/">
                <div class="card-body">
                    <h5 class="card-title">
                        
                        lingaccept/cola@ukp
                        
                        <span class="notice">
                            roberta-base
                        </span>
                    </h5>
                    <div class="features">
                        
                            <span class="badge badge-primary">1 version</span>
                            <span class="badge badge-primary">Architecture: pfeiffer</span>
                            
                            
                        
                        <span class="badge badge-primary">Head:&nbsp;
                            
                            <i class="fas fa-check"></i>
                            
                        </span>
                    </div>
                    <p class="card-text">
                        
                            Adapter (with head) trained using the `run_glue.py` script with an extension that retains the best checkpoint (out of 30 epochs).

                        
                    </p>
                </div>
            </a>
        </div>
    
        <div class="col-lg-6 adapter my-lg-3 my-2">
            <a class="btn card bg-light border-0" href="/adapters/ukp/bert-base-uncased_lingaccept_cola_houlsby/">
                <div class="card-body">
                    <h5 class="card-title">
                        
                        lingaccept/cola@ukp
                        
                        <span class="notice">
                            bert-base-uncased
                        </span>
                    </h5>
                    <div class="features">
                        
                            <span class="badge badge-primary">1 version</span>
                            <span class="badge badge-primary">Architecture: houlsby</span>
                            
                            
                        
                        <span class="badge badge-primary">Head:&nbsp;
                            
                            <i class="fas fa-check"></i>
                            
                        </span>
                    </div>
                    <p class="card-text">
                        
                            Adapter in Houlsby architecture trained on the CoLA task for 20 epochs with early stopping and a learning rate of 1e-4.
See https://arxiv.org/pdf/2007.07779.pdf.

                        
                    </p>
                </div>
            </a>
        </div>
    
        <div class="col-lg-6 adapter my-lg-3 my-2">
            <a class="btn card bg-light border-0" href="/adapters/ukp/roberta-base-cola_houlsby/">
                <div class="card-body">
                    <h5 class="card-title">
                        
                        lingaccept/cola@ukp
                        
                        <span class="notice">
                            roberta-base
                        </span>
                    </h5>
                    <div class="features">
                        
                            <span class="badge badge-primary">1 version</span>
                            <span class="badge badge-primary">Architecture: houlsby</span>
                            
                            
                        
                        <span class="badge badge-primary">Head:&nbsp;
                            
                            <i class="fas fa-check"></i>
                            
                        </span>
                    </div>
                    <p class="card-text">
                        
                            Adapter (with head) trained using the `run_glue.py` script with an extension that retains the best checkpoint (out of 30 epochs).

                        
                    </p>
                </div>
            </a>
        </div>
    
        <div class="col-lg-6 adapter my-lg-3 my-2">
            <a class="btn card bg-light border-0" href="/adapters/ukp/roberta-large-cola_houlsby/">
                <div class="card-body">
                    <h5 class="card-title">
                        
                        lingaccept/cola@ukp
                        
                        <span class="notice">
                            roberta-large
                        </span>
                    </h5>
                    <div class="features">
                        
                            <span class="badge badge-primary">1 version</span>
                            <span class="badge badge-primary">Architecture: houlsby</span>
                            
                            
                        
                        <span class="badge badge-primary">Head:&nbsp;
                            
                            <i class="fas fa-check"></i>
                            
                        </span>
                    </div>
                    <p class="card-text">
                        
                            Adapter (with head) trained using the `run_glue.py` script with an extension that retains the best checkpoint (out of 30 epochs).

                        
                    </p>
                </div>
            </a>
        </div>
    
        <div class="col-lg-6 adapter my-lg-3 my-2">
            <a class="btn card bg-light border-0" href="/adapters/ukp/distilbert-base-uncased_lingaccept_cola_pfeiffer/">
                <div class="card-body">
                    <h5 class="card-title">
                        
                        lingaccept/cola@ukp
                        
                        <span class="notice">
                            distilbert-base-uncased
                        </span>
                    </h5>
                    <div class="features">
                        
                            <span class="badge badge-primary">1 version</span>
                            <span class="badge badge-primary">Architecture: pfeiffer</span>
                            
                            
                        
                        <span class="badge badge-primary">Head:&nbsp;
                            
                            <i class="fas fa-check"></i>
                            
                        </span>
                    </div>
                    <p class="card-text">
                        
                            Adapter for distilbert-base-uncased in Pfeiffer architecture trained on the CoLA dataset for 15 epochs with early stopping and a learning rate of 1e-4.

                        
                    </p>
                </div>
            </a>
        </div>
    
        <div class="col-lg-6 adapter my-lg-3 my-2">
            <a class="btn card bg-light border-0" href="/adapters/ukp/roberta-large-cola_pfeiffer/">
                <div class="card-body">
                    <h5 class="card-title">
                        
                        lingaccept/cola@ukp
                        
                        <span class="notice">
                            roberta-large
                        </span>
                    </h5>
                    <div class="features">
                        
                            <span class="badge badge-primary">1 version</span>
                            <span class="badge badge-primary">Architecture: pfeiffer</span>
                            
                            
                        
                        <span class="badge badge-primary">Head:&nbsp;
                            
                            <i class="fas fa-check"></i>
                            
                        </span>
                    </div>
                    <p class="card-text">
                        
                            Adapter (with head) trained using the `run_glue.py` script with an extension that retains the best checkpoint (out of 30 epochs).

                        
                    </p>
                </div>
            </a>
        </div>
    
        <div class="col-lg-6 adapter my-lg-3 my-2">
            <a class="btn card bg-light border-0" href="/adapters/ukp/facebook-bart-base_lingaccept_cola_pfeiffer/">
                <div class="card-body">
                    <h5 class="card-title">
                        
                        lingaccept/cola@ukp
                        
                        <span class="notice">
                            facebook/bart-base
                        </span>
                    </h5>
                    <div class="features">
                        
                            <span class="badge badge-primary">1 version</span>
                            <span class="badge badge-primary">Architecture: pfeiffer</span>
                            
                            <span class="badge badge-primary">non-linearity: relu</span>
                            
                            
                            <span class="badge badge-primary">reduction factor: 16</span>
                            
                        
                        <span class="badge badge-primary">Head:&nbsp;
                            
                            <i class="fas fa-check"></i>
                            
                        </span>
                    </div>
                    <p class="card-text">
                        
                            Adapter for bart-base in Pfeiffer architecture trained on the CoLA dataset for 15 epochs with early stopping and a learning rate of 1e-4.

                        
                    </p>
                </div>
            </a>
        </div>
    
        <div class="col-lg-6 adapter my-lg-3 my-2">
            <a class="btn card bg-light border-0" href="/adapters/ukp/gpt2_lingaccept_cola_pfeiffer/">
                <div class="card-body">
                    <h5 class="card-title">
                        
                        lingaccept/cola@ukp
                        
                        <span class="notice">
                            gpt2
                        </span>
                    </h5>
                    <div class="features">
                        
                            <span class="badge badge-primary">1 version</span>
                            <span class="badge badge-primary">Architecture: pfeiffer</span>
                            
                            <span class="badge badge-primary">non-linearity: relu</span>
                            
                            
                            <span class="badge badge-primary">reduction factor: 16</span>
                            
                        
                        <span class="badge badge-primary">Head:&nbsp;
                            
                            <i class="fas fa-check"></i>
                            
                        </span>
                    </div>
                    <p class="card-text">
                        
                            Adapter for gpt2 in Pfeiffer architecture trained on the COLA dataset for 10 epochs with a learning rate of 1e-4.

                        
                    </p>
                </div>
            </a>
        </div>
    
        <div class="col-lg-6 adapter my-lg-3 my-2">
            <a class="btn card bg-light border-0" href="/adapters/ukp/facebook-bart-base_lingaccept_cola_houlsby/">
                <div class="card-body">
                    <h5 class="card-title">
                        
                        lingaccept/cola@ukp
                        
                        <span class="notice">
                            facebook/bart-base
                        </span>
                    </h5>
                    <div class="features">
                        
                            <span class="badge badge-primary">1 version</span>
                            <span class="badge badge-primary">Architecture: houlsby</span>
                            
                            <span class="badge badge-primary">non-linearity: swish</span>
                            
                            
                            <span class="badge badge-primary">reduction factor: 16</span>
                            
                        
                        <span class="badge badge-primary">Head:&nbsp;
                            
                            <i class="fas fa-check"></i>
                            
                        </span>
                    </div>
                    <p class="card-text">
                        
                            Adapter for bart-base in Houlsby architecture trained on the CoLA dataset for 15 epochs with early stopping and a learning rate of 1e-4.

                        
                    </p>
                </div>
            </a>
        </div>
    
        <div class="col-lg-6 adapter my-lg-3 my-2">
            <a class="btn card bg-light border-0" href="/adapters/AdapterHub/bert-base-uncased-pf-cola/">
                <div class="card-body">
                    <h5 class="card-title">
                        
                        AdapterHub/bert-base-uncased-pf-cola
                        
                        <span class="notice">
                            bert-base-uncased
                        </span>
                    </h5>
                    <div class="features">
                        
                            <span class="badge badge-hf text-white">huggingface.co</span>
                        
                        <span class="badge badge-primary">Head:&nbsp;
                            
                            <i class="fas fa-check"></i>
                            
                        </span>
                    </div>
                    <p class="card-text">
                        
                            # Adapter `AdapterHub/bert-base-uncased-pf-cola` for bert-base-uncased

An [adapter](https://adapterhub.ml) for the `bert-base-uncased` model that was trained on the...
                        
                    </p>
                </div>
            </a>
        </div>
    
        <div class="col-lg-6 adapter my-lg-3 my-2">
            <a class="btn card bg-light border-0" href="/adapters/AdapterHub/roberta-base-pf-cola/">
                <div class="card-body">
                    <h5 class="card-title">
                        
                        AdapterHub/roberta-base-pf-cola
                        
                        <span class="notice">
                            roberta-base
                        </span>
                    </h5>
                    <div class="features">
                        
                            <span class="badge badge-hf text-white">huggingface.co</span>
                        
                        <span class="badge badge-primary">Head:&nbsp;
                            
                            <i class="fas fa-check"></i>
                            
                        </span>
                    </div>
                    <p class="card-text">
                        
                            # Adapter `AdapterHub/roberta-base-pf-cola` for roberta-base

An [adapter](https://adapterhub.ml) for the `roberta-base` model that was trained on the...
                        
                    </p>
                </div>
            </a>
        </div>
    
</div>




</div>

<footer>
    <div class="container">
        <p class="float-md-right text-center text-md-right">
            <a href="https://arxiv.org/abs/2311.11077" target="_blank">Paper</a>
            <!--<span class="text-black-30 px-1">|</span>
            <a href="/imprint-privacy/">Imprint & Privacy</a>-->
        </p>
        <p class="text-muted text-center text-md-left">Brought to you with ❤️ by the AdapterHub Team</p>
    </div>
</footer>

<script>
    document.addEventListener('DOMContentLoaded', function (event) {
        codecopy('pre') // your code tag selector!
        $('.activate-tooltip').tooltip();   // bootstrap tooltips
    })
</script>
<script src="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.js" data-cfasync="false"></script>
<script>
    window.cookieconsent.initialise({
    "palette": {
        "popup": {
        "background": "#d7f0f4"
        },
        "button": {
        "background": "#39b3c6",
        "text": "#ffffff"
        }
    },
    "theme": "classic"
    });
</script>
</body>
</html>